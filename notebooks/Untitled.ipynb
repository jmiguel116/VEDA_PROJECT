{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b94f8c34-44eb-4ffb-ac69-286a10991517",
   "metadata": {},
   "source": [
    "## Initialization Script\n",
    "This cell contains the initialization script for setting up the Flask application with SQLAlchemy and Flask-Migrate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4d3f0c-af1e-4b40-b763-c42c8a582b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask\n",
    "from flask_sqlalchemy import SQLAlchemy\n",
    "from flask_migrate import Migrate\n",
    "from config import Config\n",
    "import logging\n",
    "\n",
    "# Initialize extensions\n",
    "db = SQLAlchemy()\n",
    "migrate = Migrate()\n",
    "\n",
    "def create_app():\n",
    "    # Create and configure the Flask application\n",
    "    app = Flask(__name__)\n",
    "    app.config.from_object(Config)\n",
    "    \n",
    "    # Initialize extensions\n",
    "    db.init_app(app)\n",
    "    migrate.init_app(app, db)\n",
    "    \n",
    "    # Set up logging\n",
    "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "    \n",
    "    # Import and register blueprints\n",
    "    from . import routes, models\n",
    "    \n",
    "    # Example of how to register a blueprint\n",
    "    # from .routes import main as main_blueprint\n",
    "    # app.register_blueprint(main_blueprint)\n",
    "    \n",
    "    @app.errorhandler(500)\n",
    "    def internal_error(error):\n",
    "        db.session.rollback()\n",
    "        return \"500 error\"\n",
    "    \n",
    "    @app.errorhandler(404)\n",
    "    def not_found_error(error):\n",
    "        return \"404 error\"\n",
    "    \n",
    "    return app\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9a67ea-d371-42f7-a6d9-9c87958b91de",
   "metadata": {},
   "source": [
    "## Enhanced OpenAI Integration Script\n",
    "This cell contains the enhanced script for integrating with the OpenAI API, with added flexibility for model selection, adjustable token limits, and improved error handling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07dea821-d9b0-47c8-934e-4061b97cd55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Set the OpenAI API key from environment variables\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "def ask_openai(prompt, model=\"text-davinci-003\", max_tokens=150):\n",
    "    \"\"\"\n",
    "    Send a prompt to the OpenAI API and return the generated response.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The input prompt to send to the OpenAI model.\n",
    "        model (str, optional): The OpenAI model to use. Defaults to \"text-davinci-003\".\n",
    "        max_tokens (int, optional): The maximum number of tokens to generate. Defaults to 150.\n",
    "\n",
    "    Returns:\n",
    "        str: The text generated by the OpenAI model in response to the prompt.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = openai.Completion.create(\n",
    "            engine=model,\n",
    "            prompt=prompt,\n",
    "            max_tokens=max_tokens\n",
    "        )\n",
    "        return response.choices[0].text.strip()\n",
    "    except openai.error.OpenAIError as e:\n",
    "        logging.error(f\"OpenAI API error: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unexpected error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    prompt = \"Write a short story about a cat who can talk.\"\n",
    "    response = ask_openai(prompt, model=\"text-davinci-003\", max_tokens=200)\n",
    "    if response:\n",
    "        print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f1b57c-1b58-4b35-86ba-cc083dcb9664",
   "metadata": {},
   "source": [
    "## Combined OCR Module\n",
    "This cell contains the combined OCR module that uses Google Vision for OCR and OpenAI for text enhancement. It includes improved logging, error handling, and flexible configuration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463026a4-d9af-4f8b-9d2c-99ec5e8fa05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from google_vision_ocr import perform_ocr, set_credentials\n",
    "from openai_post_processing import enhance_ocr_text\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def process_image(image_path, credentials_path=None):\n",
    "    \"\"\"\n",
    "    Process an image to extract and enhance text using Google Vision OCR and OpenAI.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): The path to the image file.\n",
    "        credentials_path (str, optional): The path to the Google Cloud credentials JSON file.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of enhanced texts extracted from the image.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if credentials_path:\n",
    "            set_credentials(credentials_path)\n",
    "            logging.info(f\"Credentials set from: {credentials_path}\")\n",
    "        ocr_texts = perform_ocr(image_path)\n",
    "        logging.info(f\"OCR completed successfully for image: {image_path}\")\n",
    "        enhanced_texts = [enhance_ocr_text(text.description) for text in ocr_texts]\n",
    "        logging.info(\"Text enhancement completed successfully.\")\n",
    "        return enhanced_texts\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"Image file not found: {image_path}\")\n",
    "        return []\n",
    "    except ValueError as e:\n",
    "        logging.error(f\"Error during OCR or text enhancement: {e}\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred: {e}\")\n",
    "        return []\n",
    "\n",
    "# Ensure example usage only runs when the script is executed directly\n",
    "if __name__ == \"__main__\":\n",
    "    image_path = input(\"Enter the path to your image: \")\n",
    "    credentials_path = input(\"Enter the path to your Google Cloud credentials JSON file (optional): \").strip()\n",
    "    try:\n",
    "        results = process_image(image_path, credentials_path if credentials_path else None)\n",
    "        if results:\n",
    "            print(\"\\nEnhanced Texts:\")\n",
    "            for result in results:\n",
    "                print(result)\n",
    "        else:\n",
    "            print(\"No texts were enhanced.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d40885-03be-4310-8062-0913da45a3a9",
   "metadata": {},
   "source": [
    "# VEDA OCR Processing Setup\n",
    "\n",
    "This Jupyter Notebook will guide you through setting up and running the OCR processing component of VEDA. We will use a Flask application to handle OCR requests and process images using the combined OCR module.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before we begin, ensure you have the following installed:\n",
    "- Python 3.7 or later\n",
    "- Flask\n",
    "- filetype\n",
    "- Your OCR module dependencies (e.g., Tesseract, Google Vision API, etc.)\n",
    "\n",
    "## Step 1: Setting Up the Virtual Environment\n",
    "\n",
    "First, let's create and activate a virtual environment.\n",
    "\n",
    "```bash\n",
    "# Create a virtual environment\n",
    "python -m venv venv\n",
    "\n",
    "# Activate the virtual environment\n",
    "# On Windows\n",
    "venv\\Scripts\\activate\n",
    "# On Unix or MacOS\n",
    "source venv/bin/activate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2fe72f-4d47-4f1e-8173-ff64a51e18f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from combined_ocr_module import process_image\n",
    "import logging\n",
    "import os\n",
    "import tempfile\n",
    "import filetype\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, \n",
    "                    format='%(asctime)s [%(levelname)s] %(message)s', \n",
    "                    datefmt='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/ocr', methods=['POST'])\n",
    "def ocr_endpoint():\n",
    "    \"\"\"\n",
    "    Endpoint to handle OCR processing requests.\n",
    "    Expects an image file and optional credentials path in the form data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Retrieve the uploaded image file from the request\n",
    "        image_file = request.files['image']\n",
    "        \n",
    "        # Validate the file type\n",
    "        if not filetype.guess(image_file.stream):\n",
    "            logging.warning('Invalid image file received.')\n",
    "            return jsonify({'error': 'Invalid image file'}), 400\n",
    "        \n",
    "        # Save the image file to a temporary location\n",
    "        with tempfile.NamedTemporaryFile(delete=False) as temp_file:\n",
    "            image_path = temp_file.name\n",
    "            image_file.save(image_path)\n",
    "            logging.info(f'Image saved to {image_path}')\n",
    "\n",
    "        # Retrieve credentials path from the form data\n",
    "        credentials_path = request.form.get('credentials_path')\n",
    "        if credentials_path:\n",
    "            logging.info(f'Using credentials from {credentials_path}')\n",
    "        else:\n",
    "            logging.warning('No credentials path provided.')\n",
    "\n",
    "        # Process the image using the combined OCR module\n",
    "        results = process_image(image_path, credentials_path)\n",
    "        return jsonify({'ocr_results': results})\n",
    "    \n",
    "    except KeyError as e:\n",
    "        error_message = f\"Missing form data: {e}\"\n",
    "        logging.error(error_message)\n",
    "        return jsonify({'error': error_message}), 400\n",
    "    \n",
    "    except FileNotFoundError as e:\n",
    "        error_message = f\"Credentials file not found: {e}\"\n",
    "        logging.error(error_message)\n",
    "        return jsonify({'error': error_message}), 400\n",
    "    \n",
    "    except Exception as e:\n",
    "        error_message = f\"An error occurred: {e}\"\n",
    "        logging.error(error_message)\n",
    "        return jsonify({'error': error_message}), 500\n",
    "    \n",
    "    finally:\n",
    "        # Ensure the temporary image file is deleted after processing\n",
    "        if 'image_path' in locals() and os.path.exists(image_path):\n",
    "            os.remove(image_path)\n",
    "            logging.info(f'Temporary image file {image_path} deleted.')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Use environment variables for configuration\n",
    "    debug = os.getenv('FLASK_DEBUG', 'true').lower() in ['true', '1', 't']\n",
    "    port = int(os.getenv('FLASK_PORT', 5000))\n",
    "    app.run(debug=debug, port=port)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b40772-3b43-431e-a237-b19db8ef3e5b",
   "metadata": {},
   "source": [
    "# VEDA Project - OCR Module Development\n",
    "\n",
    "## Objective\n",
    "\n",
    "Develop a module to process images using OCR and enhance the extracted text using OpenAI's language model.\n",
    "\n",
    "## Steps\n",
    "\n",
    "1. **Setup and Configuration**\n",
    "    - Configure logging for debugging and monitoring.\n",
    "    - Set up necessary environment variables for Google Cloud and OpenAI.\n",
    "\n",
    "2. **Function: `process_image`**\n",
    "    - Validate the image file type.\n",
    "    - Load the image.\n",
    "    - Perform OCR using Google Vision.\n",
    "    - Enhance the text using OpenAI.\n",
    "    - Return the results.\n",
    "\n",
    "## Implementation\n",
    "\n",
    "### combined_ocr_module.py\n",
    "\n",
    "```python\n",
    "import logging\n",
    "from google.cloud import vision\n",
    "import openai\n",
    "import os\n",
    "from PIL import Image\n",
    "import io\n",
    "import filetype\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "class ImageProcessingError(Exception):\n",
    "    \"\"\"Custom exception for image processing errors.\"\"\"\n",
    "    pass\n",
    "\n",
    "class OCRError(Exception):\n",
    "    \"\"\"Custom exception for OCR errors.\"\"\"\n",
    "    pass\n",
    "\n",
    "class OpenAIError(Exception):\n",
    "    \"\"\"Custom exception for OpenAI errors.\"\"\"\n",
    "    pass\n",
    "\n",
    "def process_image(image_path, credentials_path, openai_api_key):\n",
    "    \"\"\" \n",
    "    Process the given image to extract and enhance text using OCR and OpenAI.\n",
    "    \n",
    "    Parameters:\n",
    "    - image_path: str, path to the image file\n",
    "    - credentials_path: str, path to the Google Cloud credentials file\n",
    "    - openai_api_key: str, OpenAI API key\n",
    "    \n",
    "    Returns:\n",
    "    - str, enhanced text from the image\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if the image file type is valid\n",
    "        if not filetype.is_image(image_path):\n",
    "            raise ImageProcessingError(\"Invalid image file type.\")\n",
    "\n",
    "        # Set up Google Cloud Vision client\n",
    "        os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = credentials_path\n",
    "        client = vision.ImageAnnotatorClient()\n",
    "\n",
    "        # Load image\n",
    "        with open(image_path, 'rb') as img_file:\n",
    "            content = img_file.read()\n",
    "\n",
    "        image = vision.Image(content=content)\n",
    "\n",
    "        # Perform OCR using Google Cloud Vision\n",
    "        response = client.text_detection(image=image)\n",
    "        texts = response.text_annotations\n",
    "        if not texts:\n",
    "            logging.warning(\"No text detected in the image.\")\n",
    "            raise OCRError(\"No text detected.\")\n",
    "\n",
    "        # Extract detected text\n",
    "        detected_text = texts[0].description\n",
    "        logging.info(f'Detected text: {detected_text}')\n",
    "\n",
    "        # Enhance text using OpenAI\n",
    "        openai.api_key = openai_api_key\n",
    "        enhanced_text = enhance_text_with_openai(detected_text)\n",
    "\n",
    "        return enhanced_text\n",
    "\n",
    "    except ImageProcessingError as e:\n",
    "        logging.error(f\"An error occurred during image processing: {e}\")\n",
    "        raise\n",
    "\n",
    "    except OCRError as e:\n",
    "        logging.error(f\"An error occurred during OCR: {e}\")\n",
    "        raise\n",
    "\n",
    "    except OpenAIError as e:\n",
    "        logging.error(f\"An error occurred during text enhancement: {e}\")\n",
    "        raise\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unknown error occurred during image processing: {e}\")\n",
    "        raise ImageProcessingError(str(e))\n",
    "\n",
    "def enhance_text_with_openai(text, engine=\"text-davinci-003\"):\n",
    "    \"\"\" \n",
    "    Enhance the given text using OpenAI's language model.\n",
    "    \n",
    "    Parameters:\n",
    "    - text: str, the text to enhance\n",
    "    - engine: str, OpenAI engine to use (default: \"text-davinci-003\")\n",
    "    \n",
    "    Returns:\n",
    "    - str, enhanced text\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = openai.Completion.create(\n",
    "            engine=engine,\n",
    "            prompt=f\"Enhance the following text:\\n\\n{text}\",\n",
    "            max_tokens=200,\n",
    "            n=1,\n",
    "            stop=None,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        enhanced_text = response.choices[0].text.strip()\n",
    "        logging.info(f'Enhanced text: {enhanced_text}')\n",
    "\n",
    "        return enhanced_text\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred during text enhancement: {e}\")\n",
    "        raise OpenAIError(str(e))\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    image_path = \"path/to/your/image.jpg\"\n",
    "    credentials_path = \"path/to/your/credentials.json\"\n",
    "    openai_api_key = \"your_openai_api_key\"\n",
    "\n",
    "    try:\n",
    "        enhanced_text = process_image(image_path, credentials_path, openai_api_key)\n",
    "        print(f\"Enhanced Text: {enhanced_text}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba3d13f-fd58-48d1-bbfc-1c484175269b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from google.cloud import vision\n",
    "import openai\n",
    "import os\n",
    "from PIL import Image\n",
    "import io\n",
    "import filetype\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "class ImageProcessingError(Exception):\n",
    "    \"\"\"Custom exception for image processing errors.\"\"\"\n",
    "    pass\n",
    "\n",
    "class OCRError(Exception):\n",
    "    \"\"\"Custom exception for OCR errors.\"\"\"\n",
    "    pass\n",
    "\n",
    "class OpenAIError(Exception):\n",
    "    \"\"\"Custom exception for OpenAI errors.\"\"\"\n",
    "    pass\n",
    "\n",
    "def process_image(image_path, credentials_path, openai_api_key):\n",
    "    \"\"\" \n",
    "    Process the given image to extract and enhance text using OCR and OpenAI.\n",
    "    \n",
    "    Parameters:\n",
    "    - image_path: str, path to the image file\n",
    "    - credentials_path: str, path to the Google Cloud credentials file\n",
    "    - openai_api_key: str, OpenAI API key\n",
    "    \n",
    "    Returns:\n",
    "    - str, enhanced text from the image\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if the image file type is valid\n",
    "        if not filetype.is_image(image_path):\n",
    "            raise ImageProcessingError(\"Invalid image file type.\")\n",
    "\n",
    "        # Set up Google Cloud Vision client\n",
    "        os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = credentials_path\n",
    "        client = vision.ImageAnnotatorClient()\n",
    "\n",
    "        # Load image\n",
    "        with open(image_path, 'rb') as img_file:\n",
    "            content = img_file.read()\n",
    "\n",
    "        image = vision.Image(content=content)\n",
    "\n",
    "        # Perform OCR using Google Cloud Vision\n",
    "        response = client.text_detection(image=image)\n",
    "        texts = response.text_annotations\n",
    "        if not texts:\n",
    "            logging.warning(\"No text detected in the image.\")\n",
    "            raise OCRError(\"No text detected.\")\n",
    "\n",
    "        # Extract detected text\n",
    "        detected_text = texts[0].description\n",
    "        logging.info(f'Detected text: {detected_text}')\n",
    "\n",
    "        # Enhance text using OpenAI\n",
    "        openai.api_key = openai_api_key\n",
    "        enhanced_text = enhance_text_with_openai(detected_text)\n",
    "\n",
    "        return enhanced_text\n",
    "\n",
    "    except ImageProcessingError as e:\n",
    "        logging.error(f\"An error occurred during image processing: {e}\")\n",
    "        raise\n",
    "\n",
    "    except OCRError as e:\n",
    "        logging.error(f\"An error occurred during OCR: {e}\")\n",
    "        raise\n",
    "\n",
    "    except OpenAIError as e:\n",
    "        logging.error(f\"An error occurred during text enhancement: {e}\")\n",
    "        raise\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unknown error occurred during image processing: {e}\")\n",
    "        raise ImageProcessingError(str(e))\n",
    "\n",
    "def enhance_text_with_openai(text, engine=\"text-davinci-003\"):\n",
    "    \"\"\" \n",
    "    Enhance the given text using OpenAI's language model.\n",
    "    \n",
    "    Parameters:\n",
    "    - text: str, the text to enhance\n",
    "    - engine: str, OpenAI engine to use (default: \"text-davinci-003\")\n",
    "    \n",
    "    Returns:\n",
    "    - str, enhanced text\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = openai.Completion.create(\n",
    "            engine=engine,\n",
    "            prompt=f\"Enhance the following text:\\n\\n{text}\",\n",
    "            max_tokens=200,\n",
    "            n=1,\n",
    "            stop=None,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        enhanced_text = response.choices[0].text.strip()\n",
    "        logging.info(f'Enhanced text: {enhanced_text}')\n",
    "\n",
    "        return enhanced_text\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred during text enhancement: {e}\")\n",
    "        raise OpenAIError(str(e))\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    image_path = \"path/to/your/image.jpg\"\n",
    "    credentials_path = \"path/to/your/credentials.json\"\n",
    "    openai_api_key = \"your_openai_api_key\"\n",
    "\n",
    "    try:\n",
    "        enhanced_text = process_image(image_path, credentials_path, openai_api_key)\n",
    "        print(f\"Enhanced Text: {enhanced_text}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783ec383-9e99-438d-8a45-7786def774bb",
   "metadata": {},
   "source": [
    "# OpenAI Integration Script\n",
    "\n",
    "This script integrates OpenAI's language model to enhance text. It includes robust error handling, logging, and initialization functions to ensure smooth and reliable operations.\n",
    "\n",
    "## Script Overview\n",
    "\n",
    "1. **Logging Configuration**: Sets up logging to capture and display important events and errors.\n",
    "2. **Custom Exceptions**: Defines specific exceptions for handling different types of errors.\n",
    "3. **Function Definitions**:\n",
    "   - `initialize_openai_api(api_key)`: Initializes the OpenAI API with the provided key.\n",
    "   - `enhance_text_with_openai(text, engine=\"text-davinci-003\", max_tokens=200, temperature=0.7)`: Enhances the given text using OpenAI's language model.\n",
    "4. **Main Function**: Demonstrates the usage of the functions with an example text.\n",
    "\n",
    "## Script Code\n",
    "\n",
    "```python\n",
    "import openai\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "class OpenAIError(Exception):\n",
    "    \"\"\"Custom exception for OpenAI errors.\"\"\"\n",
    "\n",
    "def initialize_openai_api(api_key):\n",
    "    \"\"\"Initialize OpenAI API with the provided key.\"\"\"\n",
    "    if not api_key:\n",
    "        raise ValueError(\"OpenAI API key must be provided.\")\n",
    "    openai.api_key = api_key\n",
    "    logging.info(\"OpenAI API initialized with the provided key.\")\n",
    "\n",
    "def enhance_text_with_openai(text, engine=\"text-davinci-003\", max_tokens=200, temperature=0.7):\n",
    "    \"\"\"Enhance the given text using OpenAI's language model.\n",
    "\n",
    "    Args:\n",
    "    text (str): The text to enhance.\n",
    "    engine (str, optional): OpenAI engine to use. Defaults to \"text-davinci-003\".\n",
    "    max_tokens (int, optional): Maximum number of tokens for the response. Defaults to 200.\n",
    "    temperature (float, optional): Sampling temperature. Defaults to 0.7.\n",
    "\n",
    "    Returns:\n",
    "    str: Enhanced text.\n",
    "\n",
    "    Raises:\n",
    "    OpenAIError: If an error occurs during text enhancement.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Validate input text and engine\n",
    "        if not isinstance(text, str):\n",
    "            raise ValueError(\"Input text must be a string.\")\n",
    "        if not isinstance(engine, str):\n",
    "            raise ValueError(\"Engine name must be a string.\")\n",
    "\n",
    "        response = openai.Completion.create(\n",
    "            engine=engine,\n",
    "            prompt=f\"Enhance the following text:\\n\\n{text}\",\n",
    "            max_tokens=max_tokens,\n",
    "            n=1,\n",
    "            stop=None,\n",
    "            temperature=temperature\n",
    "        )\n",
    "\n",
    "        enhanced_text = response.choices[0].text.strip()\n",
    "        logging.info(f'Enhanced text: {enhanced_text}')\n",
    "        return enhanced_text\n",
    "\n",
    "    except openai.error.APIError as e:\n",
    "        error_message = f\"OpenAI API error during text enhancement: {e}\"\n",
    "        logging.error(error_message)\n",
    "        raise OpenAIError(error_message)\n",
    "\n",
    "    except openai.error.InvalidRequestError as e:\n",
    "        error_message = f\"Invalid request to OpenAI API: {e}\"\n",
    "        logging.error(error_message)\n",
    "        raise OpenAIError(error_message)\n",
    "\n",
    "    except Exception as e:\n",
    "        error_message = f\"An error occurred during text enhancement: {e}\"\n",
    "        logging.error(error_message)\n",
    "        raise OpenAIError(error_message)\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function for example usage.\"\"\"\n",
    "    openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "    if not openai_api_key:\n",
    "        logging.error(\"OpenAI API key not found in environment variables.\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        initialize_openai_api(openai_api_key)\n",
    "        text_to_enhance = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "        print(\"Processing text...\")\n",
    "        enhanced_text = enhance_text_with_openai(text_to_enhance)\n",
    "        print(f\"Enhanced Text:\\n{enhanced_text}\")\n",
    "\n",
    "    except (ValueError, OpenAIError) as e:\n",
    "        logging.error(f\"Error: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203cb0f8-f713-4c88-bd17-cde5f053a336",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "class OpenAIError(Exception):\n",
    "    \"\"\"Custom exception for OpenAI errors.\"\"\"\n",
    "\n",
    "def initialize_openai_api(api_key):\n",
    "    \"\"\"Initialize OpenAI API with the provided key.\"\"\"\n",
    "    if not api_key:\n",
    "        raise ValueError(\"OpenAI API key must be provided.\")\n",
    "    openai.api_key = api_key\n",
    "    logging.info(\"OpenAI API initialized with the provided key.\")\n",
    "\n",
    "def enhance_text_with_openai(text, engine=\"text-davinci-003\", max_tokens=200, temperature=0.7):\n",
    "    \"\"\"Enhance the given text using OpenAI's language model.\n",
    "\n",
    "    Args:\n",
    "    text (str): The text to enhance.\n",
    "    engine (str, optional): OpenAI engine to use. Defaults to \"text-davinci-003\".\n",
    "    max_tokens (int, optional): Maximum number of tokens for the response. Defaults to 200.\n",
    "    temperature (float, optional): Sampling temperature. Defaults to 0.7.\n",
    "\n",
    "    Returns:\n",
    "    str: Enhanced text.\n",
    "\n",
    "    Raises:\n",
    "    OpenAIError: If an error occurs during text enhancement.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Validate input text and engine\n",
    "        if not isinstance(text, str):\n",
    "            raise ValueError(\"Input text must be a string.\")\n",
    "        if not isinstance(engine, str):\n",
    "            raise ValueError(\"Engine name must be a string.\")\n",
    "\n",
    "        response = openai.Completion.create(\n",
    "            engine=engine,\n",
    "            prompt=f\"Enhance the following text:\\n\\n{text}\",\n",
    "            max_tokens=max_tokens,\n",
    "            n=1,\n",
    "            stop=None,\n",
    "            temperature=temperature\n",
    "        )\n",
    "\n",
    "        enhanced_text = response.choices[0].text.strip()\n",
    "        logging.info(f'Enhanced text: {enhanced_text}')\n",
    "        return enhanced_text\n",
    "\n",
    "    except openai.error.APIError as e:\n",
    "        error_message = f\"OpenAI API error during text enhancement: {e}\"\n",
    "        logging.error(error_message)\n",
    "        raise OpenAIError(error_message)\n",
    "\n",
    "    except openai.error.InvalidRequestError as e:\n",
    "        error_message = f\"Invalid request to OpenAI API: {e}\"\n",
    "        logging.error(error_message)\n",
    "        raise OpenAIError(error_message)\n",
    "\n",
    "    except Exception as e:\n",
    "        error_message = f\"An error occurred during text enhancement: {e}\"\n",
    "        logging.error(error_message)\n",
    "        raise OpenAIError(error_message)\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function for example usage.\"\"\"\n",
    "    openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "    if not openai_api_key:\n",
    "        logging.error(\"OpenAI API key not found in environment variables.\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        initialize_openai_api(openai_api_key)\n",
    "        text_to_enhance = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "        print(\"Processing text...\")\n",
    "        enhanced_text = enhance_text_with_openai(text_to_enhance)\n",
    "        print(f\"Enhanced Text:\\n{enhanced_text}\")\n",
    "\n",
    "    except (ValueError, OpenAIError) as e:\n",
    "        logging.error(f\"Error: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3187a6-4148-444f-82e6-d9a218c0f62a",
   "metadata": {},
   "source": [
    "# Google Vertex AI Integration Script\n",
    "\n",
    "This script integrates with Google Vertex AI to initialize the AI platform and make predictions using deployed models. It includes robust error handling, logging, and initialization functions.\n",
    "\n",
    "## Script Overview\n",
    "\n",
    "1. **Logging Configuration**: Sets up logging to capture and display important events and errors.\n",
    "2. **Custom Exceptions**: Defines specific exceptions for handling different types of errors.\n",
    "3. **Function Definitions**:\n",
    "   - `initialize_vertex_ai(credentials_path, project_id, location=\"us-central1\")`: Initializes Google Vertex AI with the provided credentials and project details.\n",
    "   - `predict_with_vertex_ai(model_name, instances)`: Makes predictions using a deployed model in Google Vertex AI.\n",
    "4. **Main Function**: Demonstrates the usage of the functions with example inputs.\n",
    "\n",
    "## Script Code\n",
    "\n",
    "```python\n",
    "import logging\n",
    "from google.cloud import aiplatform\n",
    "import os\n",
    "import yaml\n",
    "from google.api_core.exceptions import NotFound, InvalidArgument\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "class VertexAIError(Exception):\n",
    "    \"\"\"Custom exception for Google Vertex AI errors.\"\"\"\n",
    "\n",
    "def initialize_vertex_ai(credentials_path, project_id, location=\"us-central1\"):\n",
    "    \"\"\"Initialize Google Vertex AI with the provided credentials and project details.\n",
    "\n",
    "    Args:\n",
    "        credentials_path (str): Path to the Google Cloud credentials file.\n",
    "        project_id (str): Google Cloud project ID.\n",
    "        location (str, optional): Vertex AI region. Defaults to \"us-central1\".\n",
    "\n",
    "    Raises:\n",
    "        VertexAIError: If an error occurs during initialization.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = credentials_path\n",
    "        aiplatform.init(project=project_id, location=location)\n",
    "        logging.info(\"Google Vertex AI initialized.\")\n",
    "    except FileNotFoundError as e:\n",
    "        error_message = f\"Credentials file not found: {e}\"\n",
    "        logging.error(error_message)\n",
    "        raise VertexAIError(error_message)\n",
    "    except Exception as e:\n",
    "        error_message = f\"Failed to initialize Google Vertex AI: {e}\"\n",
    "        logging.error(error_message)\n",
    "        raise VertexAIError(error_message)\n",
    "\n",
    "def predict_with_vertex_ai(model_name, instances):\n",
    "    \"\"\"Make predictions using a deployed model in Google Vertex AI.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): The name of the deployed model.\n",
    "        instances (list): List of instances for prediction.\n",
    "\n",
    "    Returns:\n",
    "        list: Predictions from the model.\n",
    "\n",
    "    Raises:\n",
    "        VertexAIError: If an error occurs during prediction.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        endpoint = aiplatform.Endpoint(model_name)\n",
    "        predictions = endpoint.predict(instances=instances).predictions\n",
    "        logging.info(f\"Predictions: {predictions}\")\n",
    "        return predictions\n",
    "    except NotFound as e:\n",
    "        error_message = f\"Model or endpoint not found: {e}\"\n",
    "        logging.error(error_message)\n",
    "        raise VertexAIError(error_message)\n",
    "    except InvalidArgument as e:\n",
    "        error_message = f\"Invalid input data: {e}\"\n",
    "        logging.error(error_message)\n",
    "        raise VertexAIError(error_message)\n",
    "    except Exception as e:\n",
    "        error_message = f\"Failed to make predictions with Vertex AI: {e}\"\n",
    "        logging.error(error_message)\n",
    "        raise VertexAIError(error_message)\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function for example usage.\"\"\"\n",
    "    config_path = \"config.yaml\"\n",
    "    try:\n",
    "        with open(config_path, 'r') as f:\n",
    "            config = yaml.safe_load(f)\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"Configuration file not found at {config_path}\")\n",
    "        return\n",
    "\n",
    "    credentials_path = config.get('credentials_path')\n",
    "    project_id = config.get('project_id')\n",
    "    model_name = config.get('model_name')\n",
    "    location = config.get('location', \"us-central1\")\n",
    "\n",
    "    if not all([credentials_path, project_id, model_name]):\n",
    "        logging.error(\"Missing required configuration values.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        initialize_vertex_ai(credentials_path, project_id, location)\n",
    "\n",
    "        # Example input data\n",
    "        instances = [{\"input\": \"example input data\"}]\n",
    "\n",
    "        logging.info(\"Making predictions...\")\n",
    "        predictions = predict_with_vertex_ai(model_name, instances)\n",
    "        logging.info(f\"Predictions: {predictions}\")\n",
    "\n",
    "    except (ValueError, VertexAIError) as e:\n",
    "        logging.error(f\"Error: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b727da4a-f664-4d06-83e6-2049e1fb433c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from google.cloud import aiplatform\n",
    "import os\n",
    "import yaml\n",
    "from google.api_core.exceptions import NotFound, InvalidArgument\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "class VertexAIError(Exception):\n",
    "    \"\"\"Custom exception for Google Vertex AI errors.\"\"\"\n",
    "\n",
    "def initialize_vertex_ai(credentials_path, project_id, location=\"us-central1\"):\n",
    "    \"\"\"Initialize Google Vertex AI with the provided credentials and project details.\n",
    "\n",
    "    Args:\n",
    "        credentials_path (str): Path to the Google Cloud credentials file.\n",
    "        project_id (str): Google Cloud project ID.\n",
    "        location (str, optional): Vertex AI region. Defaults to \"us-central1\".\n",
    "\n",
    "    Raises:\n",
    "        VertexAIError: If an error occurs during initialization.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = credentials_path\n",
    "        aiplatform.init(project=project_id, location=location)\n",
    "        logging.info(\"Google Vertex AI initialized.\")\n",
    "    except FileNotFoundError as e:\n",
    "        error_message = f\"Credentials file not found: {e}\"\n",
    "        logging.error(error_message)\n",
    "        raise VertexAIError(error_message)\n",
    "    except Exception as e:\n",
    "        error_message = f\"Failed to initialize Google Vertex AI: {e}\"\n",
    "        logging.error(error_message)\n",
    "        raise VertexAIError(error_message)\n",
    "\n",
    "def upload_model_to_vertex_ai(model_path, display_name, project_id, location=\"us-central1\"):\n",
    "    \"\"\"Upload a model to Google Vertex AI.\n",
    "\n",
    "    Args:\n",
    "        model_path (str): Path to the model file.\n",
    "        display_name (str): Display name for the model in Vertex AI.\n",
    "        project_id (str): Google Cloud project ID.\n",
    "        location (str, optional): Vertex AI region. Defaults to \"us-central1\".\n",
    "\n",
    "    Returns:\n",
    "        model: The uploaded model.\n",
    "\n",
    "    Raises:\n",
    "        VertexAIError: If an error occurs during model upload.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        model = aiplatform.Model.upload(\n",
    "            display_name=display_name,\n",
    "            artifact_uri=model_path,\n",
    "            project=project_id,\n",
    "            location=location,\n",
    "        )\n",
    "        logging.info(f\"Model uploaded successfully: {model.resource_name}\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        error_message = f\"Failed to upload model to Vertex AI: {e}\"\n",
    "        logging.error(error_message)\n",
    "        raise VertexAIError(error_message)\n",
    "\n",
    "def deploy_model_to_endpoint(model, endpoint_name, project_id, location=\"us-central1\"):\n",
    "    \"\"\"Deploy a model to an endpoint in Google Vertex AI.\n",
    "\n",
    "    Args:\n",
    "        model (Model): The model to deploy.\n",
    "        endpoint_name (str): Name of the endpoint to deploy the model to.\n",
    "        project_id (str): Google Cloud project ID.\n",
    "        location (str, optional): Vertex AI region. Defaults to \"us-central1\".\n",
    "\n",
    "    Returns:\n",
    "        endpoint: The deployed endpoint.\n",
    "\n",
    "    Raises:\n",
    "        VertexAIError: If an error occurs during model deployment.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        endpoint = aiplatform.Endpoint.create(\n",
    "            display_name=endpoint_name,\n",
    "            project=project_id,\n",
    "            location=location,\n",
    "        )\n",
    "        model.deploy(endpoint=endpoint)\n",
    "        logging.info(f\"Model deployed to endpoint: {endpoint.resource_name}\")\n",
    "        return endpoint\n",
    "    except Exception as e:\n",
    "        error_message = f\"Failed to deploy model to endpoint: {e}\"\n",
    "        logging.error(error_message)\n",
    "        raise VertexAIError(error_message)\n",
    "\n",
    "def predict_with_vertex_ai(endpoint, instances):\n",
    "    \"\"\"Make predictions using a deployed model in Google Vertex AI.\n",
    "\n",
    "    Args:\n",
    "        endpoint (Endpoint): The endpoint to use for prediction.\n",
    "        instances (list): List of instances for prediction.\n",
    "\n",
    "    Returns:\n",
    "        list: Predictions from the model.\n",
    "\n",
    "    Raises:\n",
    "        VertexAIError: If an error occurs during prediction.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        predictions = endpoint.predict(instances=instances).predictions\n",
    "        logging.info(f\"Predictions: {predictions}\")\n",
    "        return predictions\n",
    "    except NotFound as e:\n",
    "        error_message = f\"Model or endpoint not found: {e}\"\n",
    "        logging.error(error_message)\n",
    "        raise VertexAIError(error_message)\n",
    "    except InvalidArgument as e:\n",
    "        error_message = f\"Invalid input data: {e}\"\n",
    "        logging.error(error_message)\n",
    "        raise VertexAIError(error_message)\n",
    "    except Exception as e:\n",
    "        error_message = f\"Failed to make predictions with Vertex AI: {e}\"\n",
    "        logging.error(error_message)\n",
    "        raise VertexAIError(error_message)\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function for example usage.\"\"\"\n",
    "    config_path = \"config.yaml\"\n",
    "    try:\n",
    "        with open(config_path, 'r') as f:\n",
    "            config = yaml.safe_load(f)\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"Configuration file not found at {config_path}\")\n",
    "        return\n",
    "\n",
    "    credentials_path = config.get('credentials_path')\n",
    "    project_id = config.get('project_id')\n",
    "    model_path = config.get('model_path')\n",
    "    display_name = config.get('display_name')\n",
    "    endpoint_name = config.get('endpoint_name')\n",
    "    location = config.get('location', \"us-central1\")\n",
    "\n",
    "    if not all([credentials_path, project_id, model_path, display_name, endpoint_name]):\n",
    "        logging.error(\"Missing required configuration values.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        initialize_vertex_ai(credentials_path, project_id, location)\n",
    "        model = upload_model_to_vertex_ai(model_path, display_name, project_id, location)\n",
    "        endpoint = deploy_model_to_endpoint(model, endpoint_name, project_id, location)\n",
    "\n",
    "        # Example input data\n",
    "        instances = [{\"input\": \"example input data\"}]\n",
    "\n",
    "        logging.info(\"Making predictions...\")\n",
    "        predictions = predict_with_vertex_ai(endpoint, instances)\n",
    "        logging.info(f\"Predictions: {predictions}\")\n",
    "\n",
    "    except (ValueError, VertexAIError) as e:\n",
    "        logging.error(f\"Error: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b96dba4-82f0-424d-b211-4d2275ebde28",
   "metadata": {},
   "source": [
    "# Models Script for VEDA\n",
    "\n",
    "In this notebook, we will review and enhance the `models.py` script for the VEDA project. The `models.py` script defines the database models using SQLAlchemy for the VEDA application. These models represent the database tables and their relationships.\n",
    "\n",
    "## User Model\n",
    "\n",
    "The `User` model represents the users of the VEDA application. It includes fields for storing user information such as username, email, password hash, and timestamps for creation and updates.\n",
    "\n",
    "```python\n",
    "from flask_sqlalchemy import SQLAlchemy\n",
    "from datetime import datetime\n",
    "from werkzeug.security import generate_password_hash, check_password_hash\n",
    "\n",
    "# Initialize the SQLAlchemy instance\n",
    "db = SQLAlchemy()\n",
    "\n",
    "class User(db.Model):\n",
    "    __tablename__ = 'users'\n",
    "    \n",
    "    id = db.Column(db.Integer, primary_key=True)\n",
    "    username = db.Column(db.String(150), nullable=False, unique=True)\n",
    "    email = db.Column(db.String(150), unique=True, nullable=False)\n",
    "    password_hash = db.Column(db.String(150), nullable=False)\n",
    "    created_at = db.Column(db.DateTime, default=datetime.utcnow)\n",
    "    updated_at = db.Column(db.DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)\n",
    "    \n",
    "    def set_password(self, password):\n",
    "        \"\"\"Hash and set the user's password.\"\"\"\n",
    "        self.password_hash = generate_password_hash(password)\n",
    "    \n",
    "    def check_password(self, password):\n",
    "        \"\"\"Check the user's password.\"\"\"\n",
    "        return check_password_hash(self.password_hash, password)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'<User {self.username}>'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3085054d-9290-44e0-adc4-352735230539",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask_sqlalchemy import SQLAlchemy\n",
    "from datetime import datetime\n",
    "from werkzeug.security import generate_password_hash, check_password_hash\n",
    "\n",
    "# Initialize the SQLAlchemy instance\n",
    "db = SQLAlchemy()\n",
    "\n",
    "class User(db.Model):\n",
    "    __tablename__ = 'users'\n",
    "    \n",
    "    id = db.Column(db.Integer, primary_key=True)\n",
    "    username = db.Column(db.String(150), nullable=False, unique=True)\n",
    "    email = db.Column(db.String(150), unique=True, nullable=False)\n",
    "    password_hash = db.Column(db.String(150), nullable=False)\n",
    "    created_at = db.Column(db.DateTime, default=datetime.utcnow)\n",
    "    updated_at = db.Column(db.DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)\n",
    "    \n",
    "    def set_password(self, password):\n",
    "        \"\"\"Hash and set the user's password.\"\"\"\n",
    "        self.password_hash = generate_password_hash(password)\n",
    "    \n",
    "    def check_password(self, password):\n",
    "        \"\"\"Check the user's password.\"\"\"\n",
    "        return check_password_hash(self.password_hash, password)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'<User {self.username}>'\n",
    "\n",
    "class Document(db.Model):\n",
    "    __tablename__ = 'documents'\n",
    "    \n",
    "    id = db.Column(db.Integer, primary_key=True)\n",
    "    title = db.Column(db.String(150), nullable=False)\n",
    "    content = db.Column(db.Text, nullable=False)\n",
    "    created_at = db.Column(db.DateTime, default=datetime.utcnow)\n",
    "    updated_at = db.Column(db.DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)\n",
    "    file_type = db.Column(db.String(50), nullable=True)  # Store file type, if applicable\n",
    "    file_size = db.Column(db.Integer, nullable=True)  # Store file size, if applicable\n",
    "    unique_id = db.Column(db.String(100), nullable=True)  # Store a unique identifier for the document\n",
    "    user_id = db.Column(db.Integer, db.ForeignKey('users.id'), nullable=False)\n",
    "    user = db.relationship('User', backref=db.backref('documents', lazy=True))\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'<Document {self.title}>'\n",
    "\n",
    "class OcrResult(db.Model):\n",
    "    __tablename__ = 'ocr_results'\n",
    "    \n",
    "    id = db.Column(db.Integer, primary_key=True)\n",
    "    document_id = db.Column(db.Integer, db.ForeignKey('documents.id'), nullable=False)\n",
    "    text = db.Column(db.Text, nullable=False)\n",
    "    confidence_scores = db.Column(db.JSON, nullable=True)  # Store confidence scores for the OCR results\n",
    "    bounding_boxes = db.Column(db.JSON, nullable=True)  # Store bounding boxes for the detected text\n",
    "    language = db.Column(db.String(50), nullable=True)  # Store the language of the detected text\n",
    "    created_at = db.Column(db.DateTime, default=datetime.utcnow)\n",
    "    updated_at = db.Column(db.DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)\n",
    "    document = db.relationship('Document', backref=db.backref('ocr_results', lazy=True))\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'<OcrResult {self.id}>'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3a3958-b39d-44a0-942b-35a2887ff8d3",
   "metadata": {},
   "source": [
    "## Flask Routes Integration\n",
    "\n",
    "This notebook section covers the integration of Flask routes for VEDA, including the interaction with OpenAI and Vertex AI.\n",
    "\n",
    "### Routes\n",
    "\n",
    "1. **Default Route (`/`)**\n",
    "    - Returns a simple message: \"Hello, VEDA!\"\n",
    "\n",
    "2. **Ask OpenAI Endpoint (`/ask_openai`)**\n",
    "    - Method: `POST`\n",
    "    - Description: Interacts with OpenAI to get a response based on the provided prompt.\n",
    "    - Request Body:\n",
    "        ```json\n",
    "        {\n",
    "            \"prompt\": \"Your question or prompt here\"\n",
    "        }\n",
    "        ```\n",
    "    - Response:\n",
    "        ```json\n",
    "        {\n",
    "            \"response\": \"OpenAI response here\"\n",
    "        }\n",
    "        ```\n",
    "\n",
    "3. **Predict Vertex AI Endpoint (`/predict_vertex_ai`)**\n",
    "    - Method: `POST`\n",
    "    - Description: Makes predictions using a deployed model on Vertex AI.\n",
    "    - Request Body:\n",
    "        ```json\n",
    "        {\n",
    "            \"endpoint_id\": \"Your Vertex AI endpoint ID\",\n",
    "            \"instances\": [\"Input data for prediction\"]\n",
    "        }\n",
    "        ```\n",
    "    - Response:\n",
    "        ```json\n",
    "        {\n",
    "            \"prediction\": \"Prediction results here\"\n",
    "        }\n",
    "        ```\n",
    "\n",
    "### Code\n",
    "\n",
    "Below is the Flask routes integration code with enhancements for better logging, error handling, and documentation.\n",
    "\n",
    "```python\n",
    "# Flask routes integration code\n",
    "# Ensure to review, comment, and apply as per the guidelines\n",
    "from flask import render_template, request, jsonify\n",
    "from veda_app import create_app\n",
    "from veda_app.openai_integration import ask_openai\n",
    "from veda_app.google_vertex_integration import init_vertex_ai, predict_with_vertex_ai\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Initialize the Flask app and Vertex AI\n",
    "app = create_app()\n",
    "init_vertex_ai()\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    \"\"\"Default route for VEDA.\"\"\"\n",
    "    return \"Hello, VEDA!\"\n",
    "\n",
    "def handle_api_error(error_message, status_code=500):\n",
    "    \"\"\"Helper function to handle API errors.\"\"\"\n",
    "    logging.error(error_message)\n",
    "    return jsonify({'error': error_message}), status_code\n",
    "\n",
    "@app.route('/ask_openai', methods=['POST'])\n",
    "def ask_openai_endpoint():\n",
    "    \"\"\"Endpoint to interact with OpenAI.\"\"\"\n",
    "    try:\n",
    "        data = request.json\n",
    "        prompt = data.get('prompt')\n",
    "        if not prompt or not isinstance(prompt, str):\n",
    "            return handle_api_error(\"Invalid prompt provided.\", 400)\n",
    "        \n",
    "        logging.info(f\"Received prompt: {prompt}\")\n",
    "        response = ask_openai(prompt)\n",
    "        logging.info(f\"OpenAI response: {response}\")\n",
    "        \n",
    "        return jsonify({'response': response})\n",
    "    \n",
    "    except Exception as e:\n",
    "        return handle_api_error(f\"An error occurred while interacting with OpenAI: {e}\")\n",
    "\n",
    "@app.route('/predict_vertex_ai', methods=['POST'])\n",
    "def predict_vertex_ai_endpoint():\n",
    "    \"\"\"Endpoint to make predictions using Vertex AI.\"\"\"\n",
    "    try:\n",
    "        data = request.json\n",
    "        endpoint_id = data.get('endpoint_id')\n",
    "        instances = data.get('instances')\n",
    "        \n",
    "        if not endpoint_id or not isinstance(endpoint_id, str):\n",
    "            return handle_api_error(\"Invalid endpoint ID provided.\", 400)\n",
    "        \n",
    "        if not instances or not isinstance(instances, list):\n",
    "            return handle_api_error(\"Invalid instances provided.\", 400)\n",
    "        \n",
    "        logging.info(f\"Endpoint ID: {endpoint_id}, Instances: {instances}\")\n",
    "        prediction = predict_with_vertex_ai(endpoint_id, instances)\n",
    "        logging.info(f\"Vertex AI prediction: {prediction}\")\n",
    "        \n",
    "        return jsonify({'prediction': prediction})\n",
    "    \n",
    "    except Exception as e:\n",
    "        return handle_api_error(f\"An error occurred while making predictions with Vertex AI: {e}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Use environment variables for configuration\n",
    "    debug = os.getenv('FLASK_DEBUG', 'true').lower() in ['true', '1', 't']\n",
    "    port = int(os.getenv('FLASK_PORT', 5000))\n",
    "    app.run(debug=debug, port=port)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd7751f-4eee-4102-a759-a0f23959426e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import render_template, request, jsonify\n",
    "from veda_app import create_app\n",
    "from veda_app.openai_integration import ask_openai\n",
    "from veda_app.google_vertex_integration import init_vertex_ai, predict_with_vertex_ai\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Initialize the Flask app and Vertex AI\n",
    "app = create_app()\n",
    "init_vertex_ai()\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    \"\"\"Default route for VEDA.\"\"\"\n",
    "    return \"Hello, VEDA!\"\n",
    "\n",
    "def handle_api_error(error_message, status_code=500):\n",
    "    \"\"\"Helper function to handle API errors.\"\"\"\n",
    "    logging.error(error_message)\n",
    "    return jsonify({'error': error_message}), status_code\n",
    "\n",
    "@app.route('/ask_openai', methods=['POST'])\n",
    "def ask_openai_endpoint():\n",
    "    \"\"\"Endpoint to interact with OpenAI.\"\"\"\n",
    "    try:\n",
    "        data = request.json\n",
    "        prompt = data.get('prompt')\n",
    "        if not prompt or not isinstance(prompt, str):\n",
    "            return handle_api_error(\"Invalid prompt provided.\", 400)\n",
    "        \n",
    "        logging.info(f\"Received prompt: {prompt}\")\n",
    "        response = ask_openai(prompt)\n",
    "        logging.info(f\"OpenAI response: {response}\")\n",
    "        \n",
    "        return jsonify({'response': response})\n",
    "    \n",
    "    except Exception as e:\n",
    "        return handle_api_error(f\"An error occurred while interacting with OpenAI: {e}\")\n",
    "\n",
    "@app.route('/predict_vertex_ai', methods=['POST'])\n",
    "def predict_vertex_ai_endpoint():\n",
    "    \"\"\"Endpoint to make predictions using Vertex AI.\"\"\"\n",
    "    try:\n",
    "        data = request.json\n",
    "        endpoint_id = data.get('endpoint_id')\n",
    "        instances = data.get('instances')\n",
    "        \n",
    "        if not endpoint_id or not isinstance(endpoint_id, str):\n",
    "            return handle_api_error(\"Invalid endpoint ID provided.\", 400)\n",
    "        \n",
    "        if not instances or not isinstance(instances, list):\n",
    "            return handle_api_error(\"Invalid instances provided.\", 400)\n",
    "        \n",
    "        logging.info(f\"Endpoint ID: {endpoint_id}, Instances: {instances}\")\n",
    "        prediction = predict_with_vertex_ai(endpoint_id, instances)\n",
    "        logging.info(f\"Vertex AI prediction: {prediction}\")\n",
    "        \n",
    "        return jsonify({'prediction': prediction})\n",
    "    \n",
    "    except Exception as e:\n",
    "        return handle_api_error(f\"An error occurred while making predictions with Vertex AI: {e}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Use environment variables for configuration\n",
    "    debug = os.getenv('FLASK_DEBUG', 'true').lower() in ['true', '1', 't']\n",
    "    port = int(os.getenv('FLASK_PORT', 5000))\n",
    "    app.run(debug=debug, port=port)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa68698-5404-4779-bccb-41a975696db1",
   "metadata": {},
   "source": [
    "## Configuring the Flask Application\n",
    "\n",
    "The `config.py` script is essential for setting up the configuration of the Flask application. It handles various configuration settings such as database URIs, secret keys, and other environment-specific settings.\n",
    "\n",
    "### Base Configuration Class\n",
    "The `Config` class is the base configuration class that includes default settings. Environment-specific configurations inherit from this base class and can override its settings.\n",
    "\n",
    "### Environment-Specific Configurations\n",
    "- `DevelopmentConfig`: Configuration for the development environment. Enables debugging and sets the logging level to DEBUG.\n",
    "- `TestingConfig`: Configuration for the testing environment. Uses a separate test database and sets the logging level to DEBUG.\n",
    "- `ProductionConfig`: Configuration for the production environment. Disables debugging and sets the logging level to WARNING.\n",
    "\n",
    "### Logging Configuration\n",
    "The `configure_logging` function sets up logging based on the current environment's configuration. It ensures that log messages have a consistent format and appropriate log level.\n",
    "\n",
    "### Environment Variables\n",
    "Sensitive data, like secret keys and database URIs, should be sourced from environment variables. This practice enhances security and flexibility.\n",
    "\n",
    "### Example Usage\n",
    "```python\n",
    "from flask import Flask\n",
    "from config import init_app\n",
    "\n",
    "app = Flask(__name__)\n",
    "init_app(app)\n",
    "\n",
    "# Additional initialization can be done here\n",
    "# db.init_app(app)\n",
    "# migrate.init_app(app, db)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50f580d-a65d-4c81-8c55-7b9ca56b0080",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import secrets\n",
    "import logging\n",
    "\n",
    "class Config:\n",
    "    \"\"\"Base configuration class.\"\"\"\n",
    "    SECRET_KEY = os.environ.get('SECRET_KEY') or secrets.token_urlsafe(32)\n",
    "    SQLALCHEMY_DATABASE_URI = os.environ.get('DATABASE_URL') or \\\n",
    "        'sqlite:///{}'.format(os.path.join(os.path.dirname(__file__), 'app.db'))\n",
    "    SQLALCHEMY_TRACK_MODIFICATIONS = False\n",
    "    UPLOAD_FOLDER = os.path.join(os.path.dirname(__file__), 'uploads')\n",
    "    LOGGING_LEVEL = logging.INFO\n",
    "\n",
    "class DevelopmentConfig(Config):\n",
    "    \"\"\"Development configuration class.\"\"\"\n",
    "    DEBUG = True\n",
    "    LOGGING_LEVEL = logging.DEBUG\n",
    "\n",
    "class TestingConfig(Config):\n",
    "    \"\"\"Testing configuration class.\"\"\"\n",
    "    TESTING = True\n",
    "    SQLALCHEMY_DATABASE_URI = 'sqlite:///{}'.format(os.path.join(os.path.dirname(__file__), 'test.db'))\n",
    "    LOGGING_LEVEL = logging.DEBUG\n",
    "\n",
    "class ProductionConfig(Config):\n",
    "    \"\"\"Production configuration class.\"\"\"\n",
    "    DEBUG = False\n",
    "    LOGGING_LEVEL = logging.WARNING\n",
    "    # Add other production-specific settings here\n",
    "\n",
    "def configure_logging():\n",
    "    \"\"\"Configure logging based on the environment.\"\"\"\n",
    "    logging.basicConfig(level=os.getenv('LOGGING_LEVEL', logging.INFO),\n",
    "                        format='%(asctime)s [%(levelname)s] %(message)s',\n",
    "                        datefmt='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Mapping of configuration classes\n",
    "config = {\n",
    "    'development': DevelopmentConfig,\n",
    "    'testing': TestingConfig,\n",
    "    'production': ProductionConfig,\n",
    "    'default': DevelopmentConfig\n",
    "}\n",
    "\n",
    "def init_app(app):\n",
    "    \"\"\"Initialize the Flask application with the appropriate configuration.\"\"\"\n",
    "    env = os.getenv('FLASK_ENV', 'default')\n",
    "    app.config.from_object(config.get(env, 'default'))\n",
    "    configure_logging()\n",
    "\n",
    "    # Example of additional initialization\n",
    "    # db.init_app(app)\n",
    "    # migrate.init_app(app, db)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b95350c-e91a-4338-8bd9-0ecd9fa61352",
   "metadata": {},
   "source": [
    "# Flask Application Initialization (`__init__.py`)\n",
    "\n",
    "This script sets up the Flask application, initializes extensions, and configures error handling and logging.\n",
    "\n",
    "## Key Components\n",
    "\n",
    "1. **Application Factory**:\n",
    "    - `create_app` function initializes the app with configuration settings.\n",
    "    - Supports multiple configuration classes for different environments.\n",
    "\n",
    "2. **Extensions**:\n",
    "    - Initializes `SQLAlchemy`, `Migrate`, and `LoginManager`.\n",
    "\n",
    "3. **Blueprints**:\n",
    "    - Registers `main` and `auth` blueprints.\n",
    "\n",
    "4. **Error Handling**:\n",
    "    - Custom error handlers for 404 and 500 HTTP errors.\n",
    "\n",
    "5. **Logging**:\n",
    "    - Configures logging to stdout or a file based on the environment.\n",
    "    - Logs application startup and other significant events.\n",
    "\n",
    "## Code Example\n",
    "\n",
    "```python\n",
    "import os\n",
    "import logging\n",
    "from logging.handlers import RotatingFileHandler\n",
    "from flask import Flask, render_template\n",
    "from config import Config\n",
    "from flask_sqlalchemy import SQLAlchemy\n",
    "from flask_migrate import Migrate\n",
    "from flask_login import LoginManager\n",
    "\n",
    "db = SQLAlchemy()\n",
    "migrate = Migrate()\n",
    "login = LoginManager()\n",
    "login.login_view = 'auth.login'\n",
    "\n",
    "def create_app(config_class=Config):\n",
    "    app = Flask(__name__, instance_relative_config=True)\n",
    "    app.config.from_object(config_class)\n",
    "    \n",
    "    # Initialize extensions\n",
    "    db.init_app(app)\n",
    "    migrate.init_app(app, db)\n",
    "    login.init_app(app)\n",
    "    \n",
    "    # Register blueprints\n",
    "    from app.main import bp as main_bp\n",
    "    app.register_blueprint(main_bp)\n",
    "\n",
    "    from app.auth import bp as auth_bp\n",
    "    app.register_blueprint(auth_bp, url_prefix='/auth')\n",
    "    \n",
    "    # Error handling\n",
    "    @app.errorhandler(404)\n",
    "    def page_not_found(error):\n",
    "        return render_template('errors/404.html'), 404\n",
    "\n",
    "    @app.errorhandler(500)\n",
    "    def internal_server_error(error):\n",
    "        db.session.rollback()  # Ensure session is rolled back to avoid any database inconsistency\n",
    "        return render_template('errors/500.html'), 500\n",
    "    \n",
    "    # Logging\n",
    "    if not app.debug:\n",
    "        if app.config.get('LOG_TO_STDOUT'):\n",
    "            stream_handler = logging.StreamHandler()\n",
    "            stream_handler.setLevel(logging.INFO)\n",
    "            app.logger.addHandler(stream_handler)\n",
    "        else:\n",
    "            if not os.path.exists('logs'):\n",
    "                os.mkdir('logs')\n",
    "            file_handler = RotatingFileHandler('logs/veda.log', maxBytes=10240, backupCount=10)\n",
    "            file_handler.setFormatter(logging.Formatter(\n",
    "                '%(asctime)s %(levelname)s: %(message)s [in %(pathname)s:%(lineno)d]'\n",
    "            ))\n",
    "            file_handler.setLevel(logging.INFO)\n",
    "            app.logger.addHandler(file_handler)\n",
    "\n",
    "        app.logger.setLevel(logging.INFO)\n",
    "        app.logger.info('Veda App startup')\n",
    "\n",
    "    return app\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb97855-a612-4ccf-84ce-f05c917b75fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from logging.handlers import RotatingFileHandler\n",
    "from flask import Flask, render_template\n",
    "from config import Config\n",
    "from flask_sqlalchemy import SQLAlchemy\n",
    "from flask_migrate import Migrate\n",
    "from flask_login import LoginManager\n",
    "\n",
    "db = SQLAlchemy()\n",
    "migrate = Migrate()\n",
    "login = LoginManager()\n",
    "login.login_view = 'auth.login'\n",
    "\n",
    "def create_app(config_class=Config):\n",
    "    app = Flask(__name__, instance_relative_config=True)\n",
    "    app.config.from_object(config_class)\n",
    "    \n",
    "    # Initialize extensions\n",
    "    db.init_app(app)\n",
    "    migrate.init_app(app, db)\n",
    "    login.init_app(app)\n",
    "    \n",
    "    # Register blueprints\n",
    "    from app.main import bp as main_bp\n",
    "    app.register_blueprint(main_bp)\n",
    "\n",
    "    from app.auth import bp as auth_bp\n",
    "    app.register_blueprint(auth_bp, url_prefix='/auth')\n",
    "    \n",
    "    # Error handling\n",
    "    @app.errorhandler(404)\n",
    "    def page_not_found(error):\n",
    "        return render_template('errors/404.html'), 404\n",
    "\n",
    "    @app.errorhandler(500)\n",
    "    def internal_server_error(error):\n",
    "        db.session.rollback()  # Ensure session is rolled back to avoid any database inconsistency\n",
    "        return render_template('errors/500.html'), 500\n",
    "    \n",
    "    # Logging\n",
    "    if not app.debug:\n",
    "        if app.config.get('LOG_TO_STDOUT'):\n",
    "            stream_handler = logging.StreamHandler()\n",
    "            stream_handler.setLevel(logging.INFO)\n",
    "            app.logger.addHandler(stream_handler)\n",
    "        else:\n",
    "            if not os.path.exists('logs'):\n",
    "                os.mkdir('logs')\n",
    "            file_handler = RotatingFileHandler('logs/veda.log', maxBytes=10240, backupCount=10)\n",
    "            file_handler.setFormatter(logging.Formatter(\n",
    "                '%(asctime)s %(levelname)s: %(message)s [in %(pathname)s:%(lineno)d]'\n",
    "            ))\n",
    "            file_handler.setLevel(logging.INFO)\n",
    "            app.logger.addHandler(file_handler)\n",
    "\n",
    "        app.logger.setLevel(logging.INFO)\n",
    "        app.logger.info('Veda App startup')\n",
    "\n",
    "    return app\n",
    "\n",
    "# Example configuration in config.py\n",
    "class Config:\n",
    "    SECRET_KEY = os.environ.get('SECRET_KEY') or 'you-will-never-guess'\n",
    "    SQLALCHEMY_DATABASE_URI = os.environ.get('DATABASE_URL') or \\\n",
    "        'sqlite:///' + os.path.join(basedir, 'app.db')\n",
    "    SQLALCHEMY_TRACK_MODIFICATIONS = False\n",
    "    LOG_TO_STDOUT = os.environ.get('LOG_TO_STDOUT')\n",
    "\n",
    "# Ensure the following directories and files exist:\n",
    "# - templates/errors/404.html\n",
    "# - templates/errors/500.html\n",
    "# - logs directory for file logging\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e42d44-5100-4747-b17a-e89738bac55f",
   "metadata": {},
   "source": [
    "# Main Blueprint (`main.py`)\n",
    "\n",
    "The `main.py` script defines the primary routes for the VEDA application. It includes routes for the homepage, about page, and contact page.\n",
    "\n",
    "## Key Components\n",
    "\n",
    "1. **Routes**:\n",
    "    - `index`: Homepage route.\n",
    "    - `about`: About page route.\n",
    "    - `contact`: Contact page route with both GET and POST methods.\n",
    "\n",
    "2. **Error Handling**:\n",
    "    - Proper error handling for form submissions in the contact page route.\n",
    "\n",
    "3. **Logging**:\n",
    "    - Logging is implemented to track the rendering of pages and form submissions.\n",
    "\n",
    "4. **Input Sanitization**:\n",
    "    - User inputs are sanitized using the `bleach` library to prevent XSS attacks.\n",
    "\n",
    "5. **Email Validation**:\n",
    "    - Email format is validated using regular expressions.\n",
    "\n",
    "6. **Email Sending**:\n",
    "    - A `send_contact_email` function is included to handle the logic for sending emails.\n",
    "\n",
    "## Code Example\n",
    "\n",
    "```python\n",
    "from flask import Blueprint, render_template, request, jsonify\n",
    "import logging\n",
    "import bleach\n",
    "import re\n",
    "from flask_mail import Mail, Message\n",
    "\n",
    "bp = Blueprint('main', __name__)\n",
    "\n",
    "@bp.route('/')\n",
    "def index():\n",
    "    \"\"\"Homepage route.\"\"\"\n",
    "    logging.info(\"Rendering homepage.\")\n",
    "    return render_template('index.html', title=\"Homepage\")\n",
    "\n",
    "@bp.route('/about')\n",
    "def about():\n",
    "    \"\"\"About page route.\"\"\"\n",
    "    logging.info(\"Rendering about page.\")\n",
    "    return render_template('about.html', title=\"About\")\n",
    "\n",
    "@bp.route('/contact', methods=['GET', 'POST'])\n",
    "def contact():\n",
    "    \"\"\"Contact page route.\"\"\"\n",
    "    if request.method == 'POST':\n",
    "        try:\n",
    "            name = bleach.clean(request.form.get('name', ''))\n",
    "            email = bleach.clean(request.form.get('email', ''))\n",
    "            message = bleach.clean(request.form.get('message', ''))\n",
    "\n",
    "            if not all([name, email, message]):\n",
    "                logging.warning(\"Incomplete form submission.\")\n",
    "                return jsonify({'error': 'All fields are required.'}), 400\n",
    "\n",
    "            if not re.match(r\"^[^@]+@[^@]+\\.[^@]+$\", email):\n",
    "                logging.warning(\"Invalid email format.\")\n",
    "                return jsonify({'error': 'Invalid email address.'}), 400\n",
    "\n",
    "            # Process the contact form (e.g., send email)\n",
    "            send_contact_email(name, email, message)\n",
    "            logging.info(f\"Contact form submitted by {name} with email {email}.\")\n",
    "            return jsonify({'success': 'Message sent successfully.'})\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing contact form: {e}\")\n",
    "            return jsonify({'error': 'An error occurred. Please try again later.'}), 500\n",
    "\n",
    "    logging.info(\"Rendering contact page.\")\n",
    "    return render_template('contact.html', title=\"Contact\")\n",
    "\n",
    "def send_contact_email(name, email, message):\n",
    "    \"\"\"Sends a contact email.\"\"\"\n",
    "    try:\n",
    "        msg = Message(subject=\"Contact Form Submission\",\n",
    "                      sender=email,\n",
    "                      recipients=[\"your-email@example.com\"],  # Replace with your email\n",
    "                      body=f\"Name: {name}\\nEmail: {email}\\nMessage: {message}\")\n",
    "        mail.send(msg)\n",
    "        logging.info(\"Contact email sent successfully.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error sending contact email: {e}\")\n",
    "        raise\n",
    "\n",
    "# Assuming Flask-Mail is set up in your application factory\n",
    "mail = Mail()\n",
    "\n",
    "def create_app(config_class=Config):\n",
    "    app = Flask(__name__, instance_relative_config=True)\n",
    "    app.config.from_object(config_class)\n",
    "    \n",
    "    db.init_app(app)\n",
    "    migrate.init_app(app, db)\n",
    "    login.init_app(app)\n",
    "    mail.init_app(app)\n",
    "\n",
    "    app.register_blueprint(bp)\n",
    "\n",
    "    # Logging\n",
    "    if not app.debug:\n",
    "        if app.config.get('LOG_TO_STDOUT'):\n",
    "            stream_handler = logging.StreamHandler()\n",
    "            stream_handler.setLevel(logging.INFO)\n",
    "            app.logger.addHandler(stream_handler)\n",
    "        else:\n",
    "            file_handler = logging.FileHandler('app.log')\n",
    "            file_handler.setLevel(logging.INFO)\n",
    "            app.logger.addHandler(file_handler)\n",
    "\n",
    "        app.logger.setLevel(logging.INFO)\n",
    "        app.logger.info('Veda App startup')\n",
    "\n",
    "    return app\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f453d2-5e0d-4c79-98ae-d5cf8223a835",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Blueprint, render_template, request, jsonify\n",
    "import logging\n",
    "import bleach\n",
    "import re\n",
    "from flask_mail import Mail, Message\n",
    "\n",
    "bp = Blueprint('main', __name__)\n",
    "\n",
    "@bp.route('/')\n",
    "def index():\n",
    "    \"\"\"Homepage route.\"\"\"\n",
    "    logging.info(\"Rendering homepage.\")\n",
    "    return render_template('index.html', title=\"Homepage\")\n",
    "\n",
    "@bp.route('/about')\n",
    "def about():\n",
    "    \"\"\"About page route.\"\"\"\n",
    "    logging.info(\"Rendering about page.\")\n",
    "    return render_template('about.html', title=\"About\")\n",
    "\n",
    "@bp.route('/contact', methods=['GET', 'POST'])\n",
    "def contact():\n",
    "    \"\"\"Contact page route.\"\"\"\n",
    "    if request.method == 'POST':\n",
    "        try:\n",
    "            name = bleach.clean(request.form.get('name', ''))\n",
    "            email = bleach.clean(request.form.get('email', ''))\n",
    "            message = bleach.clean(request.form.get('message', ''))\n",
    "\n",
    "            if not all([name, email, message]):\n",
    "                logging.warning(\"Incomplete form submission.\")\n",
    "                return jsonify({'error': 'All fields are required.'}), 400\n",
    "\n",
    "            if not re.match(r\"^[^@]+@[^@]+\\.[^@]+$\", email):\n",
    "                logging.warning(\"Invalid email format.\")\n",
    "                return jsonify({'error': 'Invalid email address.'}), 400\n",
    "\n",
    "            # Process the contact form (e.g., send email)\n",
    "            send_contact_email(name, email, message)\n",
    "            logging.info(f\"Contact form submitted by {name} with email {email}.\")\n",
    "            return jsonify({'success': 'Message sent successfully.'})\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing contact form: {e}\")\n",
    "            return jsonify({'error': 'An error occurred. Please try again later.'}), 500\n",
    "\n",
    "    logging.info(\"Rendering contact page.\")\n",
    "    return render_template('contact.html', title=\"Contact\")\n",
    "\n",
    "def send_contact_email(name, email, message):\n",
    "    \"\"\"Sends a contact email.\"\"\"\n",
    "    try:\n",
    "        msg = Message(subject=\"Contact Form Submission\",\n",
    "                      sender=email,\n",
    "                      recipients=[\"your-email@example.com\"],  # Replace with your email\n",
    "                      body=f\"Name: {name}\\nEmail: {email}\\nMessage: {message}\")\n",
    "        mail.send(msg)\n",
    "        logging.info(\"Contact email sent successfully.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error sending contact email: {e}\")\n",
    "        raise\n",
    "\n",
    "# Assuming Flask-Mail is set up in your application factory\n",
    "mail = Mail()\n",
    "\n",
    "def create_app(config_class=Config):\n",
    "    app = Flask(__name__, instance_relative_config=True)\n",
    "    app.config.from_object(config_class)\n",
    "    \n",
    "    db.init_app(app)\n",
    "    migrate.init_app(app, db)\n",
    "    login.init_app(app)\n",
    "    mail.init_app(app)\n",
    "\n",
    "    app.register_blueprint(bp)\n",
    "\n",
    "    # Logging\n",
    "    if not app.debug:\n",
    "        if app.config.get('LOG_TO_STDOUT'):\n",
    "            stream_handler = logging.StreamHandler()\n",
    "            stream_handler.setLevel(logging.INFO)\n",
    "            app.logger.addHandler(stream_handler)\n",
    "        else:\n",
    "            file_handler = logging.FileHandler('app.log')\n",
    "            file_handler.setLevel(logging.INFO)\n",
    "            app.logger.addHandler(file_handler)\n",
    "\n",
    "        app.logger.setLevel(logging.INFO)\n",
    "        app.logger.info('Veda App startup')\n",
    "\n",
    "    return app\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cee95c4-9fd1-4104-8ad5-037891efaedc",
   "metadata": {},
   "source": [
    "# OCR Script (`ocr_script.py`)\n",
    "\n",
    "The `ocr_script.py` script processes images to extract and enhance text using Google Cloud Vision OCR and OpenAI's language model.\n",
    "\n",
    "## Key Components\n",
    "\n",
    "1. **Image Processing**:\n",
    "    - Load and validate image files.\n",
    "    - Perform OCR using Google Cloud Vision.\n",
    "    - Handle errors during image processing and OCR.\n",
    "\n",
    "2. **Text Enhancement**:\n",
    "    - Enhance extracted text using OpenAI's language model.\n",
    "    - Handle errors during text enhancement.\n",
    "\n",
    "3. **Logging**:\n",
    "    - Detailed logging to track the flow and potential errors.\n",
    "\n",
    "4. **Custom Exceptions**:\n",
    "    - Defined custom exceptions for better error handling.\n",
    "\n",
    "## Code Example\n",
    "\n",
    "```python\n",
    "import logging\n",
    "from google.cloud import vision\n",
    "import openai\n",
    "import os\n",
    "from PIL import Image\n",
    "import io\n",
    "import filetype\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "class ImageProcessingError(Exception):\n",
    "    \"\"\"Custom exception for image processing errors.\"\"\"\n",
    "    pass\n",
    "\n",
    "class OCRError(Exception):\n",
    "    \"\"\"Custom exception for OCR errors.\"\"\"\n",
    "    pass\n",
    "\n",
    "class OpenAIError(Exception):\n",
    "    \"\"\"Custom exception for OpenAI errors.\"\"\"\n",
    "    pass\n",
    "\n",
    "def process_image(image_path, credentials_path, openai_api_key, engine=\"text-davinci-003\"):\n",
    "    \"\"\"Process the given image to extract and enhance text using OCR and OpenAI.\n",
    "    \n",
    "    Parameters:\n",
    "    - image_path: str, path to the image file or URL\n",
    "    - credentials_path: str, path to the Google Cloud credentials file\n",
    "    - openai_api_key: str, OpenAI API key\n",
    "    - engine: str, OpenAI engine to use (default: \"text-davinci-003\")\n",
    "    \n",
    "    Returns:\n",
    "    - str, enhanced text from the image\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Set up Google Cloud Vision client\n",
    "        os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = credentials_path\n",
    "        client = vision.ImageAnnotatorClient()\n",
    "        \n",
    "        # Load image and validate file type\n",
    "        kind = filetype.guess(image_path)\n",
    "        if kind is None:\n",
    "            logging.error(f\"Invalid file type: {image_path}\")\n",
    "            raise ImageProcessingError(\"Invalid file type.\")\n",
    "        \n",
    "        image = Image.open(image_path)\n",
    "        image_bytes = io.BytesIO()\n",
    "        image.save(image_bytes, format=kind.extension)\n",
    "        content = image_bytes.getvalue()\n",
    "        \n",
    "        # Perform OCR using Google Cloud Vision\n",
    "        response = client.text_detection(image=vision.Image(content=content))\n",
    "        texts = response.text_annotations\n",
    "        if not texts:\n",
    "            logging.warning(\"No text detected in the image.\")\n",
    "            raise OCRError(\"No text detected.\")\n",
    "        \n",
    "        # Choose the text annotation with the highest confidence\n",
    "        best_text = max(texts, key=lambda text: text.confidence)\n",
    "        extracted_text = best_text.description\n",
    "        \n",
    "        # Enhance text using OpenAI\n",
    "        openai.api_key = openai_api_key\n",
    "        prompt = f\"Original text: {extracted_text}\\nEnhance the text:\"\n",
    "        response = openai.Completion.create(engine=engine, prompt=prompt, max_tokens=200, temperature=0.7)\n",
    "        enhanced_text = response.choices[0].text.strip()\n",
    "        \n",
    "        logging.info(f\"Enhanced text: {enhanced_text}\")\n",
    "        return enhanced_text\n",
    "    \n",
    "    except FileNotFoundError as e:\n",
    "        logging.error(f\"File not found: {e.filename}\")\n",
    "        raise ImageProcessingError(\"File not found.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    image_path = \"path/to/image.jpg\"\n",
    "    credentials_path = \"path/to/credentials.json\"\n",
    "    openai_api_key = \"YOUR_OPENAI_API_KEY\"\n",
    "    try:\n",
    "        enhanced_text = process_image(image_path, credentials_path, openai_api_key)\n",
    "        print(f\"Enhanced Text: {enhanced_text}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114c9bb0-89e7-4b71-9a7f-3778fb6669d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from google.cloud import vision\n",
    "import openai\n",
    "import os\n",
    "from PIL import Image\n",
    "import io\n",
    "import filetype\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "class ImageProcessingError(Exception):\n",
    "    \"\"\"Custom exception for image processing errors.\"\"\"\n",
    "    pass\n",
    "\n",
    "class OCRError(Exception):\n",
    "    \"\"\"Custom exception for OCR errors.\"\"\"\n",
    "    pass\n",
    "\n",
    "class OpenAIError(Exception):\n",
    "    \"\"\"Custom exception for OpenAI errors.\"\"\"\n",
    "    pass\n",
    "\n",
    "def process_image(image_path, credentials_path, openai_api_key, engine=\"text-davinci-003\"):\n",
    "    \"\"\"Process the given image to extract and enhance text using OCR and OpenAI.\n",
    "    \n",
    "    Parameters:\n",
    "    - image_path: str, path to the image file or URL\n",
    "    - credentials_path: str, path to the Google Cloud credentials file\n",
    "    - openai_api_key: str, OpenAI API key\n",
    "    - engine: str, OpenAI engine to use (default: \"text-davinci-003\")\n",
    "    \n",
    "    Returns:\n",
    "    - str, enhanced text from the image\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Set up Google Cloud Vision client\n",
    "        os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = credentials_path\n",
    "        client = vision.ImageAnnotatorClient()\n",
    "        \n",
    "        # Load image and validate file type\n",
    "        kind = filetype.guess(image_path)\n",
    "        if kind is None:\n",
    "            logging.error(f\"Invalid file type: {image_path}\")\n",
    "            raise ImageProcessingError(\"Invalid file type.\")\n",
    "        \n",
    "        image = Image.open(image_path)\n",
    "        image_bytes = io.BytesIO()\n",
    "        image.save(image_bytes, format=kind.extension)\n",
    "        content = image_bytes.getvalue()\n",
    "        \n",
    "        # Perform OCR using Google Cloud Vision\n",
    "        response = client.text_detection(image=vision.Image(content=content))\n",
    "        texts = response.text_annotations\n",
    "        if not texts:\n",
    "            logging.warning(\"No text detected in the image.\")\n",
    "            raise OCRError(\"No text detected.\")\n",
    "        \n",
    "        # Choose the text annotation with the highest confidence\n",
    "        best_text = max(texts, key=lambda text: text.confidence)\n",
    "        extracted_text = best_text.description\n",
    "        \n",
    "        # Enhance text using OpenAI\n",
    "        openai.api_key = openai_api_key\n",
    "        prompt = f\"Original text: {extracted_text}\\nEnhance the text:\"\n",
    "        response = openai.Completion.create(engine=engine, prompt=prompt, max_tokens=200, temperature=0.7)\n",
    "        enhanced_text = response.choices[0].text.strip()\n",
    "        \n",
    "        logging.info(f\"Enhanced text: {enhanced_text}\")\n",
    "        return enhanced_text\n",
    "    \n",
    "    except FileNotFoundError as e:\n",
    "        logging.error(f\"File not found: {e.filename}\")\n",
    "        raise ImageProcessingError(\"File not found.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    image_path = \"path/to/image.jpg\"\n",
    "    credentials_path = \"path/to/credentials.json\"\n",
    "    openai_api_key = \"YOUR_OPENAI_API_KEY\"\n",
    "    try:\n",
    "        enhanced_text = process_image(image_path, credentials_path, openai_api_key)\n",
    "        print(f\"Enhanced Text: {enhanced_text}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff11acb2-2fdd-4657-84fb-ddaef52a5f64",
   "metadata": {},
   "source": [
    "# FSPL Dataset Creation Script (`create_fspl_dataset.py`)\n",
    "\n",
    "The `create_fspl_dataset.py` script generates a dataset for Free Space Path Loss (FSPL) calculations. This dataset can be used for training machine learning models or other analysis purposes.\n",
    "\n",
    "## Key Components\n",
    "\n",
    "1. **Dataset Generation**:\n",
    "    - Generates distances and frequencies.\n",
    "    - Calculates FSPL in decibels (dB) using the formula:\n",
    "      \\[\n",
    "      \\text{FSPL} = 20 \\log_{10}(\\text{distance}) + 20 \\log_{10}(\\text{frequency}) - 147.55\n",
    "      \\]\n",
    "\n",
    "2. **Logging**:\n",
    "    - Detailed logging to track dataset generation and potential errors.\n",
    "\n",
    "3. **Efficiency**:\n",
    "    - Used `tqdm` outside the `Parallel` loop for efficient progress tracking.\n",
    "\n",
    "4. **Parameter Validation**:\n",
    "    - Included basic error handling and parameter validation.\n",
    "\n",
    "## Code Example\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Constants\n",
    "SPEED_OF_LIGHT = 3e8  # Speed of light in m/s\n",
    "\n",
    "def calculate_fspl(frequency_mhz, distance_m, tx_gain_dbi, rx_gain_dbi):\n",
    "    \"\"\"\n",
    "    Calculate the Free Space Path Loss (FSPL) for given parameters.\n",
    "\n",
    "    Args:\n",
    "        frequency_mhz: Frequency in MHz.\n",
    "        distance_m: Distance in meters.\n",
    "        tx_gain_dbi: Transmitter gain in dBi.\n",
    "        rx_gain_dbi: Receiver gain in dBi.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the calculated FSPL and input parameters.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        frequency_hz = frequency_mhz * 1e6  # Convert MHz to Hz\n",
    "        wavelength = SPEED_OF_LIGHT / frequency_hz\n",
    "        fspl = (4 * np.pi * distance_m / wavelength) ** 2\n",
    "        fspl_db = 10 * np.log10(fspl)\n",
    "        return {\n",
    "            'Frequency (MHz)': frequency_mhz,\n",
    "            'Distance (m)': distance_m,\n",
    "            'Tx Gain (dBi)': tx_gain_dbi,\n",
    "            'Rx Gain (dBi)': rx_gain_dbi,\n",
    "            'FSPL (dB)': fspl_db - tx_gain_dbi - rx_gain_dbi\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in FSPL calculation: {e}\")\n",
    "        return {\n",
    "            'Frequency (MHz)': frequency_mhz,\n",
    "            'Distance (m)': distance_m,\n",
    "            'Tx Gain (dBi)': tx_gain_dbi,\n",
    "            'Rx Gain (dBi)': rx_gain_dbi,\n",
    "            'FSPL (dB)': None\n",
    "        }\n",
    "\n",
    "def generate_dataset(frequencies, distances, tx_gains, rx_gains):\n",
    "    \"\"\"\n",
    "    Generate a dataset for FSPL calculations.\n",
    "\n",
    "    Args:\n",
    "        frequencies: Array of frequencies in MHz.\n",
    "        distances: Array of distances in meters.\n",
    "        tx_gains: Array of transmitter gains in dBi.\n",
    "        rx_gains: Array of receiver gains in dBi.\n",
    "\n",
    "    Returns:\n",
    "        A pandas DataFrame containing the generated FSPL dataset.\n",
    "    \"\"\"\n",
    "    total_tasks = len(frequencies) * len(distances) * len(tx_gains) * len(rx_gains)\n",
    "    logging.info(f\"Total calculations to perform: {total_tasks}\")\n",
    "\n",
    "    # Use tqdm outside the Parallel loop for efficient progress tracking\n",
    "    data = []\n",
    "    with tqdm(total=total_tasks, desc=\"Generating FSPL Dataset\") as pbar:\n",
    "        for result in Parallel(n_jobs=-1)(\n",
    "            delayed(calculate_fspl)(freq, dist, tx_gain, rx_gain)\n",
    "            for freq in frequencies\n",
    "            for dist in distances\n",
    "            for tx_gain in tx_gains\n",
    "            for rx_gain in rx_gains\n",
    "        ):\n",
    "            data.append(result)\n",
    "            pbar.update(1)\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "# Define ranges\n",
    "frequencies = np.arange(1, 4001, 0.5)  # 1 MHz to 4 GHz with 0.5 MHz steps\n",
    "distances = np.arange(3, 10001, 0.5)  # 3 meters to 10,000 meters with 0.5 meter steps\n",
    "tx_gains = np.arange(0, 31, 0.5)  # 0 dBi to 30 dBi with 0.5 dBi steps\n",
    "rx_gains = np.arange(0, 31, 0.5)  # 0 dBi to 30 dBi with 0.5 dBi steps\n",
    "\n",
    "logging.info(\"Starting dataset generation...\")\n",
    "dataset = generate_dataset(frequencies, distances, tx_gains, rx_gains)\n",
    "logging.info(\"Dataset generation completed.\")\n",
    "\n",
    "# Specify the save path\n",
    "save_path = os.getenv('FSPL_DATASET_PATH', '/media/jmiguel-rai-control/fd67fcf7-7925-43d5-9ad0-85c2882c0795/fspl_dataset.csv.gz')\n",
    "\n",
    "# Save to CSV\n",
    "dataset.to_csv(save_path, index=False, compression='gzip')\n",
    "logging.info(f\"Dataset saved to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134a3f8c-bb54-491c-96e5-0b842388b23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Constants\n",
    "SPEED_OF_LIGHT = 3e8  # Speed of light in m/s\n",
    "\n",
    "def calculate_fspl(frequency_mhz, distance_m, tx_gain_dbi, rx_gain_dbi):\n",
    "    \"\"\"\n",
    "    Calculate the Free Space Path Loss (FSPL) for given parameters.\n",
    "\n",
    "    Args:\n",
    "        frequency_mhz: Frequency in MHz.\n",
    "        distance_m: Distance in meters.\n",
    "        tx_gain_dbi: Transmitter gain in dBi.\n",
    "        rx_gain_dbi: Receiver gain in dBi.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the calculated FSPL and input parameters.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        frequency_hz = frequency_mhz * 1e6  # Convert MHz to Hz\n",
    "        wavelength = SPEED_OF_LIGHT / frequency_hz\n",
    "        fspl = (4 * np.pi * distance_m / wavelength) ** 2\n",
    "        fspl_db = 10 * np.log10(fspl)\n",
    "        return {\n",
    "            'Frequency (MHz)': frequency_mhz,\n",
    "            'Distance (m)': distance_m,\n",
    "            'Tx Gain (dBi)': tx_gain_dbi,\n",
    "            'Rx Gain (dBi)': rx_gain_dbi,\n",
    "            'FSPL (dB)': fspl_db - tx_gain_dbi - rx_gain_dbi\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in FSPL calculation: {e}\")\n",
    "        return {\n",
    "            'Frequency (MHz)': frequency_mhz,\n",
    "            'Distance (m)': distance_m,\n",
    "            'Tx Gain (dBi)': tx_gain_dbi,\n",
    "            'Rx Gain (dBi)': rx_gain_dbi,\n",
    "            'FSPL (dB)': None\n",
    "        }\n",
    "\n",
    "def generate_dataset(frequencies, distances, tx_gains, rx_gains):\n",
    "    \"\"\"\n",
    "    Generate a dataset for FSPL calculations.\n",
    "\n",
    "    Args:\n",
    "        frequencies: Array of frequencies in MHz.\n",
    "        distances: Array of distances in meters.\n",
    "        tx_gains: Array of transmitter gains in dBi.\n",
    "        rx_gains: Array of receiver gains in dBi.\n",
    "\n",
    "    Returns:\n",
    "        A pandas DataFrame containing the generated FSPL dataset.\n",
    "    \"\"\"\n",
    "    total_tasks = len(frequencies) * len(distances) * len(tx_gains) * len(rx_gains)\n",
    "    logging.info(f\"Total calculations to perform: {total_tasks}\")\n",
    "\n",
    "    # Use tqdm outside the Parallel loop for efficient progress tracking\n",
    "    data = []\n",
    "    with tqdm(total=total_tasks, desc=\"Generating FSPL Dataset\") as pbar:\n",
    "        for result in Parallel(n_jobs=-1)(\n",
    "            delayed(calculate_fspl)(freq, dist, tx_gain, rx_gain)\n",
    "            for freq in frequencies\n",
    "            for dist in distances\n",
    "            for tx_gain in tx_gains\n",
    "            for rx_gain in rx_gains\n",
    "        ):\n",
    "            data.append(result)\n",
    "            pbar.update(1)\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "# Define ranges\n",
    "frequencies = np.arange(1, 4001, 0.5)  # 1 MHz to 4 GHz with 0.5 MHz steps\n",
    "distances = np.arange(3, 10001, 0.5)  # 3 meters to 10,000 meters with 0.5 meter steps\n",
    "tx_gains = np.arange(0, 31, 0.5)  # 0 dBi to 30 dBi with 0.5 dBi steps\n",
    "rx_gains = np.arange(0, 31, 0.5)  # 0 dBi to 30 dBi with 0.5 dBi steps\n",
    "\n",
    "logging.info(\"Starting dataset generation...\")\n",
    "dataset = generate_dataset(frequencies, distances, tx_gains, rx_gains)\n",
    "logging.info(\"Dataset generation completed.\")\n",
    "\n",
    "# Specify the save path\n",
    "save_path = os.getenv('FSPL_DATASET_PATH', '/media/jmiguel-rai-control/fd67fcf7-7925-43d5-9ad0-85c2882c0795/fspl_dataset.csv.gz')\n",
    "\n",
    "# Save to CSV\n",
    "dataset.to_csv(save_path, index=False, compression='gzip')\n",
    "logging.info(f\"Dataset saved to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ca3a84-9655-4939-8966-bafcf6f994ab",
   "metadata": {},
   "source": [
    "## `ingest_files.py` Script\n",
    "\n",
    "### Overview\n",
    "\n",
    "This script handles file uploads, processes the uploaded files using OCR, and saves the results to a database. It includes error handling, logging, and security measures to ensure the application's stability and security.\n",
    "\n",
    "### Key Functions\n",
    "\n",
    "- `allowed_file(filename)`: Checks if the uploaded file has an allowed extension.\n",
    "- `save_document(filename, ocr_results, file_type, file_size, user_id)`: Saves the document and its OCR results to the database.\n",
    "- `upload_file()`: Handles the file upload, processes the file using OCR, and saves the results to the database.\n",
    "\n",
    "### Enhancements\n",
    "\n",
    "- Added docstrings to all functions.\n",
    "- Improved logging with more specific messages.\n",
    "- Implemented robust error handling.\n",
    "- Enhanced security measures with input sanitization and secure filename handling.\n",
    "- Configured the application using environment variables for better flexibility.\n",
    "\n",
    "### Example Usage\n",
    "\n",
    "To run the Flask application:\n",
    "\n",
    "```bash\n",
    "export FLASK_DEBUG=true\n",
    "export FLASK_PORT=5000\n",
    "export GOOGLE_CLOUD_CREDENTIALS=/path/to/credentials.json\n",
    "export OPENAI_API_KEY=your_openai_api_key\n",
    "flask run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d6b64b-2074-43b7-a14d-f641a793a804",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from flask import Flask, request, jsonify, send_from_directory\n",
    "from werkzeug.utils import secure_filename\n",
    "from veda_app import create_app\n",
    "from veda_app.models import db, Document\n",
    "from combined_ocr_module import process_image\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(filename)s - %(lineno)d - %(levelname)s - %(message)s')\n",
    "\n",
    "app = create_app()\n",
    "\n",
    "# Allowed file extensions\n",
    "ALLOWED_EXTENSIONS = {'png', 'jpg', 'jpeg', 'gif', 'pdf'}\n",
    "\n",
    "def allowed_file(filename):\n",
    "    \"\"\"Check if the file has an allowed extension.\"\"\"\n",
    "    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS\n",
    "\n",
    "def save_document(filename, ocr_results, file_type, file_size, user_id):\n",
    "    \"\"\"Save the document and its OCR results to the database.\"\"\"\n",
    "    try:\n",
    "        document = Document(\n",
    "            title=filename,\n",
    "            content=ocr_results,\n",
    "            file_type=file_type,\n",
    "            file_size=file_size,\n",
    "            user_id=user_id\n",
    "        )\n",
    "        db.session.add(document)\n",
    "        db.session.commit()\n",
    "        logging.info(f\"Document {filename} processed and saved to database.\")\n",
    "        return True\n",
    "    except SQLAlchemyError as e:\n",
    "        logging.error(f\"Database error: {e}\")\n",
    "        db.session.rollback()\n",
    "        return False\n",
    "\n",
    "@app.route('/upload', methods=['POST'])\n",
    "def upload_file():\n",
    "    \"\"\"Handle file upload and processing.\"\"\"\n",
    "    if 'file' not in request.files:\n",
    "        logging.error(\"No file part in the request.\")\n",
    "        return jsonify({'error': 'No file part'}), 400\n",
    "\n",
    "    file = request.files['file']\n",
    "    if file.filename == '':\n",
    "        logging.error(\"No selected file.\")\n",
    "        return jsonify({'error': 'No selected file'}), 400\n",
    "\n",
    "    if file and allowed_file(file.filename):\n",
    "        filename = secure_filename(file.filename)\n",
    "        file_path = os.path.join(app.config['UPLOAD_FOLDER'], filename)\n",
    "        file.save(file_path)\n",
    "        logging.info(f\"File {filename} saved at {file_path}\")\n",
    "\n",
    "        # Process the file\n",
    "        try:\n",
    "            credentials_path = app.config.get('GOOGLE_CLOUD_CREDENTIALS')\n",
    "            openai_api_key = app.config.get('OPENAI_API_KEY')\n",
    "            ocr_results = process_image(file_path, credentials_path, openai_api_key)\n",
    "\n",
    "            # Save to database\n",
    "            if save_document(filename, ocr_results, file.mimetype, os.path.getsize(file_path), request.form.get('user_id')):\n",
    "                return jsonify({'success': 'File uploaded and processed successfully', 'ocr_results': ocr_results})\n",
    "            else:\n",
    "                return jsonify({'error': 'An error occurred while saving to the database.'}), 500\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing file: {e}\")\n",
    "            return jsonify({'error': 'An error occurred while processing the file.'}), 500\n",
    "\n",
    "    else:\n",
    "        logging.error(\"File type not allowed.\")\n",
    "        return jsonify({'error': 'File type not allowed'}), 400\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    debug = os.getenv('FLASK_DEBUG', 'true').lower() in ['true', '1', 't']\n",
    "    port = int(os.getenv('FLASK_PORT', 5000))\n",
    "    app.run(debug=debug, port=port)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4572d49e-7b48-4ffb-9d02-bf1dcf97555c",
   "metadata": {},
   "source": [
    "## `train_model.py` Script\n",
    "\n",
    "### Overview\n",
    "\n",
    "This script is responsible for training a machine learning model using the Free Space Path Loss (FSPL) dataset. It includes steps for loading data, preprocessing, training, evaluation, and saving the model.\n",
    "\n",
    "### Key Functions\n",
    "\n",
    "- `load_data(file_path)`: Loads the dataset from the specified file path.\n",
    "- `preprocess_data(data)`: Preprocesses the dataset for training.\n",
    "- `train_model(X_train, y_train)`: Trains a RandomForestRegressor model using the training data and performs hyperparameter tuning using GridSearchCV.\n",
    "- `evaluate_model(model, X_test, y_test)`: Evaluates the model using the test data.\n",
    "- `save_model(model, save_path)`: Saves the trained model to the specified path.\n",
    "- `load_model(save_path)`: Loads a trained model from the specified path.\n",
    "\n",
    "### Enhancements\n",
    "\n",
    "- Added robust error handling for each function.\n",
    "- Improved logging with detailed messages for tracking progress and errors.\n",
    "- Used environment variables for configuration to enhance flexibility.\n",
    "- Included data validation steps to ensure the dataset is suitable for training.\n",
    "- Added docstrings and comments for better understanding and maintainability.\n",
    "- Implemented GridSearchCV for hyperparameter tuning.\n",
    "\n",
    "### Example Usage\n",
    "\n",
    "To run the training script:\n",
    "\n",
    "```bash\n",
    "export DATASET_PATH=/path/to/fspl_dataset.csv.gz\n",
    "export MODEL_SAVE_PATH=/path/to/rf_model.joblib\n",
    "python train_model.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8724f739-9e27-4121-be90-e218cff0ac8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from joblib import dump, load\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(filename)s - %(lineno)d - %(levelname)s - %(message)s')\n",
    "\n",
    "def load_data(file_path):\n",
    "    \"\"\"Load dataset from the specified file path.\"\"\"\n",
    "    try:\n",
    "        data = pd.read_csv(file_path)\n",
    "        logging.info(f\"Data loaded from {file_path}\")\n",
    "        return data\n",
    "    except FileNotFoundError as e:\n",
    "        logging.error(f\"File not found: {file_path}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading data: {e}\")\n",
    "        raise\n",
    "\n",
    "def preprocess_data(data):\n",
    "    \"\"\"Preprocess the dataset for training.\"\"\"\n",
    "    try:\n",
    "        X = data.drop('FSPL (dB)', axis=1)\n",
    "        y = data['FSPL (dB)']\n",
    "        logging.info(\"Data preprocessing completed.\")\n",
    "        return X, y\n",
    "    except KeyError as e:\n",
    "        logging.error(f\"Missing key in data: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error preprocessing data: {e}\")\n",
    "        raise\n",
    "\n",
    "def train_model(X_train, y_train):\n",
    "    \"\"\"Train the model using the training data.\"\"\"\n",
    "    try:\n",
    "        param_grid = {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'max_depth': [5, 10, 20],\n",
    "            'min_samples_split': [2, 5, 10]\n",
    "        }\n",
    "        model = RandomForestRegressor(random_state=42)\n",
    "        grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring='neg_mean_squared_error', cv=5)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        best_model = grid_search.best_estimator_\n",
    "        logging.info(\"Model training completed.\")\n",
    "        return best_model\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error training model: {e}\")\n",
    "        raise\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    \"\"\"Evaluate the model using the test data.\"\"\"\n",
    "    try:\n",
    "        predictions = model.predict(X_test)\n",
    "        mse = mean_squared_error(y_test, predictions)\n",
    "        r2 = r2_score(y_test, predictions)\n",
    "        logging.info(f\"Model evaluation completed. MSE: {mse}, R-squared: {r2}\")\n",
    "        return mse, r2\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error evaluating model: {e}\")\n",
    "        raise\n",
    "\n",
    "def save_model(model, save_path):\n",
    "    \"\"\"Save the trained model to the specified path.\"\"\"\n",
    "    try:\n",
    "        dump(model, save_path)\n",
    "        logging.info(f\"Model saved to {save_path}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving model: {e}\")\n",
    "        raise\n",
    "\n",
    "def load_model(save_path):\n",
    "    \"\"\"Load the trained model from the specified path.\"\"\"\n",
    "    try:\n",
    "        model = load(save_path)\n",
    "        logging.info(f\"Model loaded from {save_path}\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading model: {e}\")\n",
    "        raise\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to orchestrate the training process.\"\"\"\n",
    "    dataset_path = os.getenv('DATASET_PATH', 'fspl_dataset.csv.gz')\n",
    "    model_save_path = os.getenv('MODEL_SAVE_PATH', 'rf_model.joblib')\n",
    "\n",
    "    data = load_data(dataset_path)\n",
    "    X, y = preprocess_data(data)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    model = train_model(X_train, y_train)\n",
    "\n",
    "    mse, r2 = evaluate_model(model, X_test, y_test)\n",
    "\n",
    "    save_model(model, model_save_path)\n",
    "\n",
    "    logging.info(f\"Training process completed with MSE: {mse}, R-squared: {r2}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e00720b-847a-4ded-8350-ad5a66c37b65",
   "metadata": {},
   "source": [
    "## `ingest_files.py` Script\n",
    "\n",
    "### Overview\n",
    "\n",
    "This combined script handles both file ingestion for OCR processing and data preprocessing, model training, and evaluation for FSPL (Free Space Path Loss) prediction.\n",
    "\n",
    "### Key Functions\n",
    "\n",
    "- **OCR File Ingestion**:\n",
    "  - `allowed_file(filename)`: Checks if a file has an allowed extension.\n",
    "  - `save_document(filename, ocr_results, file_type, file_size, user_id)`: Saves the OCR results and file metadata to the database.\n",
    "  - `process_file(file_path, user_id)`: Processes an individual file, including OCR and saving to the database.\n",
    "  - `ingest_files(file_paths, user_id)`: Ingests multiple files using parallel processing.\n",
    "\n",
    "- **FSPL Model Training**:\n",
    "  - `load_data(file_path)`: Loads dataset from the specified file path.\n",
    "  - `preprocess_data(data)`: Preprocesses the dataset for training.\n",
    "  - `train_model(X_train, y_train)`: Trains the model using the training data.\n",
    "  - `evaluate_model(model, X_test, y_test)`: Evaluates the model using the test data.\n",
    "  - `save_model(model, save_path)`: Saves the trained model to the specified path.\n",
    "  - `load_model(save_path)`: Loads the trained model from the specified path.\n",
    "\n",
    "### Enhancements\n",
    "\n",
    "- Added comprehensive error handling to manage various exceptions.\n",
    "- Implemented detailed logging to track the progress and identify issues.\n",
    "- Ensured secure handling of file paths and contents.\n",
    "- Used parallel processing to handle multiple files efficiently.\n",
    "- Added docstrings and comments for better understanding and maintainability.\n",
    "\n",
    "### Example Usage\n",
    "\n",
    "To run the script:\n",
    "\n",
    "```bash\n",
    "export GOOGLE_CLOUD_CREDENTIALS=/path/to/credentials.json\n",
    "export OPENAI_API_KEY=your_openai_api_key\n",
    "export FLASK_ENV=development\n",
    "export DATASET_PATH=path/to/fspl_dataset.csv.gz\n",
    "export MODEL_SAVE_PATH=path/to/rf_model.joblib\n",
    "\n",
    "python ingest_files.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0372f978-9586-4f13-a41e-1acac7c995f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from joblib import dump, load\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from werkzeug.utils import secure_filename\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "from veda_app import create_app, db\n",
    "from veda_app.models import Document\n",
    "from combined_ocr_module import process_image\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(filename)s - %(lineno)d - %(levelname)s - %(message)s')\n",
    "\n",
    "app = create_app()\n",
    "\n",
    "# Allowed file extensions\n",
    "ALLOWED_EXTENSIONS = {'png', 'jpg', 'jpeg', 'gif', 'pdf'}\n",
    "\n",
    "def allowed_file(filename):\n",
    "    \"\"\"Check if the file has an allowed extension.\"\"\"\n",
    "    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS\n",
    "\n",
    "def save_document(filename, ocr_results, file_type, file_size, user_id):\n",
    "    \"\"\"Save the processed document to the database.\"\"\"\n",
    "    try:\n",
    "        document = Document(\n",
    "            title=filename,\n",
    "            content=ocr_results,\n",
    "            file_type=file_type,\n",
    "            file_size=file_size,\n",
    "            user_id=user_id\n",
    "        )\n",
    "        db.session.add(document)\n",
    "        db.session.commit()\n",
    "        logging.info(f\"Document {filename} processed and saved to database.\")\n",
    "        return True\n",
    "    except SQLAlchemyError as e:\n",
    "        logging.error(f\"Database error: {e}\")\n",
    "        db.session.rollback()\n",
    "        return False\n",
    "\n",
    "def process_file(file_path, user_id):\n",
    "    \"\"\"Process a single file, perform OCR, and save the results.\"\"\"\n",
    "    try:\n",
    "        filename = secure_filename(os.path.basename(file_path))\n",
    "        if allowed_file(filename):\n",
    "            file_type = os.path.splitext(filename)[1][1:]\n",
    "            file_size = os.path.getsize(file_path)\n",
    "\n",
    "            credentials_path = app.config.get('GOOGLE_CLOUD_CREDENTIALS')\n",
    "            openai_api_key = app.config.get('OPENAI_API_KEY')\n",
    "            ocr_results = process_image(file_path, credentials_path, openai_api_key)\n",
    "\n",
    "            if save_document(filename, ocr_results, file_type, file_size, user_id):\n",
    "                logging.info(f\"File {filename} processed successfully.\")\n",
    "            else:\n",
    "                logging.error(f\"Failed to save document {filename} to database.\")\n",
    "        else:\n",
    "            logging.error(f\"File type not allowed: {filename}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing file {file_path}: {e}\")\n",
    "\n",
    "def ingest_files(file_paths, user_id):\n",
    "    \"\"\"Ingest multiple files concurrently using a thread pool.\"\"\"\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        for file_path in file_paths:\n",
    "            executor.submit(process_file, file_path, user_id)\n",
    "\n",
    "def load_data(file_path):\n",
    "    \"\"\"Load the dataset from the specified file path.\"\"\"\n",
    "    try:\n",
    "        data = pd.read_csv(file_path)\n",
    "        logging.info(f\"Data loaded from {file_path}\")\n",
    "        return data\n",
    "    except FileNotFoundError as e:\n",
    "        logging.error(f\"File not found: {file_path}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading data: {e}\")\n",
    "        raise\n",
    "\n",
    "def preprocess_data(data):\n",
    "    \"\"\"Preprocess the dataset for training.\"\"\"\n",
    "    try:\n",
    "        X = data.drop('FSPL (dB)', axis=1)\n",
    "        y = data['FSPL (dB)']\n",
    "        logging.info(\"Data preprocessing completed.\")\n",
    "        return X, y\n",
    "    except KeyError as e:\n",
    "        logging.error(f\"Missing key in data: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error preprocessing data: {e}\")\n",
    "        raise\n",
    "\n",
    "def train_model(X_train, y_train):\n",
    "    \"\"\"Train the model using the training data.\"\"\"\n",
    "    try:\n",
    "        param_grid = {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'max_depth': [5, 10, 20],\n",
    "            'min_samples_split': [2, 5, 10]\n",
    "        }\n",
    "        model = RandomForestRegressor(random_state=42)\n",
    "        grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring='neg_mean_squared_error', cv=5)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        best_model = grid_search.best_estimator_\n",
    "        logging.info(\"Model training completed.\")\n",
    "        return best_model\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error training model: {e}\")\n",
    "        raise\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    \"\"\"Evaluate the model using the test data.\"\"\"\n",
    "    try:\n",
    "        predictions = model.predict(X_test)\n",
    "        mse = mean_squared_error(y_test, predictions)\n",
    "        r2 = r2_score(y_test, predictions)\n",
    "        logging.info(f\"Model evaluation completed. MSE: {mse}, R-squared: {r2}\")\n",
    "        return mse, r2\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error evaluating model: {e}\")\n",
    "        raise\n",
    "\n",
    "def save_model(model, save_path):\n",
    "    \"\"\"Save the trained model to the specified path.\"\"\"\n",
    "    try:\n",
    "        dump(model, save_path)\n",
    "        logging.info(f\"Model saved to {save_path}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving model: {e}\")\n",
    "        raise\n",
    "\n",
    "def load_model(save_path):\n",
    "    \"\"\"Load the trained model from the specified path.\"\"\"\n",
    "    try:\n",
    "        model = load(save_path)\n",
    "        logging.info(f\"Model loaded from {save_path}\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading model: {e}\")\n",
    "        raise\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to orchestrate the entire process.\"\"\"\n",
    "    user_id = 'default_user_id'  # Replace with actual user ID logic\n",
    "    file_directory = '/path/to/files'  # Replace with actual file directory\n",
    "    file_paths = [os.path.join(file_directory, f) for f in os.listdir(file_directory) if allowed_file(f)]\n",
    "\n",
    "    logging.info(\"Starting file ingestion process...\")\n",
    "    ingest_files(file_paths, user_id)\n",
    "    logging.info(\"File ingestion process completed.\")\n",
    "\n",
    "    dataset_path = os.getenv('DATASET_PATH', 'fspl_dataset.csv.gz')\n",
    "    model_save_path = os.getenv('MODEL_SAVE_PATH', 'rf_model.joblib')\n",
    "\n",
    "    data = load_data(dataset_path)\n",
    "    X, y = preprocess_data(data)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    model = train_model(X_train, y_train)\n",
    "\n",
    "    mse, r2 = evaluate_model(model, X_test, y_test)\n",
    "\n",
    "    save_model(model, model_save_path)\n",
    "\n",
    "    logging.info(f\"Training process completed with MSE: {mse}, R-squared: {r2}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51692bb9-dac6-4cd5-b525-77b6e73565b9",
   "metadata": {},
   "source": [
    "### Summary of Completed Tasks:\n",
    "Basic Infrastructure Setup:\n",
    "\n",
    "Configured the Flask application with necessary modules (Flask, SQLAlchemy, Migrate, LoginManager).\n",
    "Set up configurations (config.py) for different environments (development, testing, production).\n",
    "Database Models:\n",
    "\n",
    "Created User, Document, and OcrResult models in models.py.\n",
    "Added relationships and appropriate fields for each model.\n",
    "Main Application Routes:\n",
    "\n",
    "Implemented main routes (routes.py) including endpoints for OpenAI and Google Vertex AI predictions.\n",
    "Added logging, input validation, and error handling.\n",
    "OCR and Image Processing:\n",
    "\n",
    "Developed combined_ocr_module.py for processing images with OCR and enhancing text using OpenAI.\n",
    "Integrated Google Cloud Vision and OpenAI APIs for OCR and text enhancement.\n",
    "Training and Evaluation Scripts:\n",
    "\n",
    "Created scripts for loading data, preprocessing, training, evaluating, and saving machine learning models.\n",
    "Implemented hyperparameter tuning using GridSearchCV and saved models using joblib.\n",
    "File Ingestion:\n",
    "\n",
    "Set up file ingestion with support for concurrent processing using ThreadPoolExecutor.\n",
    "Processed files, performed OCR, and saved results to the database.\n",
    "Remaining Tasks:\n",
    "Testing:\n",
    "\n",
    "Unit Tests: Develop comprehensive unit tests for each function and module.\n",
    "Integration Tests: Ensure that all components work together seamlessly.\n",
    "Deployment:\n",
    "\n",
    "Dockerization: Create Dockerfiles for containerizing the application.\n",
    "CI/CD Pipeline: Set up continuous integration and deployment pipelines.\n",
    "Cloud Deployment: Deploy the application on a cloud platform (e.g., Google Cloud, AWS).\n",
    "User Authentication and Authorization:\n",
    "\n",
    "Authentication: Implement user registration, login, and logout functionalities.\n",
    "Authorization: Ensure that users have appropriate permissions to access different resources.\n",
    "API Enhancements:\n",
    "\n",
    "Additional Endpoints: Review and enhance any additional API endpoints required for VEDAs functionality.\n",
    "API Documentation: Document all API endpoints for better usability and maintenance.\n",
    "Frontend Integration:\n",
    "\n",
    "Frontend Framework: Choose a frontend framework (e.g., React, Angular) and begin integration.\n",
    "UI/UX Design: Design user interfaces for interacting with VEDA.\n",
    "API Integration: Connect frontend components with backend APIs.\n",
    "Security Enhancements:\n",
    "\n",
    "Input Validation: Ensure all inputs are properly validated and sanitized.\n",
    "CSRF Protection: Implement CSRF protection for forms.\n",
    "Rate Limiting: Implement rate limiting to prevent abuse of the API.\n",
    "Performance Optimization:\n",
    "\n",
    "Caching: Implement caching mechanisms for frequently accessed data.\n",
    "Load Testing: Perform load testing to ensure the application can handle high traffic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b8666e-7b9d-4698-8549-43cd5867fe23",
   "metadata": {},
   "source": [
    "# Step 1: Database Design\n",
    "\n",
    "## User Management Model\n",
    "\n",
    "```python\n",
    "from sqlalchemy import Column, Integer, String, DateTime, ForeignKey, Text\n",
    "from sqlalchemy.orm import relationship\n",
    "from datetime import datetime\n",
    "from werkzeug.security import generate_password_hash, check_password_hash\n",
    "from app import db\n",
    "\n",
    "class User(db.Model):\n",
    "    __tablename__ = 'users'\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    username = Column(String(150), nullable=False, unique=True)\n",
    "    email = Column(String(150), unique=True, nullable=False)\n",
    "    password_hash = Column(String(150), nullable=False)\n",
    "    created_at = Column(DateTime, default=datetime.utcnow)\n",
    "    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)\n",
    "    documents = relationship('Document', backref='user', lazy=True)\n",
    "\n",
    "    def set_password(self, password: str) -> None:\n",
    "        \"\"\"\n",
    "        Hash and set the user's password.\n",
    "        \n",
    "        Args:\n",
    "            password (str): The password to be hashed and set.\n",
    "        \"\"\"\n",
    "        self.password_hash = generate_password_hash(password)\n",
    "\n",
    "    def check_password(self, password: str) -> bool:\n",
    "        \"\"\"\n",
    "        Check the user's password.\n",
    "        \n",
    "        Args:\n",
    "            password (str): The password to be checked.\n",
    "            \n",
    "        Returns:\n",
    "            bool: True if the password matches, False otherwise.\n",
    "        \"\"\"\n",
    "        return check_password_hash(self.password_hash, password)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f'<User {self.username}>'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e3b509-f04a-4329-b6c1-bf95d7b90448",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import Column, Integer, String, DateTime, ForeignKey, Text\n",
    "from sqlalchemy.orm import relationship\n",
    "from datetime import datetime\n",
    "from werkzeug.security import generate_password_hash, check_password_hash\n",
    "from app import db\n",
    "\n",
    "class User(db.Model):\n",
    "    __tablename__ = 'users'\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    username = Column(String(150), nullable=False, unique=True)\n",
    "    email = Column(String(150), unique=True, nullable=False)\n",
    "    password_hash = Column(String(150), nullable=False)\n",
    "    created_at = Column(DateTime, default=datetime.utcnow)\n",
    "    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)\n",
    "    documents = relationship('Document', backref='user', lazy=True)\n",
    "\n",
    "    def set_password(self, password: str) -> None:\n",
    "        \"\"\"\n",
    "        Hash and set the user's password.\n",
    "        \n",
    "        Args:\n",
    "            password (str): The password to be hashed and set.\n",
    "        \"\"\"\n",
    "        self.password_hash = generate_password_hash(password)\n",
    "\n",
    "    def check_password(self, password: str) -> bool:\n",
    "        \"\"\"\n",
    "        Check the user's password.\n",
    "        \n",
    "        Args:\n",
    "            password (str): The password to be checked.\n",
    "            \n",
    "        Returns:\n",
    "            bool: True if the password matches, False otherwise.\n",
    "        \"\"\"\n",
    "        return check_password_hash(self.password_hash, password)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f'<User {self.username}>'\n",
    "\n",
    "class Document(db.Model):\n",
    "    __tablename__ = 'documents'\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    title = Column(String(150), nullable=False)\n",
    "    content = Column(Text, nullable=False)\n",
    "    created_at = Column(DateTime, default=datetime.utcnow)\n",
    "    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)\n",
    "    file_type = Column(String(50), nullable=True)\n",
    "    file_size = Column(Integer, nullable=True)\n",
    "    user_id = Column(Integer, ForeignKey('users.id'), nullable=False)\n",
    "    file_path = Column(String(255), nullable=True)  # Added file_path attribute for file storage\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f'<Document {self.title}>'\n",
    "\n",
    "class Formula(db.Model):\n",
    "    __tablename__ = 'formulas'\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    name = Column(String(150), nullable=False)\n",
    "    description = Column(Text, nullable=True)\n",
    "    expression = Column(Text, nullable=False)\n",
    "    jurisdiction = Column(String(150), nullable=True)\n",
    "    category = Column(String(150), nullable=True)\n",
    "    tags = Column(String(255), nullable=True)\n",
    "    created_at = Column(DateTime, default=datetime.utcnow)\n",
    "    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f'<Formula {self.name}>'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738c063b-1500-4287-bd55-cbf04d4fd3bd",
   "metadata": {},
   "source": [
    "# Step 3: Database Configuration and Initialization\n",
    "\n",
    "## Database Configuration and Initialization Code\n",
    "\n",
    "```python\n",
    "from flask import Flask, render_template\n",
    "from config import config\n",
    "from flask_sqlalchemy import SQLAlchemy\n",
    "from flask_migrate import Migrate\n",
    "from flask_login import LoginManager\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# Initialize extensions\n",
    "db = SQLAlchemy()\n",
    "migrate = Migrate()\n",
    "login = LoginManager()\n",
    "login.login_view = 'auth.login'\n",
    "\n",
    "def create_app(config_name='default'):\n",
    "    app = Flask(__name__)\n",
    "    app.config.from_object(config[config_name])\n",
    "\n",
    "    # Initialize extensions\n",
    "    db.init_app(app)\n",
    "    migrate.init_app(app, db)\n",
    "    login.init_app(app)\n",
    "\n",
    "    # Configure logging\n",
    "    if not app.debug and not app.testing:\n",
    "        # Configure logging level\n",
    "        app.logger.setLevel(app.config.get('LOG_LEVEL', logging.INFO))\n",
    "        # Configure log handler\n",
    "        if app.config.get('LOG_TO_STDOUT'):\n",
    "            stream_handler = logging.StreamHandler()\n",
    "            stream_handler.setLevel(app.logger.level)\n",
    "            app.logger.addHandler(stream_handler)\n",
    "        else:\n",
    "            if not os.path.exists('logs'):\n",
    "                os.mkdir('logs')\n",
    "            file_handler = logging.FileHandler('logs/veda.log')\n",
    "            file_handler.setLevel(app.logger.level)\n",
    "            app.logger.addHandler(file_handler)\n",
    "        app.logger.info('VEDA startup')\n",
    "\n",
    "    # Register blueprints\n",
    "    from app.main import main_blueprint\n",
    "    app.register_blueprint(main_blueprint)\n",
    "\n",
    "    from app.auth import auth_blueprint\n",
    "    app.register_blueprint(auth_blueprint, url_prefix='/auth')\n",
    "\n",
    "    # Error handling\n",
    "    @app.errorhandler(404)\n",
    "    def page_not_found(error):\n",
    "        return render_template('404.html'), 404\n",
    "\n",
    "    @app.errorhandler(500)\n",
    "    def internal_server_error(error):\n",
    "        return render_template('500.html'), 500\n",
    "\n",
    "    # Additional error handlers can be added here\n",
    "\n",
    "    return app\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9eb3d43-c0ee-457a-a7e3-f48258114f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, render_template\n",
    "from config import config\n",
    "from flask_sqlalchemy import SQLAlchemy\n",
    "from flask_migrate import Migrate\n",
    "from flask_login import LoginManager\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# Initialize extensions\n",
    "db = SQLAlchemy()\n",
    "migrate = Migrate()\n",
    "login = LoginManager()\n",
    "login.login_view = 'auth.login'\n",
    "\n",
    "def create_app(config_name='default'):\n",
    "    app = Flask(__name__)\n",
    "    app.config.from_object(config[config_name])\n",
    "\n",
    "    # Initialize extensions\n",
    "    db.init_app(app)\n",
    "    migrate.init_app(app, db)\n",
    "    login.init_app(app)\n",
    "\n",
    "    # Configure logging\n",
    "    if not app.debug and not app.testing:\n",
    "        # Configure logging level\n",
    "        app.logger.setLevel(app.config.get('LOG_LEVEL', logging.INFO))\n",
    "        # Configure log handler\n",
    "        if app.config.get('LOG_TO_STDOUT'):\n",
    "            stream_handler = logging.StreamHandler()\n",
    "            stream_handler.setLevel(app.logger.level)\n",
    "            app.logger.addHandler(stream_handler)\n",
    "        else:\n",
    "            if not os.path.exists('logs'):\n",
    "                os.mkdir('logs')\n",
    "            file_handler = logging.FileHandler('logs/veda.log')\n",
    "            file_handler.setLevel(app.logger.level)\n",
    "            app.logger.addHandler(file_handler)\n",
    "        app.logger.info('VEDA startup')\n",
    "\n",
    "    # Register blueprints\n",
    "    from app.main import main_blueprint\n",
    "    app.register_blueprint(main_blueprint)\n",
    "\n",
    "    from app.auth import auth_blueprint\n",
    "    app.register_blueprint(auth_blueprint, url_prefix='/auth')\n",
    "\n",
    "    # Error handling\n",
    "    @app.errorhandler(404)\n",
    "    def page_not_found(error):\n",
    "        return render_template('404.html'), 404\n",
    "\n",
    "    @app.errorhandler(500)\n",
    "    def internal_server_error(error):\n",
    "        return render_template('500.html'), 500\n",
    "\n",
    "    # Additional error handlers can be added here\n",
    "\n",
    "    return app\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
