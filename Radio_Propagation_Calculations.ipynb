{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c7257b3-b481-4707-9c5c-7aade0efcc61",
   "metadata": {},
   "source": [
    "# Log-distance Path Loss Model\n",
    "\n",
    "## Introduction\n",
    "The log-distance path loss model is used to predict the path loss a signal encounters inside a building or densely populated areas over distance.\n",
    "\n",
    "## Parameters\n",
    "- **Frequency (MHz)**: The frequency of the transmitted signal.\n",
    "- **Distance (ft)**: The distance between the transmitter and receiver in feet.\n",
    "- **Tx Height (ft)**: The height of the transmitting antenna in feet.\n",
    "- **Rx Height (ft)**: The height of the receiving antenna in feet.\n",
    "- **Path Loss Exponent**: The path loss exponent for the environment.\n",
    "- **Tx Power (dBm)**: The transmitted power in dBm.\n",
    "\n",
    "## Calculation\n",
    "The path loss is calculated using the formula:\n",
    "\n",
    "\\[\n",
    "L_p(d) = L_p(d_0) + 10n \\log_{10}\\left(\\frac{d}{d_0}\\right)\n",
    "\\]\n",
    "\n",
    "where:\n",
    "- \\(L_p(d)\\) = path loss at distance \\(d\\) (dB)\n",
    "- \\(L_p(d_0)\\) = path loss at reference distance \\(d_0\\) (dB)\n",
    "- \\(n\\) = path loss exponent\n",
    "- \\(d\\) = distance between antennas (meters)\n",
    "- \\(d_0\\) = reference distance (meters)\n",
    "\n",
    "## Implementation\n",
    "\n",
    "### Logger Initialization\n",
    "```python\n",
    "log_distance_path_loss.init_logger()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe7ca42-883e-4d4c-bc00-f4af33db3e82",
   "metadata": {},
   "source": [
    "# Initialize the logger\n",
    "log_distance_path_loss.init_logger()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d22b4c1-30ff-4fd1-9ca2-8e8ee9f559c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the logger\n",
    "log_distance_path_loss.init_logger()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fc4d37-f653-4abd-ac81-5eb820688fa0",
   "metadata": {},
   "source": [
    "### Path Loss Calculation\n",
    "Calculate the path loss using the defined parameters, process the data in parallel, and save the results in batches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bd35f3-93c3-4c2c-a4dc-103dfffcbc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform path loss calculations\n",
    "log_distance_path_loss.calculate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c866030d-8bb5-401d-88e1-e820f0d3985b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Log-distance Path Loss Model Script\n",
    "\n",
    "# ## Introduction\n",
    "# This script calculates the path loss using the Log-distance Path Loss Model. The model is used to predict the path loss a signal encounters inside a building or densely populated areas over distance. The script processes data in batches and saves each batch with an individual naming scheme.\n",
    "\n",
    "# ## Import Libraries\n",
    "import math\n",
    "import os\n",
    "import logging\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from pathlib import Path\n",
    "from zipfile import ZipFile, ZIP_DEFLATED\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ## Path Loss Class\n",
    "class PathLoss:\n",
    "    def __init__(self, frequency, distance_ft, tx_height=None, rx_height=None, tx_power=0):\n",
    "        self.frequency = frequency\n",
    "        self.distance_ft = distance_ft\n",
    "        self.tx_height = tx_height if tx_height is not None else 0.0\n",
    "        self.rx_height = rx_height if rx_height is not None else 0.0\n",
    "        self.tx_power = tx_power  # Transmit power in dBm\n",
    "\n",
    "    @staticmethod\n",
    "    def init_logger():\n",
    "        \"\"\"Initialize logger\"\"\"\n",
    "        log_folder = \"logs\"\n",
    "        Path(log_folder).mkdir(parents=True, exist_ok=True)\n",
    "        logging.basicConfig(filename=os.path.join(log_folder, 'path_loss.log'), level=logging.DEBUG)\n",
    "        logging.debug(\"Logger initialized\")\n",
    "\n",
    "    def calculate_path_loss(self):\n",
    "        \"\"\"Calculate path loss\"\"\"\n",
    "        L0 = 20 * math.log10(self.frequency) + 20 * math.log10(1) - 27.55\n",
    "        path_loss = L0 + 10 * 3.0 * math.log10(self.distance_ft) + self.tx_power  # Assuming path loss exponent n=3.0 for indoor\n",
    "        return path_loss\n",
    "\n",
    "    def calculate(self, num_workers=None):\n",
    "        \"\"\"Calculate path loss with parallel processing\"\"\"\n",
    "        if num_workers is None:\n",
    "            num_workers = min(cpu_count(), 16)\n",
    "        logging.debug(f\"Using {num_workers} CPU cores for parallel processing\")\n",
    "\n",
    "        results = []\n",
    "        with Pool(processes=num_workers) as pool:\n",
    "            params = [(f, d, h, self.rx_height, p) for f in range(700, 3001, 50)\n",
    "                      for d in range(5, 751, 5)\n",
    "                      for h in range(0, 16)\n",
    "                      for p in range(20, 44)]\n",
    "            for result in tqdm(pool.imap_unordered(self._calculate, params), total=len(params)):\n",
    "                if result is not None:\n",
    "                    results.append(result)\n",
    "\n",
    "        self.log_results(results)\n",
    "        self.save_results(results)\n",
    "\n",
    "    def _calculate(self, params):\n",
    "        frequency, distance_ft, tx_height, rx_height, tx_power = params\n",
    "        L0 = 20 * math.log10(frequency) + 20 * math.log10(1) - 27.55\n",
    "        path_loss = L0 + 10 * 3.0 * math.log10(distance_ft) + tx_power  # Assuming path loss exponent n=3.0 for indoor\n",
    "        return {\"frequency\": frequency, \"distance_ft\": distance_ft, \"tx_height\": tx_height, \"rx_height\": rx_height, \"tx_power\": tx_power, \"path_loss\": path_loss}\n",
    "\n",
    "    @staticmethod\n",
    "    def log_results(results):\n",
    "        \"\"\"Log the results\"\"\"\n",
    "        with open(\"results_path_loss.txt\", \"a\") as f:\n",
    "            for r in results:\n",
    "                f.write(f\"{r}\\n\")\n",
    "\n",
    "    def save_results(self, results):\n",
    "        \"\"\"Save the results to CSV and compress\"\"\"\n",
    "        batch_size = 1000\n",
    "        num_batches = len(results) // batch_size + (1 if len(results) % batch_size > 0 else 0)\n",
    "        for i in range(num_batches):\n",
    "            batch_results = results[i*batch_size:(i+1)*batch_size]\n",
    "            db_folder = f\"path_loss_batches\"\n",
    "            Path(db_folder).mkdir(parents=True, exist_ok=True)\n",
    "            db_filename = f\"{db_folder}/path_loss_batch_{i}.csv\"\n",
    "\n",
    "            with open(db_filename, 'w') as csvfile:\n",
    "                header = \"frequency,distance_ft,tx_height,rx_height,tx_power,path_loss\\n\"\n",
    "                csvfile.write(header)\n",
    "\n",
    "                for r in batch_results:\n",
    "                    line = f\"{r['frequency']},{r['distance_ft']},{r['tx_height']},{r['rx_height']},{r['tx_power']},{r['path_loss']}\\n\"\n",
    "                    csvfile.write(line)\n",
    "\n",
    "            self.compress_database(db_folder, i)\n",
    "\n",
    "    @staticmethod\n",
    "    def compress_database(db_folder, batch_num):\n",
    "        \"\"\"Compress database folder\"\"\"\n",
    "        zip_filename = f\"{db_folder}/path_loss_batch_{batch_num}.zip\"\n",
    "        with ZipFile(zip_filename, \"w\", ZIP_DEFLATED) as zipf:\n",
    "            for root, _, files in os.walk(db_folder):\n",
    "                for fn in files:\n",
    "                    if fn.endswith('.csv'):\n",
    "                        file_path = os.path.join(root, fn)\n",
    "                        zipf.write(file_path, arcname=os.path.relpath(file_path, db_folder))\n",
    "        print(f\"Batch {batch_num} compressed to {zip_filename}\")\n",
    "\n",
    "# ## Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    PathLoss.init_logger()\n",
    "    path_loss = PathLoss(frequency=2400, distance_ft=100, tx_height=2, rx_height=2, tx_power=20)\n",
    "    path_loss.calculate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8410d1-025c-41fe-b4e1-9fed269f3484",
   "metadata": {},
   "source": [
    "### Compressing Database\n",
    "After generating the batches of path loss data, compress them into a zip file for easier handling and storage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7257d18b-36f2-4052-9c89-b8ef099f6aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compress the database into a zip file\n",
    "LogDistancePathLoss.compress_database()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a71f7c9-1cdf-49fc-be88-73d82ec37b30",
   "metadata": {},
   "source": [
    "# ## Path Loss Calculation Script for VEDA\n",
    "# This script calculates the path loss using the Free Space Path Loss (FSPL) model.\n",
    "# It utilizes advanced parallel processing techniques and efficient logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55567a4d-0f9c-4a31-bf34-3bd67e6d00d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "from typing import Optional, List, Tuple\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from part1 import FSPL\n",
    "\n",
    "# Validation functions\n",
    "def is_positive_float(value: float):\n",
    "    if not isinstance(value, (float, int)) or value <= 0:\n",
    "        raise ValueError(f\"Value must be a positive float. Got {value}\")\n",
    "\n",
    "def is_positive_int(value: int):\n",
    "    if not isinstance(value, int) or value <= 0:\n",
    "        raise ValueError(f\"Value must be a positive integer. Got {value}\")\n",
    "\n",
    "# Initialize logger\n",
    "def init_logger():\n",
    "    log_folder = \"logs\"\n",
    "    Path(log_folder).mkdir(parents=True, exist_ok=True)\n",
    "    logging.basicConfig(\n",
    "        filename=os.path.join(log_folder, 'pathloss.log'), \n",
    "        level=logging.DEBUG,\n",
    "        format='%(asctime)s %(levelname)s:%(message)s'\n",
    "    )\n",
    "    logging.debug(\"Logger initialized\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24e75d7-5b77-4386-96db-a51fff9b49a4",
   "metadata": {},
   "source": [
    "# Link Budget Calculation\n",
    "\n",
    "## Introduction\n",
    "The link budget calculation is essential for evaluating the feasibility of wireless communication systems. It accounts for all gains and losses from the transmitter to the receiver.\n",
    "\n",
    "## Link Budget Formula\n",
    "The received power is calculated using the equation:\n",
    "\n",
    "\\[\n",
    "P_r = P_t + G_t - L_t - L_p + G_r - L_r\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( P_r \\) = Received power (dBm)\n",
    "- \\( P_t \\) = Transmitted power (dBm)\n",
    "- \\( G_t \\) = Gain of the transmitting antenna (dBi)\n",
    "- \\( L_t \\) = Losses in the transmitter (dB)\n",
    "- \\( L_p \\) = Path loss (dB)\n",
    "- \\( G_r \\) = Gain of the receiving antenna (dBi)\n",
    "- \\( L_r \\) = Losses in the receiver (dB)\n",
    "\n",
    "## Parameters\n",
    "- **Transmitter Power**: 20 dBm to 43 dBm\n",
    "- **Frequency Range**: 700 MHz to 3000 MHz\n",
    "- **Distance Range**: 5 ft to 500 ft\n",
    "- **Antenna Gains**: Tx and Rx gains from 0 dBi to 15 dBi\n",
    "- **Losses**: Standard transmitter and receiver losses\n",
    "\n",
    "## Implementation\n",
    "\n",
    "### Initialize Logger\n",
    "```python\n",
    "import logging\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def init_logger():\n",
    "    log_folder = \"logs\"\n",
    "    Path(log_folder).mkdir(parents=True, exist_ok=True)\n",
    "    logging.basicConfig(filename=os.path.join(log_folder, 'link_budget.log'), level=logging.DEBUG)\n",
    "    logging.debug(\"Logger initialized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9877e059-0247-4e43-b7aa-4f8bc53eeabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_received_power(P_t, G_t, L_t, L_p, G_r, L_r):\n",
    "    \"\"\"\n",
    "    Calculate received power using the link budget formula.\n",
    "\n",
    "    Parameters:\n",
    "    P_t (float): Transmitted power in dBm\n",
    "    G_t (float): Gain of the transmitting antenna in dBi\n",
    "    L_t (float): Losses in the transmitter in dB\n",
    "    L_p (float): Path loss in dB\n",
    "    G_r (float): Gain of the receiving antenna in dBi\n",
    "    L_r (float): Losses in the receiver in dB\n",
    "\n",
    "    Returns:\n",
    "    float: Received power in dBm\n",
    "    \"\"\"\n",
    "    P_r = P_t + G_t - L_t - L_p + G_r - L_r\n",
    "    return P_r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b296d1-8108-4646-bb97-b1f6347d9fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_path_loss(frequency, distance_ft):\n",
    "    \"\"\"\n",
    "    Placeholder function for path loss calculation.\n",
    "    Replace with the actual path loss calculation as needed.\n",
    "    \"\"\"\n",
    "    distance_m = distance_ft * 0.3048  # Convert feet to meters\n",
    "    path_loss = 20 * np.log10(distance_m) + 20 * np.log10(frequency) - 27.55\n",
    "    return path_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94da7ac-aaf8-429c-bc7c-37b6a184e518",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def generate_link_budget_data():\n",
    "    frequencies = np.arange(700, 3001, 50)  # Frequency range from 700 MHz to 3000 MHz\n",
    "    distances = np.arange(5, 501, 5)  # Distance range from 5 ft to 500 ft\n",
    "    tx_powers = np.arange(20, 44, 1)  # Transmitter power range from 20 dBm to 43 dBm\n",
    "    tx_gains = np.arange(0, 16, 1)  # Tx gain from 0 dBi to 15 dBi\n",
    "    rx_gains = np.arange(0, 16, 1)  # Rx gain from 0 dBi to 15 dBi\n",
    "    losses_tx = 2.0  # Example transmitter losses in dB\n",
    "    losses_rx = 2.0  # Example receiver losses in dB\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for freq in frequencies:\n",
    "        for dist in tqdm(distances, desc=\"Distances\"):\n",
    "            for P_t in tx_powers:\n",
    "                for G_t in tx_gains:\n",
    "                    for G_r in rx_gains:\n",
    "                        L_p = calculate_path_loss(freq, dist)  # Calculate path loss\n",
    "                        P_r = calculate_received_power(P_t, G_t, losses_tx, L_p, G_r, losses_rx)\n",
    "                        data.append([freq, dist, P_t, G_t, losses_tx, L_p, G_r, losses_rx, P_r])\n",
    "\n",
    "    df = pd.DataFrame(data, columns=['Frequency_MHz', 'Distance_ft', 'Tx_Power_dBm', 'Tx_Gain_dBi',\n",
    "                                      'Losses_Tx_dB', 'Path_Loss_dB', 'Rx_Gain_dBi', 'Losses_Rx_dB', 'Received_Power_dBm'])\n",
    "    return df\n",
    "\n",
    "link_budget_data = generate_link_budget_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2df730b-14fb-479d-ba57-5ddabda9486c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_and_compress_data(df):\n",
    "    file_path = 'link_budget_data.csv'\n",
    "    df.to_csv(file_path, index=False)\n",
    "    # Compressing the file\n",
    "    import zipfile\n",
    "    with zipfile.ZipFile('link_budget_data.zip', 'w') as zipf:\n",
    "        zipf.write(file_path, os.path.basename(file_path))\n",
    "    os.remove(file_path)  # Remove the CSV file after compression\n",
    "    logging.info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec35ff6e-f562-4ec6-af4f-6cce3e12dbc6",
   "metadata": {},
   "source": [
    "# EIRP Calculation Script for VEDA\n",
    "# This script generates a comprehensive dataset for EIRP (Effective Isotropic Radiated Power)\n",
    "# considering a wide range of variables to ensure it's robust and effective for VEDA's learning and knowledge bank.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c387ed77-30b0-4a2f-9666-5c294fb76271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EIRP Calculation Script for VEDA\n",
    "# This script generates a comprehensive dataset for EIRP (Effective Isotropic Radiated Power)\n",
    "# considering a wide range of variables to ensure it's robust and effective for VEDA's learning and knowledge bank.\n",
    "\n",
    "# ## Import necessary libraries\n",
    "import os\n",
    "import logging\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from pathlib import Path\n",
    "from zipfile import ZipFile, ZIP_DEFLATED\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# ## Define the EIRP class\n",
    "class EIRP:\n",
    "    def __init__(self, tx_power, tx_gain, tx_loss):\n",
    "        \"\"\"Initialize the EIRP class\"\"\"\n",
    "        self.tx_power = tx_power  # in dBm\n",
    "        self.tx_gain = tx_gain    # in dBi\n",
    "        self.tx_loss = tx_loss    # in dB\n",
    "\n",
    "    @staticmethod\n",
    "    def init_logger():\n",
    "        \"\"\"Initialize logger\"\"\"\n",
    "        log_folder = \"logs\"\n",
    "        Path(log_folder).mkdir(parents=True, exist_ok=True)\n",
    "        logging.basicConfig(\n",
    "            filename=os.path.join(log_folder, 'eirp.log'),\n",
    "            level=logging.DEBUG,\n",
    "            format='%(asctime)s %(levelname)s:%(message)s'\n",
    "        )\n",
    "        logging.debug(\"Logger initialized\")\n",
    "\n",
    "    @staticmethod\n",
    "    def log_results(results):\n",
    "        \"\"\"Log the results\"\"\"\n",
    "        with open(\"results_eirp.txt\", \"a\") as f:\n",
    "            for r in results:\n",
    "                f.write(f\"{r}\\n\")\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_eirp(tx_power, tx_gain, tx_loss):\n",
    "        \"\"\"Calculate EIRP\"\"\"\n",
    "        eirp = tx_power + tx_gain - tx_loss\n",
    "        return {\"EIRP\": eirp, \"tx_power\": tx_power, \"tx_gain\": tx_gain, \"tx_loss\": tx_loss}\n",
    "\n",
    "    def calculate(self):\n",
    "        \"\"\"Calculate EIRP for a range of parameters\"\"\"\n",
    "        # Define the range of variables\n",
    "        tx_powers = np.arange(20, 44, 1)  # Transmit power from 20 dBm to 43 dBm\n",
    "        tx_gains = np.arange(0, 16, 1)    # Transmit gain from 0 dBi to 15 dBi\n",
    "        tx_losses = np.arange(0, 6, 1)    # Transmit losses from 0 dB to 5 dB\n",
    "\n",
    "        parameters = [(p, g, l) for p in tx_powers for g in tx_gains for l in tx_losses]\n",
    "        num_cpus = min(cpu_count(), 16)  # Limit to 16 cores\n",
    "        logging.debug(f\"Using {num_cpus} CPU cores for parallel processing\")\n",
    "\n",
    "        results = []\n",
    "        with Pool(processes=num_cpus) as pool:\n",
    "            for result in tqdm(pool.imap_unordered(lambda p: EIRP.calculate_eirp(*p), parameters), total=len(parameters)):\n",
    "                if result is not None:\n",
    "                    results.append(result)\n",
    "\n",
    "        self.log_results(results)\n",
    "\n",
    "        db_folder = \"db_eirp\"\n",
    "        Path(db_folder).mkdir(parents=True, exist_ok=True)\n",
    "        batch_size = 100000  # Define batch size for saving\n",
    "        for i in range(0, len(results), batch_size):\n",
    "            batch_results = results[i:i + batch_size]\n",
    "            db_filename = f\"{db_folder}/eirp_batch_{i // batch_size}.csv\"\n",
    "\n",
    "            with open(db_filename, 'w') as csvfile:\n",
    "                header = \"EIRP,tx_power,tx_gain,tx_loss\\n\"\n",
    "                csvfile.write(header)\n",
    "\n",
    "                for r in batch_results:\n",
    "                    line = f\"{r['EIRP']},{r['tx_power']},{r['tx_gain']},{r['tx_loss']}\\n\"\n",
    "                    csvfile.write(line)\n",
    "\n",
    "        self.compress_database(db_folder)b\n",
    "\n",
    "    @staticmethod\n",
    "    def compress_database(db_folder):\n",
    "        \"\"\"Compress database folder\"\"\"\n",
    "        zip_filename = f\"{db_folder}.zip\"\n",
    "        with ZipFile(zip_filename, \"w\", ZIP_DEFLATED) as zipf:\n",
    "            for root, _, files in os.walk(db_folder):\n",
    "                for fn in files:\n",
    "                    file_path = os.path.join(root, fn)\n",
    "                    zipf.write(file_path, arcname=os.path.relpath(file_path, db_folder))\n",
    "        print(f\"Database compressed to {zip_filename}\")\n",
    "\n",
    "# ## Run the EIRP Calculation\n",
    "if __name__ == \"__main__\":\n",
    "    EIRP.init_logger()\n",
    "    eirp = EIRP(tx_power=20, tx_gain=0, tx_loss=0)  # Initial values, will be overwritten by the parameter ranges\n",
    "    eirp.calculate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc04ba7-357e-492f-9fe5-c05d93570590",
   "metadata": {},
   "source": [
    "# Noise Figure Calculation Script for VEDA\n",
    "# This script generates a comprehensive dataset for Noise Figure (NF)\n",
    "# considering a wide range of variables to ensure it's robust and effective for VEDA's learning and knowledge bank.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a30c1b-d6bb-4063-ac2a-ade00897bc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noise Figure Calculation Script for VEDA\n",
    "# This script generates a comprehensive dataset for Noise Figure (NF)\n",
    "# considering a wide range of variables to ensure it's robust and effective for VEDA's learning and knowledge bank.\n",
    "\n",
    "# ## Import necessary libraries\n",
    "import os\n",
    "import logging\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from pathlib import Path\n",
    "from zipfile import ZipFile, ZIP_DEFLATED\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# ## Define the NoiseFigure class\n",
    "class NoiseFigure:\n",
    "    def __init__(self, snr_in, snr_out):\n",
    "        \"\"\"Initialize the Noise Figure class\"\"\"\n",
    "        self.snr_in = snr_in  # Signal-to-noise ratio at input (dB)\n",
    "        self.snr_out = snr_out  # Signal-to-noise ratio at output (dB)\n",
    "\n",
    "    @staticmethod\n",
    "    def init_logger():\n",
    "        \"\"\"Initialize logger\"\"\"\n",
    "        log_folder = \"logs\"\n",
    "        Path(log_folder).mkdir(parents=True, exist_ok=True)\n",
    "        logging.basicConfig(\n",
    "            filename=os.path.join(log_folder, 'nf.log'),\n",
    "            level=logging.DEBUG,\n",
    "            format='%(asctime)s %(levelname)s:%(message)s'\n",
    "        )\n",
    "        logging.debug(\"Logger initialized\")\n",
    "\n",
    "    @staticmethod\n",
    "    def log_results(results):\n",
    "        \"\"\"Log the results\"\"\"\n",
    "        with open(\"results_nf.txt\", \"a\") as f:\n",
    "            for r in results:\n",
    "                f.write(f\"{r}\\n\")\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_nf(snr_in, snr_out):\n",
    "        \"\"\"Calculate Noise Figure\"\"\"\n",
    "        nf = snr_in - snr_out\n",
    "        return {\"NF\": nf, \"snr_in\": snr_in, \"snr_out\": snr_out}\n",
    "\n",
    "    def calculate(self):\n",
    "        \"\"\"Calculate Noise Figure for a range of parameters\"\"\"\n",
    "        # Define the range of variables\n",
    "        snr_ins = np.arange(0, 30, 1)  # SNR in from 0 dB to 29 dB\n",
    "        snr_outs = np.arange(-10, 20, 1)  # SNR out from -10 dB to 19 dB\n",
    "\n",
    "        parameters = [(i, o) for i in snr_ins for o in snr_outs]\n",
    "        num_cpus = min(cpu_count(), 16)  # Limit to 16 cores\n",
    "        logging.debug(f\"Using {num_cpus} CPU cores for parallel processing\")\n",
    "\n",
    "        results = []\n",
    "        with Pool(processes=num_cpus) as pool:\n",
    "            for result in tqdm(pool.imap_unordered(lambda p: NoiseFigure.calculate_nf(*p), parameters), total=len(parameters)):\n",
    "                if result is not None:\n",
    "                    results.append(result)\n",
    "\n",
    "        self.log_results(results)\n",
    "\n",
    "        db_folder = \"db_nf\"\n",
    "        Path(db_folder).mkdir(parents=True, exist_ok=True)\n",
    "        batch_size = 100000  # Define batch size for saving\n",
    "        for i in range(0, len(results), batch_size):\n",
    "            batch_results = results[i:i + batch_size]\n",
    "            db_filename = f\"{db_folder}/nf_batch_{i // batch_size}.csv\"\n",
    "\n",
    "            with open(db_filename, 'w') as csvfile:\n",
    "                header = \"NF,snr_in,snr_out\\n\"\n",
    "                csvfile.write(header)\n",
    "\n",
    "                for r in batch_results:\n",
    "                    line = f\"{r['NF']},{r['snr_in']},{r['snr_out']}\\n\"\n",
    "                    csvfile.write(line)\n",
    "\n",
    "        self.compress_database(db_folder)\n",
    "\n",
    "    @staticmethod\n",
    "    def compress_database(db_folder):\n",
    "        \"\"\"Compress database folder\"\"\"\n",
    "        zip_filename = f\"{db_folder}.zip\"\n",
    "        with ZipFile(zip_filename, \"w\", ZIP_DEFLATED) as zipf:\n",
    "            for root, _, files in os.walk(db_folder):\n",
    "                for fn in files:\n",
    "                    file_path = os.path.join(root, fn)\n",
    "                    zipf.write(file_path, arcname=os.path.relpath(file_path, db_folder))\n",
    "        print(f\"Database compressed to {zip_filename}\")\n",
    "\n",
    "# ## Run the Noise Figure Calculation\n",
    "if __name__ == \"__main__\":\n",
    "    NoiseFigure.init_logger()\n",
    "    nf = NoiseFigure(snr_in=0, snr_out=0)  # Initial values, will be overwritten by the parameter ranges\n",
    "    nf.calculate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3773945-6bd9-407c-bc33-325af3949edf",
   "metadata": {},
   "source": [
    "# RSSI Calculation Script for VEDA\n",
    "# This script generates a comprehensive dataset for RSSI (Received Signal Strength Indicator)\n",
    "# considering a wide range of variables to ensure it's robust and effective for VEDA's learning and knowledge bank.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cb56a0-7ea9-4e9e-996e-e87f36efbacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RSSI Calculation Script for VEDA\n",
    "# This script generates a comprehensive dataset for RSSI (Received Signal Strength Indicator)\n",
    "# considering a wide range of variables to ensure it's robust and effective for VEDA's learning and knowledge bank.\n",
    "\n",
    "# ## Import necessary libraries\n",
    "import os\n",
    "import logging\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from pathlib import Path\n",
    "from zipfile import ZipFile, ZIP_DEFLATED\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# ## Define the RSSI class\n",
    "class RSSI:\n",
    "    def __init__(self, pr, path_loss, nf):\n",
    "        \"\"\"Initialize the RSSI class\"\"\"\n",
    "        self.pr = pr  # Received power (dBm)\n",
    "        self.path_loss = path_loss  # Path loss (dB)\n",
    "        self.nf = nf  # Noise figure (dB)\n",
    "\n",
    "    @staticmethod\n",
    "    def init_logger():\n",
    "        \"\"\"Initialize logger\"\"\"\n",
    "        log_folder = \"logs\"\n",
    "        Path(log_folder).mkdir(parents=True, exist_ok=True)\n",
    "        logging.basicConfig(\n",
    "            filename=os.path.join(log_folder, 'rssi.log'),\n",
    "            level=logging.DEBUG,\n",
    "            format='%(asctime)s %(levelname)s:%(message)s'\n",
    "        )\n",
    "        logging.debug(\"Logger initialized\")\n",
    "\n",
    "    @staticmethod\n",
    "    def log_results(results):\n",
    "        \"\"\"Log the results\"\"\"\n",
    "        with open(\"results_rssi.txt\", \"a\") as f:\n",
    "            for r in results:\n",
    "                f.write(f\"{r}\\n\")\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_rssi(pr, path_loss, nf):\n",
    "        \"\"\"Calculate RSSI\"\"\"\n",
    "        rssi = pr - path_loss - nf\n",
    "        return {\"RSSI\": rssi, \"pr\": pr, \"path_loss\": path_loss, \"nf\": nf}\n",
    "\n",
    "    def calculate(self):\n",
    "        \"\"\"Calculate RSSI for a range of parameters\"\"\"\n",
    "        # Define the range of variables\n",
    "        pr_values = np.arange(-100, 0, 1)  # Received power from -100 dBm to -1 dBm\n",
    "        path_loss_values = np.arange(0, 150, 1)  # Path loss from 0 dB to 149 dB\n",
    "        nf_values = np.arange(0, 10, 0.5)  # Noise figure from 0 dB to 9.5 dB\n",
    "\n",
    "        parameters = [(p, pl, n) for p in pr_values for pl in path_loss_values for n in nf_values]\n",
    "        num_cpus = min(cpu_count(), 16)  # Limit to 16 cores\n",
    "        logging.debug(f\"Using {num_cpus} CPU cores for parallel processing\")\n",
    "\n",
    "        results = []\n",
    "        with Pool(processes=num_cpus) as pool:\n",
    "            for result in tqdm(pool.imap_unordered(lambda p: RSSI.calculate_rssi(*p), parameters), total=len(parameters)):\n",
    "                if result is not None:\n",
    "                    results.append(result)\n",
    "\n",
    "        self.log_results(results)\n",
    "\n",
    "        db_folder = \"db_rssi\"\n",
    "        Path(db_folder).mkdir(parents=True, exist_ok=True)\n",
    "        batch_size = 100000  # Define batch size for saving\n",
    "        for i in range(0, len(results), batch_size):\n",
    "            batch_results = results[i:i + batch_size]\n",
    "            db_filename = f\"{db_folder}/rssi_batch_{i // batch_size}.csv\"\n",
    "\n",
    "            with open(db_filename, 'w') as csvfile:\n",
    "                header = \"RSSI,pr,path_loss,nf\\n\"\n",
    "                csvfile.write(header)\n",
    "\n",
    "                for r in batch_results:\n",
    "                    line = f\"{r['RSSI']},{r['pr']},{r['path_loss']},{r['nf']}\\n\"\n",
    "                    csvfile.write(line)\n",
    "\n",
    "        self.compress_database(db_folder)\n",
    "\n",
    "    @staticmethod\n",
    "    def compress_database(db_folder):\n",
    "        \"\"\"Compress database folder\"\"\"\n",
    "        zip_filename = f\"{db_folder}.zip\"\n",
    "        with ZipFile(zip_filename, \"w\", ZIP_DEFLATED) as zipf:\n",
    "            for root, _, files in os.walk(db_folder):\n",
    "                for fn in files:\n",
    "                    file_path = os.path.join(root, fn)\n",
    "                    zipf.write(file_path, arcname=os.path.relpath(file_path, db_folder))\n",
    "        print(f\"Database compressed to {zip_filename}\")\n",
    "\n",
    "# ## Run the RSSI Calculation\n",
    "if __name__ == \"__main__\":\n",
    "    RSSI.init_logger()\n",
    "    rssi = RSSI(pr=-50, path_loss=50, nf=5)  # Initial values, will be overwritten by the parameter ranges\n",
    "    rssi.calculate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a8916c-83cb-4acd-b5b9-0bb7de96adec",
   "metadata": {},
   "source": [
    "# SNR Calculation Script for VEDA\n",
    "# This script generates a comprehensive dataset for SNR (Signal-to-Noise Ratio)\n",
    "# considering a wide range of variables to ensure it's robust and effective for VEDA's learning and knowledge bank.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80052e33-96b1-4f25-83d8-2e64104e883d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SNR Calculation Script for VEDA\n",
    "# This script generates a comprehensive dataset for SNR (Signal-to-Noise Ratio)\n",
    "# considering a wide range of variables to ensure it's robust and effective for VEDA's learning and knowledge bank.\n",
    "\n",
    "# ## Import necessary libraries\n",
    "import os\n",
    "import logging\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from pathlib import Path\n",
    "from zipfile import ZipFile, ZIP_DEFLATED\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# ## Define the SNR class\n",
    "class SNR:\n",
    "    def __init__(self, p_signal, p_noise):\n",
    "        \"\"\"Initialize the SNR class\"\"\"\n",
    "        self.p_signal = p_signal  # Signal power (dBm)\n",
    "        self.p_noise = p_noise  # Noise power (dBm)\n",
    "\n",
    "    @staticmethod\n",
    "    def init_logger():\n",
    "        \"\"\"Initialize logger\"\"\"\n",
    "        log_folder = \"logs\"\n",
    "        Path(log_folder).mkdir(parents=True, exist_ok=True)\n",
    "        logging.basicConfig(\n",
    "            filename=os.path.join(log_folder, 'snr.log'),\n",
    "            level=logging.DEBUG,\n",
    "            format='%(asctime)s %(levelname)s:%(message)s'\n",
    "        )\n",
    "        logging.debug(\"Logger initialized\")\n",
    "\n",
    "    @staticmethod\n",
    "    def log_results(results):\n",
    "        \"\"\"Log the results\"\"\"\n",
    "        with open(\"results_snr.txt\", \"a\") as f:\n",
    "            for r in results:\n",
    "                f.write(f\"{r}\\n\")\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_snr(p_signal, p_noise):\n",
    "        \"\"\"Calculate SNR\"\"\"\n",
    "        snr = p_signal - p_noise\n",
    "        return {\"SNR\": snr, \"p_signal\": p_signal, \"p_noise\": p_noise}\n",
    "\n",
    "    def calculate(self):\n",
    "        \"\"\"Calculate SNR for a range of parameters\"\"\"\n",
    "        # Define the range of variables\n",
    "        p_signal_values = np.arange(-100, 50, 1)  # Signal power from -100 dBm to 49 dBm\n",
    "        p_noise_values = np.arange(-150, -50, 1)  # Noise power from -150 dBm to -51 dBm\n",
    "\n",
    "        parameters = [(s, n) for s in p_signal_values for n in p_noise_values]\n",
    "        num_cpus = min(cpu_count(), 16)  # Limit to 16 cores\n",
    "        logging.debug(f\"Using {num_cpus} CPU cores for parallel processing\")\n",
    "\n",
    "        results = []\n",
    "        with Pool(processes=num_cpus) as pool:\n",
    "            for result in tqdm(pool.imap_unordered(lambda p: SNR.calculate_snr(*p), parameters), total=len(parameters)):\n",
    "                if result is not None:\n",
    "                    results.append(result)\n",
    "\n",
    "        self.log_results(results)\n",
    "\n",
    "        db_folder = \"db_snr\"\n",
    "        Path(db_folder).mkdir(parents=True, exist_ok=True)\n",
    "        batch_size = 100000  # Define batch size for saving\n",
    "        for i in range(0, len(results), batch_size):\n",
    "            batch_results = results[i:i + batch_size]\n",
    "            db_filename = f\"{db_folder}/snr_batch_{i // batch_size}.csv\"\n",
    "\n",
    "            with open(db_filename, 'w') as csvfile:\n",
    "                header = \"SNR,p_signal,p_noise\\n\"\n",
    "                csvfile.write(header)\n",
    "\n",
    "                for r in batch_results:\n",
    "                    line = f\"{r['SNR']},{r['p_signal']},{r['p_noise']}\\n\"\n",
    "                    csvfile.write(line)\n",
    "\n",
    "        self.compress_database(db_folder)\n",
    "\n",
    "    @staticmethod\n",
    "    def compress_database(db_folder):\n",
    "        \"\"\"Compress database folder\"\"\"\n",
    "        zip_filename = f\"{db_folder}.zip\"\n",
    "        with ZipFile(zip_filename, \"w\", ZIP_DEFLATED) as zipf:\n",
    "            for root, _, files in os.walk(db_folder):\n",
    "                for fn in files:\n",
    "                    file_path = os.path.join(root, fn)\n",
    "                    zipf.write(file_path, arcname=os.path.relpath(file_path, db_folder))\n",
    "        print(f\"Database compressed to {zip_filename}\")\n",
    "\n",
    "# ## Run the SNR Calculation\n",
    "if __name__ == \"__main__\":\n",
    "    SNR.init_logger()\n",
    "    snr = SNR(p_signal=-50, p_noise=-100)  # Initial values, will be overwritten by the parameter ranges\n",
    "    snr.calculate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4d1a88-4250-472c-83a3-72f2b6a431e4",
   "metadata": {},
   "source": [
    "# Friis Transmission Equation Script for VEDA\n",
    "# This script generates a comprehensive dataset for the Friis Transmission Equation considering various environments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60209db9-b7a1-43cd-9f69-378b7f70da5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Friis Transmission Equation Script for VEDA\n",
    "# This script generates a comprehensive dataset for the Friis Transmission Equation considering various environments.\n",
    "\n",
    "# ## Import necessary libraries\n",
    "import os\n",
    "import logging\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from pathlib import Path\n",
    "from zipfile import ZipFile, ZIP_DEFLATED\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# ## Define the Friis class\n",
    "class Friis:\n",
    "    def __init__(self, p_tx, g_tx, g_rx, l_tx, distance, frequency, environment):\n",
    "        \"\"\"Initialize the Friis class\"\"\"\n",
    "        self.p_tx = p_tx  # Transmitted power (dBm)\n",
    "        self.g_tx = g_tx  # Gain of the transmitting antenna (dBi)\n",
    "        self.g_rx = g_rx  # Gain of the receiving antenna (dBi)\n",
    "        self.l_tx = l_tx  # Losses in the transmitter (dB)\n",
    "        self.distance = distance  # Distance (m)\n",
    "        self.frequency = frequency  # Frequency (MHz)\n",
    "        self.environment = environment  # Environment type\n",
    "\n",
    "    @staticmethod\n",
    "    def init_logger():\n",
    "        \"\"\"Initialize logger\"\"\"\n",
    "        log_folder = \"logs\"\n",
    "        Path(log_folder).mkdir(parents=True, exist_ok=True)\n",
    "        logging.basicConfig(\n",
    "            filename=os.path.join(log_folder, 'friis.log'),\n",
    "            level=logging.DEBUG,\n",
    "            format='%(asctime)s %(levelname)s:%(message)s'\n",
    "        )\n",
    "        logging.debug(\"Logger initialized\")\n",
    "\n",
    "    @staticmethod\n",
    "    def log_results(results):\n",
    "        \"\"\"Log the results\"\"\"\n",
    "        with open(\"results_friis.txt\", \"a\") as f:\n",
    "            for r in results:\n",
    "                f.write(f\"{r}\\n\")\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_friis(p_tx, g_tx, g_rx, l_tx, distance, frequency, environment):\n",
    "        \"\"\"Calculate Friis Transmission Equation\"\"\"\n",
    "        c = 3 * 10**8  # Speed of light in m/s\n",
    "        lambda_ = c / (frequency * 10**6)  # Wavelength in meters\n",
    "\n",
    "        # Environment factor\n",
    "        if environment == 'urban':\n",
    "            path_loss_exponent = 2.7\n",
    "        elif environment == 'suburban':\n",
    "            path_loss_exponent = 2.2\n",
    "        else:  # rural\n",
    "            path_loss_exponent = 1.8\n",
    "\n",
    "        # Friis transmission equation with environment factor\n",
    "        l_p = 20 * np.log10(distance / lambda_) + 10 * path_loss_exponent * np.log10(distance)\n",
    "        p_r = p_tx + g_tx + g_rx - l_tx - l_p\n",
    "\n",
    "        return {\"p_r\": p_r, \"p_tx\": p_tx, \"g_tx\": g_tx, \"g_rx\": g_rx, \"l_tx\": l_tx, \"distance\": distance, \"frequency\": frequency, \"environment\": environment}\n",
    "\n",
    "    def calculate(self):\n",
    "        \"\"\"Calculate Friis Transmission Equation for a range of parameters\"\"\"\n",
    "        # Define the range of variables\n",
    "        p_tx_values = np.arange(0, 50, 1)  # Transmitted power from 0 dBm to 49 dBm\n",
    "        g_tx_values = np.arange(0, 15, 1)  # Transmitting antenna gain from 0 dBi to 14 dBi\n",
    "        g_rx_values = np.arange(0, 15, 1)  # Receiving antenna gain from 0 dBi to 14 dBi\n",
    "        l_tx_values = np.arange(0, 5, 1)  # Transmitter losses from 0 dB to 4 dB\n",
    "        distance_values = np.arange(1, 1001, 10)  # Distance from 1 meter to 1000 meters\n",
    "        frequency_values = np.arange(700, 3001, 50)  # Frequency from 700 MHz to 3000 MHz\n",
    "        environment_values = ['urban', 'suburban', 'rural']  # Environment types\n",
    "\n",
    "        parameters = [(p, g_t, g_r, l_t, d, f, e) for p in p_tx_values for g_t in g_tx_values for g_r in g_rx_values for l_t in l_tx_values for d in distance_values for f in frequency_values for e in environment_values]\n",
    "        num_cpus = min(cpu_count(), 16)  # Limit to 16 cores\n",
    "        logging.debug(f\"Using {num_cpus} CPU cores for parallel processing\")\n",
    "\n",
    "        results = []\n",
    "        with Pool(processes=num_cpus) as pool:\n",
    "            for result in tqdm(pool.imap_unordered(lambda p: Friis.calculate_friis(*p), parameters), total=len(parameters)):\n",
    "                if result is not None:\n",
    "                    results.append(result)\n",
    "\n",
    "        self.log_results(results)\n",
    "\n",
    "        db_folder = \"db_friis\"\n",
    "        Path(db_folder).mkdir(parents=True, exist_ok=True)\n",
    "        batch_size = 100000  # Define batch size for saving\n",
    "        for i in range(0, len(results), batch_size):\n",
    "            batch_results = results[i:i + batch_size]\n",
    "            db_filename = f\"{db_folder}/friis_batch_{i // batch_size}.csv\"\n",
    "\n",
    "            with open(db_filename, 'w') as csvfile:\n",
    "                header = \"p_r,p_tx,g_tx,g_rx,l_tx,distance,frequency,environment\\n\"\n",
    "                csvfile.write(header)\n",
    "\n",
    "                for r in batch_results:\n",
    "                    line = f\"{r['p_r']},{r['p_tx']},{r['g_tx']},{r['g_rx']},{r['l_tx']},{r['distance']},{r['frequency']},{r['environment']}\\n\"\n",
    "                    csvfile.write(line)\n",
    "\n",
    "        self.compress_database(db_folder)\n",
    "\n",
    "    @staticmethod\n",
    "    def compress_database(db_folder):\n",
    "        \"\"\"Compress database folder\"\"\"\n",
    "        zip_filename = f\"{db_folder}.zip\"\n",
    "        with ZipFile(zip_filename, \"w\", ZIP_DEFLATED) as zipf:\n",
    "            for root, _, files in os.walk(db_folder):\n",
    "                for fn in files:\n",
    "                    file_path = os.path.join(root, fn)\n",
    "                    zipf.write(file_path, arcname=os.path.relpath(file_path, db_folder))\n",
    "        print(f\"Database compressed to {zip_filename}\")\n",
    "\n",
    "# ## Run the Friis Calculation\n",
    "if __name__ == \"__main__\":\n",
    "    Friis.init_logger()\n",
    "    friis = Friis(p_tx=20, g_tx=10, g_rx=10, l_tx=2, distance=100, frequency=2400, environment='urban')  # Initial values, will be overwritten by the parameter ranges\n",
    "    friis.calculate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e44bd79-9352-44df-b347-91779f7fa8ef",
   "metadata": {},
   "source": [
    "# Doppler Shift Calculation Script\n",
    "# This script generates a comprehensive dataset for Doppler Shift considering various scenarios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e31cc51-6de6-4172-b74e-dddb7b042cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doppler Shift Calculation Script for VEDA\n",
    "# This script generates a comprehensive dataset for Doppler Shift considering various scenarios.\n",
    "\n",
    "# ## Import necessary libraries\n",
    "import os\n",
    "import logging\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from pathlib import Path\n",
    "from zipfile import ZipFile, ZIP_DEFLATED\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# ## Define the DopplerShift class\n",
    "class DopplerShift:\n",
    "    def __init__(self, frequency, velocity, angle):\n",
    "        \"\"\"Initialize the DopplerShift class\"\"\"\n",
    "        self.frequency = frequency  # Frequency in MHz\n",
    "        self.velocity = velocity  # Velocity in m/s\n",
    "        self.angle = angle  # Angle in degrees\n",
    "\n",
    "    @staticmethod\n",
    "    def init_logger():\n",
    "        \"\"\"Initialize logger\"\"\"\n",
    "        log_folder = \"logs\"\n",
    "        Path(log_folder).mkdir(parents=True, exist_ok=True)\n",
    "        logging.basicConfig(\n",
    "            filename=os.path.join(log_folder, 'doppler_shift.log'),\n",
    "            level=logging.DEBUG,\n",
    "            format='%(asctime)s %(levelname)s:%(message)s'\n",
    "        )\n",
    "        logging.debug(\"Logger initialized\")\n",
    "\n",
    "    @staticmethod\n",
    "    def log_results(results):\n",
    "        \"\"\"Log the results\"\"\"\n",
    "        with open(\"results_doppler_shift.txt\", \"a\") as f:\n",
    "            for r in results:\n",
    "                f.write(f\"{r}\\n\")\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_doppler_shift(frequency, velocity, angle):\n",
    "        \"\"\"Calculate Doppler Shift\"\"\"\n",
    "        c = 3 * 10**8  # Speed of light in m/s\n",
    "        # Convert angle to radians\n",
    "        angle_rad = np.deg2rad(angle)\n",
    "        # Doppler shift formula\n",
    "        doppler_shift = (velocity * np.cos(angle_rad) / c) * frequency * 10**6  # Shift in Hz\n",
    "\n",
    "        return {\"frequency\": frequency, \"velocity\": velocity, \"angle\": angle, \"doppler_shift\": doppler_shift}\n",
    "\n",
    "    def calculate(self):\n",
    "        \"\"\"Calculate Doppler Shift for a range of parameters\"\"\"\n",
    "        # Define the range of variables\n",
    "        frequency_values = np.arange(700, 3001, 50)  # Frequency from 700 MHz to 3000 MHz\n",
    "        velocity_values = np.arange(0, 101, 1)  # Velocity from 0 m/s to 100 m/s\n",
    "        angle_values = np.arange(0, 181, 1)  # Angle from 0 to 180 degrees\n",
    "\n",
    "        parameters = [(f, v, a) for f in frequency_values for v in velocity_values for a in angle_values]\n",
    "        num_cpus = min(cpu_count(), 16)  # Limit to 16 cores\n",
    "        logging.debug(f\"Using {num_cpus} CPU cores for parallel processing\")\n",
    "\n",
    "        results = []\n",
    "        with Pool(processes=num_cpus) as pool:\n",
    "            for result in tqdm(pool.imap_unordered(lambda p: DopplerShift.calculate_doppler_shift(*p), parameters), total=len(parameters)):\n",
    "                if result is not None:\n",
    "                    results.append(result)\n",
    "\n",
    "        self.log_results(results)\n",
    "\n",
    "        db_folder = \"db_doppler_shift\"\n",
    "        Path(db_folder).mkdir(parents=True, exist_ok=True)\n",
    "        batch_size = 100000  # Define batch size for saving\n",
    "        for i in range(0, len(results), batch_size):\n",
    "            batch_results = results[i:i + batch_size]\n",
    "            db_filename = f\"{db_folder}/doppler_shift_batch_{i // batch_size}.csv\"\n",
    "\n",
    "            with open(db_filename, 'w') as csvfile:\n",
    "                header = \"frequency,velocity,angle,doppler_shift\\n\"\n",
    "                csvfile.write(header)\n",
    "\n",
    "                for r in batch_results:\n",
    "                    line = f\"{r['frequency']},{r['velocity']},{r['angle']},{r['doppler_shift']}\\n\"\n",
    "                    csvfile.write(line)\n",
    "\n",
    "        self.compress_database(db_folder)\n",
    "\n",
    "    @staticmethod\n",
    "    def compress_database(db_folder):\n",
    "        \"\"\"Compress database folder\"\"\"\n",
    "        zip_filename = f\"{db_folder}.zip\"\n",
    "        with ZipFile(zip_filename, \"w\", ZIP_DEFLATED) as zipf:\n",
    "            for root, _, files in os.walk(db_folder):\n",
    "                for fn in files:\n",
    "                    file_path = os.path.join(root, fn)\n",
    "                    zipf.write(file_path, arcname=os.path.relpath(file_path, db_folder))\n",
    "        print(f\"Database compressed to {zip_filename}\")\n",
    "\n",
    "# ## Run the Doppler Shift Calculation\n",
    "if __name__ == \"__main__\":\n",
    "    DopplerShift.init_logger()\n",
    "    doppler_shift = DopplerShift(frequency=2400, velocity=10, angle=45)  # Initial values, will be overwritten by the parameter ranges\n",
    "    doppler_shift.calculate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4825c04b-e9b1-473b-b605-2145e30ff810",
   "metadata": {},
   "source": [
    "# Interference-to-Noise Ratio (INR) Calculation Script for VEDA\n",
    "# This script generates a comprehensive dataset for INR considering various interference and noise levels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a358540-5e9f-4819-a9c2-4aaaafd0589e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interference-to-Noise Ratio (INR) Calculation Script for VEDA\n",
    "# This script generates a comprehensive dataset for INR considering various interference and noise levels.\n",
    "\n",
    "# ## Import necessary libraries\n",
    "import os\n",
    "import logging\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from pathlib import Path\n",
    "from zipfile import ZipFile, ZIP_DEFLATED\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# ## Define the INR class\n",
    "class INR:\n",
    "    def __init__(self, interference_power, noise_power):\n",
    "        \"\"\"Initialize the INR class\"\"\"\n",
    "        self.interference_power = interference_power  # Interference power in dBm\n",
    "        self.noise_power = noise_power  # Noise power in dBm\n",
    "\n",
    "    @staticmethod\n",
    "    def init_logger():\n",
    "        \"\"\"Initialize logger\"\"\"\n",
    "        log_folder = \"logs\"\n",
    "        Path(log_folder).mkdir(parents=True, exist_ok=True)\n",
    "        logging.basicConfig(\n",
    "            filename=os.path.join(log_folder, 'inr.log'),\n",
    "            level=logging.DEBUG,\n",
    "            format='%(asctime)s %(levelname)s:%(message)s'\n",
    "        )\n",
    "        logging.debug(\"Logger initialized\")\n",
    "\n",
    "    @staticmethod\n",
    "    def log_results(results):\n",
    "        \"\"\"Log the results\"\"\"\n",
    "        with open(\"results_inr.txt\", \"a\") as f:\n",
    "            for r in results:\n",
    "                f.write(f\"{r}\\n\")\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_inr(interference_power, noise_power):\n",
    "        \"\"\"Calculate Interference-to-Noise Ratio (INR)\"\"\"\n",
    "        inr = interference_power - noise_power  # INR in dB\n",
    "\n",
    "        return {\"interference_power\": interference_power, \"noise_power\": noise_power, \"inr\": inr}\n",
    "\n",
    "    def calculate(self):\n",
    "        \"\"\"Calculate INR for a range of parameters\"\"\"\n",
    "        # Define the range of variables\n",
    "        interference_power_values = np.arange(-100, 50, 1)  # Interference power from -100 dBm to 49 dBm\n",
    "        noise_power_values = np.arange(-174, -50, 1)  # Noise power from -174 dBm to -51 dBm\n",
    "\n",
    "        parameters = [(i, n) for i in interference_power_values for n in noise_power_values]\n",
    "        num_cpus = min(cpu_count(), 16)  # Limit to 16 cores\n",
    "        logging.debug(f\"Using {num_cpus} CPU cores for parallel processing\")\n",
    "\n",
    "        results = []\n",
    "        with Pool(processes=num_cpus) as pool:\n",
    "            for result in tqdm(pool.imap_unordered(lambda p: INR.calculate_inr(*p), parameters), total=len(parameters)):\n",
    "                if result is not None:\n",
    "                    results.append(result)\n",
    "\n",
    "        self.log_results(results)\n",
    "\n",
    "        db_folder = \"db_inr\"\n",
    "        Path(db_folder).mkdir(parents=True, exist_ok=True)\n",
    "        batch_size = 100000  # Define batch size for saving\n",
    "        for i in range(0, len(results), batch_size):\n",
    "            batch_results = results[i:i + batch_size]\n",
    "            db_filename = f\"{db_folder}/inr_batch_{i // batch_size}.csv\"\n",
    "\n",
    "            with open(db_filename, 'w') as csvfile:\n",
    "                header = \"interference_power,noise_power,inr\\n\"\n",
    "                csvfile.write(header)\n",
    "\n",
    "                for r in batch_results:\n",
    "                    line = f\"{r['interference_power']},{r['noise_power']},{r['inr']}\\n\"\n",
    "                    csvfile.write(line)\n",
    "\n",
    "        self.compress_database(db_folder)\n",
    "\n",
    "    @staticmethod\n",
    "    def compress_database(db_folder):\n",
    "        \"\"\"Compress database folder\"\"\"\n",
    "        zip_filename = f\"{db_folder}.zip\"\n",
    "        with ZipFile(zip_filename, \"w\", ZIP_DEFLATED) as zipf:\n",
    "            for root, _, files in os.walk(db_folder):\n",
    "                for fn in files:\n",
    "                    file_path = os.path.join(root, fn)\n",
    "                    zipf.write(file_path, arcname=os.path.relpath(file_path, db_folder))\n",
    "        print(f\"Database compressed to {zip_filename}\")\n",
    "\n",
    "# ## Run the INR Calculation\n",
    "if __name__ == \"__main__\":\n",
    "    INR.init_logger()\n",
    "    inr = INR(interference_power=-50, noise_power=-100)  # Initial values, will be overwritten by the parameter ranges\n",
    "    inr.calculate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6029a0f5-d11e-4b22-b451-f5ff41fae71a",
   "metadata": {},
   "source": [
    "# Carrier-to-Interference Ratio (CIR) Calculation Script for VEDA\n",
    "# This script generates a comprehensive dataset for CIR considering various carrier and interference levels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e69104-6f5a-4409-b910-504bc31ef09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrier-to-Interference Ratio (CIR) Calculation Script for VEDA\n",
    "# This script generates a comprehensive dataset for CIR considering various carrier and interference levels.\n",
    "\n",
    "# ## Import necessary libraries\n",
    "import os\n",
    "import logging\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from pathlib import Path\n",
    "from zipfile import ZipFile, ZIP_DEFLATED\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# ## Define the CIR class\n",
    "class CIR:\n",
    "    def __init__(self, carrier_power, interference_power):\n",
    "        \"\"\"Initialize the CIR class\"\"\"\n",
    "        self.carrier_power = carrier_power  # Carrier power in dBm\n",
    "        self.interference_power = interference_power  # Interference power in dBm\n",
    "\n",
    "    @staticmethod\n",
    "    def init_logger():\n",
    "        \"\"\"Initialize logger\"\"\"\n",
    "        log_folder = \"logs\"\n",
    "        Path(log_folder).mkdir(parents=True, exist_ok=True)\n",
    "        logging.basicConfig(\n",
    "            filename=os.path.join(log_folder, 'cir.log'),\n",
    "            level=logging.DEBUG,\n",
    "            format='%(asctime)s %(levelname)s:%(message)s'\n",
    "        )\n",
    "        logging.debug(\"Logger initialized\")\n",
    "\n",
    "    @staticmethod\n",
    "    def log_results(results):\n",
    "        \"\"\"Log the results\"\"\"\n",
    "        with open(\"results_cir.txt\", \"a\") as f:\n",
    "            for r in results:\n",
    "                f.write(f\"{r}\\n\")\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_cir(carrier_power, interference_power):\n",
    "        \"\"\"Calculate Carrier-to-Interference Ratio (CIR)\"\"\"\n",
    "        cir = carrier_power - interference_power  # CIR in dB\n",
    "\n",
    "        return {\"carrier_power\": carrier_power, \"interference_power\": interference_power, \"cir\": cir}\n",
    "\n",
    "    def calculate(self):\n",
    "        \"\"\"Calculate CIR for a range of parameters\"\"\"\n",
    "        # Define the range of variables\n",
    "        carrier_power_values = np.arange(0, 50, 1)  # Carrier power from 0 dBm to 49 dBm\n",
    "        interference_power_values = np.arange(-100, 50, 1)  # Interference power from -100 dBm to 49 dBm\n",
    "\n",
    "        parameters = [(c, i) for c in carrier_power_values for i in interference_power_values]\n",
    "        num_cpus = min(cpu_count(), 16)  # Limit to 16 cores\n",
    "        logging.debug(f\"Using {num_cpus} CPU cores for parallel processing\")\n",
    "\n",
    "        results = []\n",
    "        with Pool(processes=num_cpus) as pool:\n",
    "            for result in tqdm(pool.imap_unordered(lambda p: CIR.calculate_cir(*p), parameters), total=len(parameters)):\n",
    "                if result is not None:\n",
    "                    results.append(result)\n",
    "\n",
    "        self.log_results(results)\n",
    "\n",
    "        db_folder = \"db_cir\"\n",
    "        Path(db_folder).mkdir(parents=True, exist_ok=True)\n",
    "        batch_size = 100000  # Define batch size for saving\n",
    "        for i in range(0, len(results), batch_size):\n",
    "            batch_results = results[i:i + batch_size]\n",
    "            db_filename = f\"{db_folder}/cir_batch_{i // batch_size}.csv\"\n",
    "\n",
    "            with open(db_filename, 'w') as csvfile:\n",
    "                header = \"carrier_power,interference_power,cir\\n\"\n",
    "                csvfile.write(header)\n",
    "\n",
    "                for r in batch_results:\n",
    "                    line = f\"{r['carrier_power']},{r['interference_power']},{r['cir']}\\n\"\n",
    "                    csvfile.write(line)\n",
    "\n",
    "        self.compress_database(db_folder)\n",
    "\n",
    "    @staticmethod\n",
    "    def compress_database(db_folder):\n",
    "        \"\"\"Compress database folder\"\"\"\n",
    "        zip_filename = f\"{db_folder}.zip\"\n",
    "        with ZipFile(zip_filename, \"w\", ZIP_DEFLATED) as zipf:\n",
    "            for root, _, files in os.walk(db_folder):\n",
    "                for fn in files:\n",
    "                    file_path = os.path.join(root, fn)\n",
    "                    zipf.write(file_path, arcname=os.path.relpath(file_path, db_folder))\n",
    "        print(f\"Database compressed to {zip_filename}\")\n",
    "\n",
    "# ## Run the CIR Calculation\n",
    "if __name__ == \"__main__\":\n",
    "    CIR.init_logger()\n",
    "    cir = CIR(carrier_power=30, interference_power=-50)  # Initial values, will be overwritten by the parameter ranges\n",
    "    cir.calculate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c7c5b7-526c-4928-b025-f06c43502fab",
   "metadata": {},
   "source": [
    "# Propagation Delay Calculation Script for VEDA\n",
    "# This script generates a comprehensive dataset for propagation delay considering various distances and velocities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1e0000-f5b9-4409-9b68-097759bf8dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Propagation Delay Calculation Script for VEDA\n",
    "# This script generates a comprehensive dataset for propagation delay considering various distances and velocities.\n",
    "\n",
    "# ## Import necessary libraries\n",
    "import os\n",
    "import logging\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from pathlib import Path\n",
    "from zipfile import ZipFile, ZIP_DEFLATED\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# ## Define the PropagationDelay class\n",
    "class PropagationDelay:\n",
    "    def __init__(self, distance, velocity):\n",
    "        \"\"\"Initialize the PropagationDelay class\"\"\"\n",
    "        self.distance = distance  # Distance in meters\n",
    "        self.velocity = velocity  # Velocity in m/s\n",
    "\n",
    "    @staticmethod\n",
    "    def init_logger():\n",
    "        \"\"\"Initialize logger\"\"\"\n",
    "        log_folder = \"logs\"\n",
    "        Path(log_folder).mkdir(parents=True, exist_ok=True)\n",
    "        logging.basicConfig(\n",
    "            filename=os.path.join(log_folder, 'propagation_delay.log'),\n",
    "            level=logging.DEBUG,\n",
    "            format='%(asctime)s %(levelname)s:%(message)s'\n",
    "        )\n",
    "        logging.debug(\"Logger initialized\")\n",
    "\n",
    "    @staticmethod\n",
    "    def log_results(results):\n",
    "        \"\"\"Log the results\"\"\"\n",
    "        with open(\"results_propagation_delay.txt\", \"a\") as f:\n",
    "            for r in results:\n",
    "                f.write(f\"{r}\\n\")\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_propagation_delay(distance, velocity):\n",
    "        \"\"\"Calculate Propagation Delay\"\"\"\n",
    "        delay = distance / velocity  # Delay in seconds\n",
    "\n",
    "        return {\"distance\": distance, \"velocity\": velocity, \"delay\": delay}\n",
    "\n",
    "    def calculate(self):\n",
    "        \"\"\"Calculate Propagation Delay for a range of parameters\"\"\"\n",
    "        # Define the range of variables\n",
    "        distance_values = np.arange(1, 10001, 10)  # Distance from 1 meter to 10,000 meters\n",
    "        velocity_values = np.arange(1, 300001, 10)  # Velocity from 1 m/s to 300,000 m/s (speed of light in vacuum)\n",
    "\n",
    "        parameters = [(d, v) for d in distance_values for v in velocity_values]\n",
    "        num_cpus = min(cpu_count(), 16)  # Limit to 16 cores\n",
    "        logging.debug(f\"Using {num_cpus} CPU cores for parallel processing\")\n",
    "\n",
    "        results = []\n",
    "        with Pool(processes=num_cpus) as pool:\n",
    "            for result in tqdm(pool.imap_unordered(lambda p: PropagationDelay.calculate_propagation_delay(*p), parameters), total=len(parameters)):\n",
    "                if result is not None:\n",
    "                    results.append(result)\n",
    "\n",
    "        self.log_results(results)\n",
    "\n",
    "        db_folder = \"db_propagation_delay\"\n",
    "        Path(db_folder).mkdir(parents=True, exist_ok=True)\n",
    "        batch_size = 100000  # Define batch size for saving\n",
    "        for i in range(0, len(results), batch_size):\n",
    "            batch_results = results[i:i + batch_size]\n",
    "            db_filename = f\"{db_folder}/propagation_delay_batch_{i // batch_size}.csv\"\n",
    "\n",
    "            with open(db_filename, 'w') as csvfile:\n",
    "                header = \"distance,velocity,delay\\n\"\n",
    "                csvfile.write(header)\n",
    "\n",
    "                for r in batch_results:\n",
    "                    line = f\"{r['distance']},{r['velocity']},{r['delay']}\\n\"\n",
    "                    csvfile.write(line)\n",
    "\n",
    "        self.compress_database(db_folder)\n",
    "\n",
    "    @staticmethod\n",
    "    def compress_database(db_folder):\n",
    "        \"\"\"Compress database folder\"\"\"\n",
    "        zip_filename = f\"{db_folder}.zip\"\n",
    "        with ZipFile(zip_filename, \"w\", ZIP_DEFLATED) as zipf:\n",
    "            for root, _, files in os.walk(db_folder):\n",
    "                for fn in files:\n",
    "                    file_path = os.path.join(root, fn)\n",
    "                    zipf.write(file_path, arcname=os.path.relpath(file_path, db_folder))\n",
    "        print(f\"Database compressed to {zip_filename}\")\n",
    "\n",
    "# ## Run the Propagation Delay Calculation\n",
    "if __name__ == \"__main__\":\n",
    "    PropagationDelay.init_logger()\n",
    "    propagation_delay = PropagationDelay(distance=1000, velocity=300000000)  # Initial values, will be overwritten by the parameter ranges\n",
    "    propagation_delay.calculate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79256ea-4638-425d-9193-be64b4b18f6b",
   "metadata": {},
   "source": [
    "# Channel Capacity Calculation (Shannon-Hartley Theorem)\n",
    "\n",
    "This script generates a comprehensive dataset for channel capacity considering various bandwidths and signal-to-noise ratios (SNR).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f716f1-04d2-46ea-b0f4-611dea48ba1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Channel Capacity Calculation (Shannon-Hartley Theorem)\n",
    "\n",
    "This script generates a comprehensive dataset for channel capacity considering various bandwidths and signal-to-noise ratios (SNR).\n",
    "\n",
    "```python\n",
    "import os\n",
    "import logging\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from pathlib import Path\n",
    "from zipfile import ZipFile, ZIP_DEFLATED\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "class ChannelCapacity:\n",
    "    def __init__(self, bandwidth, snr):\n",
    "        \"\"\"Initialize the Channel Capacity class\"\"\"\n",
    "        self.bandwidth = bandwidth  # Bandwidth in Hz\n",
    "        self.snr = snr  # Signal-to-Noise Ratio in dB\n",
    "\n",
    "    @staticmethod\n",
    "    def init_logger():\n",
    "        \"\"\"Initialize logger\"\"\"\n",
    "        log_folder = \"logs\"\n",
    "        Path(log_folder).mkdir(parents=True, exist_ok=True)\n",
    "        logging.basicConfig(\n",
    "            filename=os.path.join(log_folder, 'channel_capacity.log'),\n",
    "            level=logging.DEBUG,\n",
    "            format='%(asctime)s %(levelname)s:%(message)s'\n",
    "        )\n",
    "        logging.debug(\"Logger initialized\")\n",
    "\n",
    "    @staticmethod\n",
    "    def log_results(results):\n",
    "        \"\"\"Log the results\"\"\"\n",
    "        with open(\"results_channel_capacity.txt\", \"a\") as f:\n",
    "            for r in results:\n",
    "                f.write(f\"{r}\\n\")\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_channel_capacity(bandwidth, snr):\n",
    "        \"\"\"Calculate Channel Capacity\"\"\"\n",
    "        snr_linear = 10**(snr / 10)  # Convert SNR from dB to linear scale\n",
    "        capacity = bandwidth * np.log2(1 + snr_linear)  # Shannon-Hartley Theorem\n",
    "        return {\"bandwidth\": bandwidth, \"snr\": snr, \"capacity\": capacity}\n",
    "\n",
    "    def calculate(self):\n",
    "        \"\"\"Calculate Channel Capacity for a range of parameters\"\"\"\n",
    "        # Define the range of variables\n",
    "        bandwidth_values = np.arange(1e6, 1e9, 1e6)  # Bandwidth from 1 MHz to 1 GHz\n",
    "        snr_values = np.arange(-20, 50, 1)  # SNR from -20 dB to 50 dB\n",
    "\n",
    "        parameters = [(bw, snr) for bw in bandwidth_values for snr in snr_values]\n",
    "        num_cpus = min(cpu_count(), 16)  # Limit to 16 cores\n",
    "        logging.debug(f\"Using {num_cpus} CPU cores for parallel processing\")\n",
    "\n",
    "        results = []\n",
    "        with Pool(processes=num_cpus) as pool:\n",
    "            for result in tqdm(pool.imap_unordered(lambda p: ChannelCapacity.calculate_channel_capacity(*p), parameters), total=len(parameters)):\n",
    "                if result is not None:\n",
    "                    results.append(result)\n",
    "\n",
    "        self.log_results(results)\n",
    "\n",
    "        db_folder = \"db_channel_capacity\"\n",
    "        Path(db_folder).mkdir(parents=True, exist_ok=True)\n",
    "        batch_size = 100000  # Define batch size for saving\n",
    "        for i in range(0, len(results), batch_size):\n",
    "            batch_results = results[i:i + batch_size]\n",
    "            db_filename = f\"{db_folder}/channel_capacity_batch_{i // batch_size}.csv\"\n",
    "\n",
    "            with open(db_filename, 'w') as csvfile:\n",
    "                header = \"bandwidth,snr,capacity\\n\"\n",
    "                csvfile.write(header)\n",
    "\n",
    "                for r in batch_results:\n",
    "                    line = f\"{r['bandwidth']},{r['snr']},{r['capacity']}\\n\"\n",
    "                    csvfile.write(line)\n",
    "\n",
    "        self.compress_database(db_folder)\n",
    "\n",
    "    @staticmethod\n",
    "    def compress_database(db_folder):\n",
    "        \"\"\"Compress database folder\"\"\"\n",
    "        zip_filename = f\"{db_folder}.zip\"\n",
    "        with ZipFile(zip_filename, \"w\", ZIP_DEFLATED) as zipf:\n",
    "            for root, _, files in os.walk(db_folder):\n",
    "                for fn in files:\n",
    "                    file_path = os.path.join(root, fn)\n",
    "                    zipf.write(file_path, arcname=os.path.relpath(file_path, db_folder))\n",
    "        print(f\"Database compressed to {zip_filename}\")\n",
    "\n",
    "# Run the Channel Capacity Calculation\n",
    "if __name__ == \"__main__\":\n",
    "    ChannelCapacity.init_logger()\n",
    "    channel_capacity = ChannelCapacity(bandwidth=1e6, snr=0)  # Initial values, will be overwritten by the parameter ranges\n",
    "    channel_capacity.calculate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48f2658-19a3-49d3-bbc2-bbd207982cfb",
   "metadata": {},
   "source": [
    "# Shadow Fading Model Calculation\n",
    "\n",
    "This script generates a comprehensive dataset for shadow fading effects using a log-normal distribution model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea06fdc-a006-4de5-aa53-95f0c44a85b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shadow Fading Model Calculation\n",
    "\n",
    "This script generates a comprehensive dataset for shadow fading effects using a log-normal distribution model.\n",
    "\n",
    "```python\n",
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from pathlib import Path\n",
    "from zipfile import ZipFile, ZIP_DEFLATED\n",
    "from tqdm import tqdm\n",
    "\n",
    "class ShadowFading:\n",
    "    def __init__(self, mean, std_dev, distance):\n",
    "        \"\"\"Initialize the Shadow Fading class\"\"\"\n",
    "        self.mean = mean  # Mean of the log-normal distribution\n",
    "        self.std_dev = std_dev  # Standard deviation of the log-normal distribution\n",
    "        self.distance = distance  # Distance in meters\n",
    "\n",
    "    @staticmethod\n",
    "    def init_logger():\n",
    "        \"\"\"Initialize logger\"\"\"\n",
    "        log_folder = \"logs\"\n",
    "        Path(log_folder).mkdir(parents=True, exist_ok=True)\n",
    "        logging.basicConfig(\n",
    "            filename=os.path.join(log_folder, 'shadow_fading.log'),\n",
    "            level=logging.DEBUG,\n",
    "            format='%(asctime)s %(levelname)s:%(message)s'\n",
    "        )\n",
    "        logging.debug(\"Logger initialized\")\n",
    "\n",
    "    @staticmethod\n",
    "    def log_results(results):\n",
    "        \"\"\"Log the results\"\"\"\n",
    "        with open(\"results_shadow_fading.txt\", \"a\") as f:\n",
    "            for r in results:\n",
    "                f.write(f\"{r}\\n\")\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_shadow_fading(mean, std_dev, distance):\n",
    "        \"\"\"Calculate Shadow Fading\"\"\"\n",
    "        fading = np.random.lognormal(mean, std_dev, 1)[0]  # Generate shadow fading value\n",
    "        return {\"mean\": mean, \"std_dev\": std_dev, \"distance\": distance, \"fading\": fading}\n",
    "\n",
    "    def calculate(self):\n",
    "        \"\"\"Calculate Shadow Fading for a range of parameters\"\"\"\n",
    "        # Define the range of variables\n",
    "        mean_values = np.arange(-3, 4, 0.5)  # Mean from -3 to 3 in steps of 0.5\n",
    "        std_dev_values = np.arange(1, 10, 1)  # Std Dev from 1 to 10 in steps of 1\n",
    "        distance_values = np.arange(10, 1000, 10)  # Distance from 10 m to 1000 m in steps of 10 m\n",
    "\n",
    "        parameters = [(mean, std_dev, dist) for mean in mean_values for std_dev in std_dev_values for dist in distance_values]\n",
    "        num_cpus = min(cpu_count(), 16)  # Limit to 16 cores\n",
    "        logging.debug(f\"Using {num_cpus} CPU cores for parallel processing\")\n",
    "\n",
    "        results = []\n",
    "        with Pool(processes=num_cpus) as pool:\n",
    "            for result in tqdm(pool.imap_unordered(lambda p: ShadowFading.calculate_shadow_fading(*p), parameters), total=len(parameters)):\n",
    "                if result is not None:\n",
    "                    results.append(result)\n",
    "\n",
    "        self.log_results(results)\n",
    "\n",
    "        db_folder = \"db_shadow_fading\"\n",
    "        Path(db_folder).mkdir(parents=True, exist_ok=True)\n",
    "        batch_size = 100000  # Define batch size for saving\n",
    "        for i in range(0, len(results), batch_size):\n",
    "            batch_results = results[i:i + batch_size]\n",
    "            db_filename = f\"{db_folder}/shadow_fading_batch_{i // batch_size}.csv\"\n",
    "\n",
    "            with open(db_filename, 'w') as csvfile:\n",
    "                header = \"mean,std_dev,distance,fading\\n\"\n",
    "                csvfile.write(header)\n",
    "\n",
    "                for r in batch_results:\n",
    "                    line = f\"{r['mean']},{r['std_dev']},{r['distance']},{r['fading']}\\n\"\n",
    "                    csvfile.write(line)\n",
    "\n",
    "        self.compress_database(db_folder)\n",
    "\n",
    "    @staticmethod\n",
    "    def compress_database(db_folder):\n",
    "        \"\"\"Compress database folder\"\"\"\n",
    "        zip_filename = f\"{db_folder}.zip\"\n",
    "        with ZipFile(zip_filename, \"w\", ZIP_DEFLATED) as zipf:\n",
    "            for root, _, files in os.walk(db_folder):\n",
    "                for fn in files:\n",
    "                    file_path = os.path.join(root, fn)\n",
    "                    zipf.write(file_path, arcname=os.path.relpath(file_path, db_folder))\n",
    "        print(f\"Database compressed to {zip_filename}\")\n",
    "\n",
    "# Run the Shadow Fading Calculation\n",
    "if __name__ == \"__main__\":\n",
    "    ShadowFading.init_logger()\n",
    "    shadow_fading = ShadowFading(mean=0, std_dev=2, distance=100)  # Initial values, will be overwritten by the parameter ranges\n",
    "    shadow_fading.calculate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29b4d5e-ef7a-4009-9933-03aa9ccfa040",
   "metadata": {},
   "source": [
    "# Rician Fading Model Calculation\n",
    "\n",
    "This script generates a comprehensive dataset for Rician fading effects, considering both line-of-sight (LOS) and multipath components.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2510c873-c91e-4783-8254-6b191b880e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rician Fading Model Calculation\n",
    "\n",
    "This script generates a comprehensive dataset for Rician fading effects, considering both line-of-sight (LOS) and multipath components.\n",
    "\n",
    "```python\n",
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from pathlib import Path\n",
    "from zipfile import ZipFile, ZIP_DEFLATED\n",
    "from tqdm import tqdm\n",
    "\n",
    "class RicianFading:\n",
    "    def __init__(self, K_factor, distance):\n",
    "        \"\"\"Initialize the Rician Fading class\"\"\"\n",
    "        self.K_factor = K_factor  # Rician K-factor\n",
    "        self.distance = distance  # Distance in meters\n",
    "\n",
    "    @staticmethod\n",
    "    def init_logger():\n",
    "        \"\"\"Initialize logger\"\"\"\n",
    "        log_folder = \"logs\"\n",
    "        Path(log_folder).mkdir(parents=True, exist_ok=True)\n",
    "        logging.basicConfig(\n",
    "            filename=os.path.join(log_folder, 'rician_fading.log'),\n",
    "            level=logging.DEBUG,\n",
    "            format='%(asctime)s %(levelname)s:%(message)s'\n",
    "        )\n",
    "        logging.debug(\"Logger initialized\")\n",
    "\n",
    "    @staticmethod\n",
    "    def log_results(results):\n",
    "        \"\"\"Log the results\"\"\"\n",
    "        with open(\"results_rician_fading.txt\", \"a\") as f:\n",
    "            for r in results:\n",
    "                f.write(f\"{r}\\n\")\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_rician_fading(K_factor, distance):\n",
    "        \"\"\"Calculate Rician Fading\"\"\"\n",
    "        # Generate Rician fading value using the K-factor and distance\n",
    "        fading = np.random.rician(K_factor, 1, 1)[0]  # Assuming unit variance\n",
    "        return {\"K_factor\": K_factor, \"distance\": distance, \"fading\": fading}\n",
    "\n",
    "    def calculate(self):\n",
    "        \"\"\"Calculate Rician Fading for a range of parameters\"\"\"\n",
    "        # Define the range of variables\n",
    "        K_factor_values = np.arange(0, 10, 0.5)  # K-factor from 0 to 10 in steps of 0.5\n",
    "        distance_values = np.arange(10, 1000, 10)  # Distance from 10 m to 1000 m in steps of 10 m\n",
    "\n",
    "        parameters = [(K_factor, dist) for K_factor in K_factor_values for dist in distance_values]\n",
    "        num_cpus = min(cpu_count(), 16)  # Limit to 16 cores\n",
    "        logging.debug(f\"Using {num_cpus} CPU cores for parallel processing\")\n",
    "\n",
    "        results = []\n",
    "        with Pool(processes=num_cpus) as pool:\n",
    "            for result in tqdm(pool.imap_unordered(lambda p: RicianFading.calculate_rician_fading(*p), parameters), total=len(parameters)):\n",
    "                if result is not None:\n",
    "                    results.append(result)\n",
    "\n",
    "        self.log_results(results)\n",
    "\n",
    "        db_folder = \"db_rician_fading\"\n",
    "        Path(db_folder).mkdir(parents=True, exist_ok=True)\n",
    "        batch_size = 100000  # Define batch size for saving\n",
    "        for i in range(0, len(results), batch_size):\n",
    "            batch_results = results[i:i + batch_size]\n",
    "            db_filename = f\"{db_folder}/rician_fading_batch_{i // batch_size}.csv\"\n",
    "\n",
    "            with open(db_filename, 'w') as csvfile:\n",
    "                header = \"K_factor,distance,fading\\n\"\n",
    "                csvfile.write(header)\n",
    "\n",
    "                for r in batch_results:\n",
    "                    line = f\"{r['K_factor']},{r['distance']},{r['fading']}\\n\"\n",
    "                    csvfile.write(line)\n",
    "\n",
    "        self.compress_database(db_folder)\n",
    "\n",
    "    @staticmethod\n",
    "    def compress_database(db_folder):\n",
    "        \"\"\"Compress database folder\"\"\"\n",
    "        zip_filename = f\"{db_folder}.zip\"\n",
    "        with ZipFile(zip_filename, \"w\", ZIP_DEFLATED) as zipf:\n",
    "            for root, _, files in os.walk(db_folder):\n",
    "                for fn in files:\n",
    "                    file_path = os.path.join(root, fn)\n",
    "                    zipf.write(file_path, arcname=os.path.relpath(file_path, db_folder))\n",
    "        print(f\"Database compressed to {zip_filename}\")\n",
    "\n",
    "# Run the Rician Fading Calculation\n",
    "if __name__ == \"__main__\":\n",
    "    RicianFading.init_logger()\n",
    "    rician_fading = RicianFading(K_factor=3, distance=100)  # Initial values, will be overwritten by the parameter ranges\n",
    "    rician_fading.calculate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6189c05c-6086-4c13-8141-5cc21f15c1bf",
   "metadata": {},
   "source": [
    "# Nakagami Fading Model Calculation\n",
    "\n",
    "This script generates a comprehensive dataset for Nakagami fading effects, considering various fading conditions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612b13bf-8801-4b11-8479-58b927799579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nakagami Fading Model Calculation\n",
    "\n",
    "This script generates a comprehensive dataset for Nakagami fading effects, considering various fading conditions.\n",
    "\n",
    "```python\n",
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from pathlib import Path\n",
    "from zipfile import ZipFile, ZIP_DEFLATED\n",
    "from tqdm import tqdm\n",
    "\n",
    "class NakagamiFading:\n",
    "    def __init__(self, m_factor, distance):\n",
    "        \"\"\"Initialize the Nakagami Fading class\"\"\"\n",
    "        self.m_factor = m_factor  # Nakagami m-factor\n",
    "        self.distance = distance  # Distance in meters\n",
    "\n",
    "    @staticmethod\n",
    "    def init_logger():\n",
    "        \"\"\"Initialize logger\"\"\"\n",
    "        log_folder = \"logs\"\n",
    "        Path(log_folder).mkdir(parents=True, exist_ok=True)\n",
    "        logging.basicConfig(\n",
    "            filename=os.path.join(log_folder, 'nakagami_fading.log'),\n",
    "            level=logging.DEBUG,\n",
    "            format='%(asctime)s %(levelname)s:%(message)s'\n",
    "        )\n",
    "        logging.debug(\"Logger initialized\")\n",
    "\n",
    "    @staticmethod\n",
    "    def log_results(results):\n",
    "        \"\"\"Log the results\"\"\"\n",
    "        with open(\"results_nakagami_fading.txt\", \"a\") as f:\n",
    "            for r in results:\n",
    "                f.write(f\"{r}\\n\")\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_nakagami_fading(m_factor, distance):\n",
    "        \"\"\"Calculate Nakagami Fading\"\"\"\n",
    "        # Generate Nakagami fading value using the m-factor and distance\n",
    "        fading = np.random.gamma(m_factor, 1 / m_factor, 1)[0]  # Assuming unit variance\n",
    "        return {\"m_factor\": m_factor, \"distance\": distance, \"fading\": fading}\n",
    "\n",
    "    def calculate(self):\n",
    "        \"\"\"Calculate Nakagami Fading for a range of parameters\"\"\"\n",
    "        # Define the range of variables\n",
    "        m_factor_values = np.arange(0.5, 5, 0.5)  # m-factor from 0.5 to 5 in steps of 0.5\n",
    "        distance_values = np.arange(10, 1000, 10)  # Distance from 10 m to 1000 m in steps of 10 m\n",
    "\n",
    "        parameters = [(m_factor, dist) for m_factor in m_factor_values for dist in distance_values]\n",
    "        num_cpus = min(cpu_count(), 16)  # Limit to 16 cores\n",
    "        logging.debug(f\"Using {num_cpus} CPU cores for parallel processing\")\n",
    "\n",
    "        results = []\n",
    "        with Pool(processes=num_cpus) as pool:\n",
    "            for result in tqdm(pool.imap_unordered(lambda p: NakagamiFading.calculate_nakagami_fading(*p), parameters), total=len(parameters)):\n",
    "                if result is not None:\n",
    "                    results.append(result)\n",
    "\n",
    "        self.log_results(results)\n",
    "\n",
    "        db_folder = \"db_nakagami_fading\"\n",
    "        Path(db_folder).mkdir(parents=True, exist_ok=True)\n",
    "        batch_size = 100000  # Define batch size for saving\n",
    "        for i in range(0, len(results), batch_size):\n",
    "            batch_results = results[i:i + batch_size]\n",
    "            db_filename = f\"{db_folder}/nakagami_fading_batch_{i // batch_size}.csv\"\n",
    "\n",
    "            with open(db_filename, 'w') as csvfile:\n",
    "                header = \"m_factor,distance,fading\\n\"\n",
    "                csvfile.write(header)\n",
    "\n",
    "                for r in batch_results:\n",
    "                    line = f\"{r['m_factor']},{r['distance']},{r['fading']}\\n\"\n",
    "                    csvfile.write(line)\n",
    "\n",
    "        self.compress_database(db_folder)\n",
    "\n",
    "    @staticmethod\n",
    "    def compress_database(db_folder):\n",
    "        \"\"\"Compress database folder\"\"\"\n",
    "        zip_filename = f\"{db_folder}.zip\"\n",
    "        with ZipFile(zip_filename, \"w\", ZIP_DEFLATED) as zipf:\n",
    "            for root, _, files in os.walk(db_folder):\n",
    "                for fn in files:\n",
    "                    file_path = os.path.join(root, fn)\n",
    "                    zipf.write(file_path, arcname=os.path.relpath(file_path, db_folder))\n",
    "        print(f\"Database compressed to {zip_filename}\")\n",
    "\n",
    "# Run the Nakagami Fading Calculation\n",
    "if __name__ == \"__main__\":\n",
    "    NakagamiFading.init_logger()\n",
    "    nakagami_fading = NakagamiFading(m_factor=1, distance=100)  # Initial values, will be overwritten by the parameter ranges\n",
    "    nakagami_fading.calculate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984938ed-67a4-4221-a9a1-f5e94008ea82",
   "metadata": {},
   "source": [
    "# Okumura-Hata Model Calculation\n",
    "\n",
    "This script generates a comprehensive dataset using the Okumura-Hata model for urban and suburban environments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7b63e0-bdec-4d74-bf24-67f962c52be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okumura-Hata Model Calculation\n",
    "\n",
    "This script generates a comprehensive dataset using the Okumura-Hata model for urban and suburban environments.\n",
    "\n",
    "```python\n",
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from pathlib import Path\n",
    "from zipfile import ZipFile, ZIP_DEFLATED\n",
    "from tqdm import tqdm\n",
    "\n",
    "class OkumuraHata:\n",
    "    def __init__(self, frequency, distance, tx_height, rx_height, environment='urban'):\n",
    "        \"\"\"Initialize the Okumura-Hata model parameters\"\"\"\n",
    "        self.frequency = frequency  # in MHz\n",
    "        self.distance = distance  # in km\n",
    "        self.tx_height = tx_height  # in meters\n",
    "        self.rx_height = rx_height  # in meters\n",
    "        self.environment = environment\n",
    "\n",
    "    @staticmethod\n",
    "    def init_logger():\n",
    "        \"\"\"Initialize logger\"\"\"\n",
    "        log_folder = \"logs\"\n",
    "        Path(log_folder).mkdir(parents=True, exist_ok=True)\n",
    "        logging.basicConfig(\n",
    "            filename=os.path.join(log_folder, 'okumura_hata.log'),\n",
    "            level=logging.DEBUG,\n",
    "            format='%(asctime)s %(levelname)s:%(message)s'\n",
    "        )\n",
    "        logging.debug(\"Logger initialized\")\n",
    "\n",
    "    @staticmethod\n",
    "    def log_results(results):\n",
    "        \"\"\"Log the results\"\"\"\n",
    "        with open(\"results_okumura_hata.txt\", \"a\") as f:\n",
    "            for r in results:\n",
    "                f.write(f\"{r}\\n\")\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_path_loss(frequency, distance, tx_height, rx_height, environment):\n",
    "        \"\"\"Calculate Okumura-Hata Path Loss\"\"\"\n",
    "        if environment == 'urban':\n",
    "            c_h = 3.2 * (np.log10(11.75 * rx_height))**2 - 4.97\n",
    "        elif environment == 'suburban':\n",
    "            c_h = 0.8 + (1.1 * np.log10(frequency) - 0.7) * rx_height - 1.56 * np.log10(frequency)\n",
    "        else:  # rural\n",
    "            c_h = 2 * (np.log10(frequency/28))**2 + 5.4\n",
    "\n",
    "        path_loss = (69.55 + 26.16 * np.log10(frequency) - 13.82 * np.log10(tx_height) -\n",
    "                     c_h + (44.9 - 6.55 * np.log10(tx_height)) * np.log10(distance))\n",
    "        \n",
    "        return {\"frequency\": frequency, \"distance\": distance, \"tx_height\": tx_height, \"rx_height\": rx_height, \"environment\": environment, \"path_loss\": path_loss}\n",
    "\n",
    "    def calculate(self):\n",
    "        \"\"\"Calculate Okumura-Hata Path Loss for a range of parameters\"\"\"\n",
    "        # Define the range of variables\n",
    "        frequency_values = np.arange(150, 2000, 50)  # Frequency from 150 MHz to 2000 MHz\n",
    "        distance_values = np.arange(0.1, 50, 0.5)  # Distance from 0.1 km to 50 km\n",
    "        tx_height_values = np.arange(30, 200, 10)  # Tx height from 30 m to 200 m\n",
    "        rx_height_values = np.arange(1, 20, 1)  # Rx height from 1 m to 20 m\n",
    "        environments = ['urban', 'suburban', 'rural']\n",
    "\n",
    "        parameters = [(freq, dist, tx_height, rx_height, env) for freq in frequency_values for dist in distance_values for tx_height in tx_height_values for rx_height in rx_height_values for env in environments]\n",
    "        num_cpus = min(cpu_count(), 16)  # Limit to 16 cores\n",
    "        logging.debug(f\"Using {num_cpus} CPU cores for parallel processing\")\n",
    "\n",
    "        results = []\n",
    "        with Pool(processes=num_cpus) as pool:\n",
    "            for result in tqdm(pool.imap_unordered(lambda p: OkumuraHata.calculate_path_loss(*p), parameters), total=len(parameters)):\n",
    "                if result is not None:\n",
    "                    results.append(result)\n",
    "\n",
    "        self.log_results(results)\n",
    "\n",
    "        db_folder = \"db_okumura_hata\"\n",
    "        Path(db_folder).mkdir(parents=True, exist_ok=True)\n",
    "        batch_size = 100000  # Define batch size for saving\n",
    "        for i in range(0, len(results), batch_size):\n",
    "            batch_results = results[i:i + batch_size]\n",
    "            db_filename = f\"{db_folder}/okumura_hata_batch_{i // batch_size}.csv\"\n",
    "\n",
    "            with open(db_filename, 'w') as csvfile:\n",
    "                header = \"frequency,distance,tx_height,rx_height,environment,path_loss\\n\"\n",
    "                csvfile.write(header)\n",
    "\n",
    "                for r in batch_results:\n",
    "                    line = f\"{r['frequency']},{r['distance']},{r['tx_height']},{r['rx_height']},{r['environment']},{r['path_loss']}\\n\"\n",
    "                    csvfile.write(line)\n",
    "\n",
    "        self.compress_database(db_folder)\n",
    "\n",
    "    @staticmethod\n",
    "    def compress_database(db_folder):\n",
    "        \"\"\"Compress database folder\"\"\"\n",
    "        zip_filename = f\"{db_folder}.zip\"\n",
    "        with ZipFile(zip_filename, \"w\", ZIP_DEFLATED) as zipf:\n",
    "            for root, _, files in os.walk(db_folder):\n",
    "                for fn in files:\n",
    "                    file_path = os.path.join(root, fn)\n",
    "                    zipf.write(file_path, arcname=os.path.relpath(file_path, db_folder))\n",
    "        print(f\"Database compressed to {zip_filename}\")\n",
    "\n",
    "# Run the Okumura-Hata Calculation\n",
    "if __name__ == \"__main__\":\n",
    "    OkumuraHata.init_logger()\n",
    "    okumura_hata = OkumuraHata(frequency=900, distance=1, tx_height=50, rx_height=1.5)  # Initial values, will be overwritten by the parameter ranges\n",
    "    okumura_hata.calculate()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
